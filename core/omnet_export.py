"""core.omnet_export
---------------------------------
Utilities to export a `Network` plus a scheduling result (`ScheduleRes`)
obtained with `ui/test.py` into two artefacts ready to be consumed by
OMNeT++ NET‑TSN:

* **<name>.ned** –  the physical topology as a `network` definition.
* **<name>.ini** –  simulation configuration that reproduces the exact
  cyclic traffic (period, payload, initial offset) *and* programmes the
  time‑aware shaping gate according to the static GCL derived during the
  DRL scheduling step.

Files are *always* overwritten if they already exist.

This module is completely decoupled from the rest of the code‑base – it
only relies on public attributes of `core.network` objects, on the final
`ScheduleRes`, and on the «GCL tables» returned by `ResAnalyzer`.
"""


from __future__ import annotations
import logging

import os
import math
from pathlib import Path
from typing import Dict, List, Tuple
from collections import defaultdict

from core.network.net import Network, Flow, Link
from core.scheduler.scheduler import ScheduleRes

# ---------------------------------------------------------------------------
#  UTILIDADES DE FORMATO  (↓ se usan en todo el módulo, por eso van primero)
# ---------------------------------------------------------------------------
μs = 1e-6

def _us_to_ms_str(us: int) -> str:
    """µs → "x.xxms" sin ceros sobrantes."""
    return f"{us * μs * 1_000:.3f}ms".rstrip("0").rstrip(".")

# ---------- helpers para puertos y PCP ----------
def _build_period_order(flows: list["Flow"]) -> dict[int, int]:
    """Mapa período→índice incremental (2000 µs⇒0 → UDP 2000, 4000 µs⇒1 → 2001…)."""
    unique = sorted({f.period for f in flows})
    return {p: i for i, p in enumerate(unique)}

def _flow_port(period: int, period_order: dict[int, int]) -> int:
    return 2000 + period_order[period]

# PCP: 4 para primer flujo por cliente, 7 el resto
def _pcp_for_flow(node_name: str, seen: dict[str, int]) -> int:
    pcp = 4 if seen[node_name] == 0 else 7
    seen[node_name] += 1
    return pcp

def _first_tx_offset_us(schedule_res: "ScheduleRes", fl: "Flow") -> int:
    """
    Devuelve el **start_time** del primer hop del flujo `fl`.

    🔸 La comparación se hace por `flow_id` (==) en lugar de identidad
       de objetos, porque los `Flow` en `schedule_res` pueden ser copias.
    """
    first_link_id = fl.path[0]            # (src, dst) del primer salto
    for link, ops in schedule_res.items():
        if link.link_id != first_link_id:
            continue
        for f, op in ops:
            if f.flow_id == fl.flow_id:   # ← comparación robusta
                return op.start_time
    # fallback – no debería ocurrir
    return 0

# ---------------------------------------------------------------------------
#  Small helpers                                                               
# ---------------------------------------------------------------------------

# (las tres funciones ya se han adelantado)


def _indent(n: int) -> str:
    return " " * n


# ---------------------------------------------------------------------------
#  NED generation                                                             
# ---------------------------------------------------------------------------


def _ned_node_line(node_name: str, node_type: str, xpos: int, ypos: int) -> str:
    """Return a *submodule* line for a node with display coordinates."""
    return (
        f"        {node_name}: <default(\"{node_type}\")> like IEthernetNetworkNode {{\n"
        f"            @display(\"p={xpos},{ypos}\");\n"
        f"        }}\n"
    )


#  ────────────────────────────────────────────────────────────────────────────
#  INI generation
#  ────────────────────────────────────────────────────────────────────────────

def _ini_general(net_name: str, gate_port: int) -> str:
    return (
        "#########################################################\n"
        "#  AUTOGENERATED BY gatingdrl omnet_export.py            #\n"
        "#########################################################\n"
        "[General]\n"
        f"network = {net_name}\n"
        "sim-time-limit   = 24ms\n"
        'description      = "Auto-imported schedule"\n\n'
        "**.displayGateSchedules = true\n"
        # Mostrar por defecto todos los puertos (el filtro se puede afinar después)
        f'**.gateFilter = "**.eth[{gate_port}].**"\n'
        "**.gateScheduleVisualizer.height = 16\n"
        '**.gateScheduleVisualizer.placementHint = "top"\n\n'
    )

def _ini_flows(
        flows: List["Flow"],
        schedule_res: ScheduleRes,
        network: "Network"           # ← necesitamos acceso al grafo
) -> str:
    """
    Construye la sección SOURCES + SINKS a partir de la lista completa de flujos.
    No debe lanzar excepción – si un flujo carece de algún dato, lo salta y sigue.
    """
    by_src: Dict[str, List["Flow"]] = {}
    by_dst: Dict[str, List["Flow"]] = {}
    for f in flows:
        by_src.setdefault(f.src_id.lower(), []).append(f)
        by_dst.setdefault(f.dst_id.lower(), []).append(f)

    txt = "#########################################################\n"
    txt += "#  SOURCES                                               #\n"
    txt += "#########################################################\n"
    # ▸ Puerto UDP único por cliente (2000 + índice del cliente)
    client_port_map: Dict[str, int] = {
        n: 2000 + i for i, n in enumerate(sorted(by_src.keys()))
    }
    seen_per_client: Dict[str, int] = defaultdict(int)
    for node, list_flows in by_src.items():
        txt += f"*.{node}.numApps           = {len(list_flows)}\n"
        txt += f"*.{node}.app[*].typename   = \"UdpSourceApp\"\n"
        txt += f"*.{node}.app[*].io.destAddress = \"srv1\"\n\n"

        for idx, fl in enumerate(list_flows):
            # ──────────── NUEVO: etiqueta legible para el flujo ────────────
            # Usa el flow_id ('F0', 'F1'…) en minúsculas  →  "f0", "f1", …
            # Si se prefiere "video-1" basta con reemplazar la siguiente línea
            display_name = fl.flow_id.lower()
            txt += f"*.{node}.app[{idx}].display-name   = \"{display_name}\"\n"

            # ➋ Offset = «Disponible (start time)» ⇒ op.start_time
            off_us = _first_tx_offset_us(schedule_res, fl)
            off_ms = _us_to_ms_str(off_us)
            
            # ➊ Tamaño de trama = payload mostrado en «INFORMACIÓN DE FLUJOS»
            #    (no restamos 54 B de cabeceras; es exactamente el valor listado).
            length = max(64, fl.payload)

            txt += f"*.{node}.app[{idx}].source.productionInterval = {fl.period*μs*1_000:.0f}ms\n"
            txt += f"*.{node}.app[{idx}].source.initialProductionOffset = {off_ms}\n"
            txt += f"*.{node}.app[{idx}].source.packetLength = {length}B\n"

            dport = client_port_map[node]           # puerto fijo por cliente
            txt  += f"*.{node}.app[{idx}].io.destPort = {dport}\n"

            pcp = _pcp_for_flow(node, seen_per_client)
            txt += (f"*.{node}.app[{idx}].bridging.streamIdentifier.identifier.mapping = "
                    f"[{{stream: \"{fl.flow_id}\", packetFilter: expr(udp.destPort == {dport})}}]\n")
            txt += (f"*.{node}.app[{idx}].bridging.streamCoder.encoder.mapping = "
                    f"[{{stream: \"{fl.flow_id}\", pcp: {pcp}}}]\n\n")

    # ------- habilitar outgoing streams por cliente -----------
    for node in by_src.keys():
        txt += f"*.{node}.hasOutgoingStreams = true\n"

    # ------- habilitar egress shaping en cada switch ----------
    sw_nodes = [n.lower() for n, d in network.graph.nodes(data=True)
                if d.get("node_type") == "SW"]
    for sw in sw_nodes:
        txt += f"*.{sw}.hasEgressTrafficShaping = true\n"
        txt += (f"*.{sw}.bridging.directionReverser.reverser."
                f"excludeEncapsulationProtocols = [\"ieee8021qctag\"]\n")

    txt += "\n#########################################################\n"
    txt += "#  SINKS                                                 #\n"
    txt += "#########################################################\n"
    for node, list_flows in by_dst.items():
        txt += f"*.{node}.numApps         = {len(list_flows)}\n"
        txt += f"*.{node}.app[*].typename = \"UdpSinkApp\"\n"
        for idx, _ in enumerate(list_flows):
            txt += f"*.{node}.app[{idx}].io.localPort = {2000+idx}\n"
        txt += "\n"

    return txt

def _ini_gcl(
    gcl: Dict["Link", List[Tuple[int, int]]],
    network: "Network",
    gate_port: int = 0,           # ← puerto declarado en **.gateFilter
) -> str:
    """
    Genera la sección de programación Time-Aware para cada puerto de switch.
    Si `gcl` está vacío simplemente devuelve cadena vacía.
    """
    if not gcl:
        return ""

    txt = "#########################################################\n"
    txt += "#  TIME-AWARE TRAFFIC SHAPING                           #\n"
    txt += "#########################################################\n"

    for link, table in gcl.items():
        src = link.link_id[0] if isinstance(link.link_id, tuple) else link.link_id.split('-')[0]
        dst = link.link_id[1] if isinstance(link.link_id, tuple) else link.link_id.split('-')[1]
        # Usa el mismo índice que figura en **.gateFilter
        port = gate_port

        # ────────────────  UNA sola cola "video" ─────────────────
        txt += f"*.{src.lower()}.eth[{port}].macLayer.queue.numTrafficClasses = 1\n"
        txt += f"*.{src.lower()}.eth[{port}].macLayer.queue.*[0].display-name = \"video\"\n"

        # Gate único TC0 – usar **la tabla que ya imprimió ResAnalyzer**.
        #     ⮑  No se vuelven a ordenar ni a recalcular tiempos.
        # ➊  Hiperperíodo = m.c.m. de los períodos de *toda la red*
        import math as _m
        hyperperiod = 1
        for _f in network.flows:
            hyperperiod = _m.lcm(hyperperiod, _f.period)

        # ► LA TABLA LLEGA TAL CUAL SE IMPRIMIO EN
        #   "TABLA GCL GENERADA (t, estado)"  ◄
        #   ─ no se toca, sólo se ordena por tiempo.
        # ── tabla "corta" que llega desde ResAnalyzer ──
        events = sorted(table, key=lambda x: x[0])  # ya viene filtrada

        # Ordenar la tabla generada y *reemplazar* su PRIMER registro
        #     por "abierto en 0 µs" (no se añade, se sustituye).
        if events:
            events[0] = (0, 1)      # primer registro forzado
        else:
            events = [(0, 1)]       # salvaguarda: lista vacía

        # Elimina duplicados consecutivos (p.ej. …, (0,1), (0,1), …)
        compact: list[tuple[int, int]] = []
        for t, s in events:
            if not compact or compact[-1][1] != s:
                compact.append((t, s))

        durations = [(compact[(i+1)%len(compact)][0] - t) % hyperperiod
                     for i, (t, _) in enumerate(compact)]

        txt += f"*.{src.lower()}.eth[{port}].macLayer.queue.transmissionGate[0].offset = 0ms\n"
        txt += f"*.{src.lower()}.eth[{port}].macLayer.queue.transmissionGate[0].durations = "
        txt += f"[{', '.join(_us_to_ms_str(d) for d in durations)}]\n"

        # ────────────────────────────────────────────────────────────────────
        #  DEBUG   Comparar la tabla original (events) con la reconstruida
        #          a partir de las durations que vamos a escribir.
        # ────────────────────────────────────────────────────────────────────
        #
        # 1.  Tabla "original" (la que viene de ResAnalyzer) ────────────────
        orig_table = sorted(table, key=lambda x: x[0])    # [(t,state), ...]

        # 2.  Reconstruir tabla a partir de durations  ─────────────────────
        recon_table: list[tuple[int,int]] = []
        t_acc = 0
        state = 1                      # siempre arrancamos "abierto"
        for d in durations:
            recon_table.append((t_acc, state))
            t_acc = (t_acc + d) % hyperperiod
            state = 1 - state          # alternar 1↔0

        # 3.  Normalizar y ordenar para comparar
        recon_norm = sorted(recon_table, key=lambda x: x[0])
        orig_norm  = orig_table

        # Si las tablas no coinciden, registra un mensaje breve solo en nivel DEBUG.
        # Así evitamos saturar la salida estándar durante `ui/test.py`.
        if recon_norm != orig_norm:
            import logging
            logging.getLogger(__name__).debug(
                f"[GCL-CHECK] Diferencia detectada en {src}.eth[{port}] "
                "(detalles omitidos; habilita DEBUG para ver el diff)."
            )

    return txt


def write_ned(network: Network, outfile: os.PathLike, pkg: str, net_name: str) -> None:
    """Generate a .ned file representing *exactly* the passed network."""

    # ── Simple – we only need node names and edges.  Use a naïve layout: ──
    # • End‑stations on the left (x ≈ 300)  – spread vertically in steps
    # • Switches   in the middle (x ≈ 500)
    # • Servers    on the right (x ≈ 700)

    es_nodes = [n for n, d in network.graph.nodes(data=True) if d.get("node_type") == "ES"]
    sw_nodes = [n for n, d in network.graph.nodes(data=True) if d.get("node_type") == "SW"]

    server_nodes = [n for n in es_nodes if n.startswith("SRV")]
    client_nodes = [n for n in es_nodes if n not in server_nodes]

    # Coordinates
    step = 80
    submod_lines: List[str] = []

    for i, name in enumerate(client_nodes):
        submod_lines.append(_ned_node_line(name.lower(), "TsnDevice", 300, 200 + i * step))

    for i, name in enumerate(sw_nodes):
        submod_lines.append(_ned_node_line(name.lower(), "TsnSwitch", 500, 250 + i * step))

    for i, name in enumerate(server_nodes):
        submod_lines.append(_ned_node_line(name.lower(), "TsnDevice",
                                         700, 250 + i * step))

    # Connections – iterate original directed edges and create <--> pairs.
    conn_lines: List[str] = []
    added = set()
    for src, dst, edata in network.graph.edges(data=True):
        # Create deterministic id for unordered pair to avoid duplicates
        key = tuple(sorted((src, dst)))
        if key in added:
            continue
        added.add(key)

        # Mantener el mismo alias en las conexiones
        def _alias(n: str) -> str:
            return n.lower()
        conn_lines.append(
            f"        {_alias(src)}.ethg++ <--> EthernetLink <--> {_alias(dst)}.ethg++;\n"
        )
        
    net_name = Path(outfile).stem          # p.ej.  «unidir»
    # Detectar bitrate (asumimos que todos los enlaces comparten valor)
    try:
        first_edge = next(iter(network.graph.edges(data=True)))
        link_rate = first_edge[2].get("link_rate", 100)
    except StopIteration:
        link_rate = 100

    ned = (
        f"package {pkg};\n\n"
        "import inet.node.ethernet.EthernetLink;\n"
        "import inet.node.contract.IEthernetNetworkNode;\n"
        "import inet.node.tsn.TsnDevice;\n"
        "import inet.node.tsn.TsnSwitch;\n"
        "import inet.networks.base.TsnNetworkBase;\n\n"
        f"network {net_name.lower()} extends TsnNetworkBase\n"  # Force lowercase here
        "{\n"
        "    parameters:\n"
        f"        *.eth[*].bitrate = default({link_rate}Mbps);\n"
        "    submodules:\n"
        + "".join(submod_lines)
        + "\n    connections:\n"
        + "".join(conn_lines)
        + "}\n"
    )

    Path(outfile).write_text(ned, encoding="utf-8")

def write_ini(
    network: Network,
    schedule_res: ScheduleRes,
    gcl_tables: Dict[Link, List[Tuple[int, int]]],
    outfile: os.PathLike,
    net_name: str,
    gate_port: int,
) -> None:
    """Generate an .ini file reproducing traffic & GCL schedule.
    
    This function never fails completely - if problems occur during generation,
    it will still produce at least a basic INI file with the [General] section.
    """
    try:
        flows = list(network.flows)
        num_flows = len(flows)
        
        parts: List[str] = [_ini_general(net_name, gate_port)]
        try:
            parts.append(_ini_flows(flows, schedule_res, network))
        except Exception as exc:
            import logging
            logging.warning(f"[omnet_export] Error generating FLOWS section: {exc}")
            parts.append(f"#  (ERROR generando sección SOURCES / SINKS: {exc})\n")

        try:
            parts.append(_ini_gcl(gcl_tables, network, gate_port))
        except Exception as exc:
            import logging
            logging.warning(f"[omnet_export] Error generating GCL section: {exc}")
            parts.append(f"#  (ERROR generando sección GCL: {exc})\n")

        Path(outfile).write_text("".join(parts), encoding="utf-8")
            
    except Exception as e:
        import logging
        logging.error(f"[omnet_export] Critical error generating INI file: {e}")
        # Ensure at least a minimal INI file is written
        minimal_ini = "[General]\nnetwork = inet.showcases.tsn.generated.GeneratedTsnNetwork\n"
        Path(outfile).write_text(minimal_ini, encoding="utf-8")



def export_omnet_files(network: Network, schedule_res: ScheduleRes, gcl_tables: Dict[Link, List[Tuple[int, int]]], label: str, out_dir: os.PathLike, gate_port:int=2) -> Tuple[Path, Path]:
    """Create *.ned & *.ini in *out_dir* (overwriting any previous version)."""
    os.makedirs(out_dir, exist_ok=True)

    # Create a subdirectory for this topology if it doesn't exist
    topo_dir = Path(out_dir) / label
    topo_dir.mkdir(exist_ok=True, parents=True)

    ned_path = topo_dir / f"{label}.ned"
    ini_path = topo_dir / f"{label}.ini"

    pkg = "inet.showcases.tsn.trafficshaping.Pruebas_tesis.Red_" + label
    net_name = label.lower()

    write_ned(network, ned_path, pkg, net_name)
    write_ini(network, schedule_res, gcl_tables, ini_path, net_name, gate_port)

    print(f"[omnet_export]  generated → {ned_path}, {ini_path}")

    # ──────────────────────────────────────────────────────────────
    #  Resumen global de planificación (ahora SÍ con variables)
    # ──────────────────────────────────────────────────────────────
    total_flows = len(network.flows)
    scheduled_ids = {
        f.flow_id
        for ops in schedule_res.values()
        for f, _ in ops
    }                                        # ← únicos
    ok_flows = len(scheduled_ids)

    msg = f"Programados con éxito: {ok_flows}/{total_flows} flujos"
    print(msg)           # ← visible siempre
    logging.info(msg)    # ← para quien tenga el logger a nivel INFO
    return ned_path, ini_path
