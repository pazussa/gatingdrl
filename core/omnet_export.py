"""core.omnet_export
---------------------------------
Utilities to export a `Network` plus a scheduling result (`ScheduleRes`)
obtained with `ui/test.py` into two artefacts ready to be consumed by
OMNeT++ NETâ€‘TSN:

* **<name>.ned** â€“  the physical topology as a `network` definition.
* **<name>.ini** â€“  simulation configuration that reproduces the exact
  cyclic traffic (period, payload, initial offset) *and* programmes the
  timeâ€‘aware shaping gate according to the static GCL derived during the
  DRL scheduling step.

Files are *always* overwritten if they already exist.

This module is completely decoupled from the rest of the codeâ€‘base â€“ it
only relies on public attributes of `core.network` objects, on the final
`ScheduleRes`, and on the Â«GCL tablesÂ» returned by `ResAnalyzer`.
"""


from __future__ import annotations
import logging

import os
import math
from pathlib import Path
from typing import Dict, List, Tuple
from collections import defaultdict

from core.network.net import Network, Flow, Link
from core.scheduler.scheduler import ScheduleRes

# ---------------------------------------------------------------------------
#  UTILIDADES DE FORMATO  (â†“ se usan en todo el mÃ³dulo, por eso van primero)
# ---------------------------------------------------------------------------
Î¼s = 1e-6

def _us_to_ms_str(us: int) -> str:
    """Âµs â†’ "x.xxms" sin ceros sobrantes."""
    return f"{us * Î¼s * 1_000:.3f}ms".rstrip("0").rstrip(".")

# ---------- helpers para puertos y PCP ----------
def _build_period_order(flows: list["Flow"]) -> dict[int, int]:
    """Mapa perÃ­odoâ†’Ã­ndice incremental (2000 Âµsâ‡’0 â†’ UDP 2000, 4000 Âµsâ‡’1 â†’ 2001â€¦)."""
    unique = sorted({f.period for f in flows})
    return {p: i for i, p in enumerate(unique)}

def _flow_port(period: int, period_order: dict[int, int]) -> int:
    return 2000 + period_order[period]

# PCP: 4 para primer flujo por cliente, 7 el resto
def _pcp_for_flow(node_name: str, seen: dict[str, int]) -> int:
    pcp = 4 if seen[node_name] == 0 else 7
    seen[node_name] += 1
    return pcp

def _first_tx_offset_us(schedule_res: "ScheduleRes", fl: "Flow") -> int:
    """
    Devuelve el **start_time** del primer hop del flujo `fl`.

    ğŸ”¸ La comparaciÃ³n se hace por `flow_id` (==) en lugar de identidad
       de objetos, porque los `Flow` en `schedule_res` pueden ser copias.
    """
    first_link_id = fl.path[0]            # (src, dst) del primer salto
    for link, ops in schedule_res.items():
        if link.link_id != first_link_id:
            continue
        for f, op in ops:
            if f.flow_id == fl.flow_id:   # â† comparaciÃ³n robusta
                return op.start_time
    # fallback â€“ no deberÃ­a ocurrir
    return 0

# ---------------------------------------------------------------------------
#  Small helpers                                                               
# ---------------------------------------------------------------------------

# (las tres funciones ya se han adelantado)


def _indent(n: int) -> str:
    return " " * n


# ---------------------------------------------------------------------------
#  NED generation                                                             
# ---------------------------------------------------------------------------


def _ned_node_line(node_name: str, node_type: str, xpos: int, ypos: int) -> str:
    """Return a *submodule* line for a node with display coordinates."""
    return (
        f"        {node_name}: <default(\"{node_type}\")> like IEthernetNetworkNode {{\n"
        f"            @display(\"p={xpos},{ypos}\");\n"
        f"        }}\n"
    )


#  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  INI generation
#  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _ini_general(net_name: str, gate_port: int) -> str:
    return (
        "#########################################################\n"
        "#  AUTOGENERATED BY gatingdrl omnet_export.py            #\n"
        "#########################################################\n"
        "[General]\n"
        f"network = {net_name}\n"
        "sim-time-limit   = 24ms\n"
        'description      = "Auto-imported schedule"\n\n'
        "**.displayGateSchedules = true\n"
        # Mostrar por defecto todos los puertos (el filtro se puede afinar despuÃ©s)
        f'**.gateFilter = "**.eth[{gate_port}].**"\n'
        "**.gateScheduleVisualizer.height = 16\n"
        '**.gateScheduleVisualizer.placementHint = "top"\n\n'
    )

def _ini_flows(
        flows: List["Flow"],
        schedule_res: ScheduleRes,
        network: "Network"           # â† necesitamos acceso al grafo
) -> str:
    """
    Construye la secciÃ³n SOURCES + SINKS a partir de la lista completa de flujos.
    No debe lanzar excepciÃ³n â€“ si un flujo carece de algÃºn dato, lo salta y sigue.
    """
    by_src: Dict[str, List["Flow"]] = {}
    by_dst: Dict[str, List["Flow"]] = {}
    for f in flows:
        by_src.setdefault(f.src_id.lower(), []).append(f)
        by_dst.setdefault(f.dst_id.lower(), []).append(f)

    txt = "#########################################################\n"
    txt += "#  SOURCES                                               #\n"
    txt += "#########################################################\n"
    # â–¸ Puerto UDP Ãºnico por cliente (2000 + Ã­ndice del cliente)
    client_port_map: Dict[str, int] = {
        n: 2000 + i for i, n in enumerate(sorted(by_src.keys()))
    }
    seen_per_client: Dict[str, int] = defaultdict(int)
    for node, list_flows in by_src.items():
        txt += f"*.{node}.numApps           = {len(list_flows)}\n"
        txt += f"*.{node}.app[*].typename   = \"UdpSourceApp\"\n"
        txt += f"*.{node}.app[*].io.destAddress = \"srv1\"\n\n"

        for idx, fl in enumerate(list_flows):
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ NUEVO: etiqueta legible para el flujo â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # Usa el flow_id ('F0', 'F1'â€¦) en minÃºsculas  â†’  "f0", "f1", â€¦
            # Si se prefiere "video-1" basta con reemplazar la siguiente lÃ­nea
            display_name = fl.flow_id.lower()
            txt += f"*.{node}.app[{idx}].display-name   = \"{display_name}\"\n"

            # â‹ Offset = Â«Disponible (start time)Â» â‡’ op.start_time
            off_us = _first_tx_offset_us(schedule_res, fl)
            off_ms = _us_to_ms_str(off_us)
            
            # âŠ TamaÃ±o de trama = payload mostrado en Â«INFORMACIÃ“N DE FLUJOSÂ»
            #    (no restamos 54 B de cabeceras; es exactamente el valor listado).
            length = max(64, fl.payload)

            txt += f"*.{node}.app[{idx}].source.productionInterval = {fl.period*Î¼s*1_000:.0f}ms\n"
            txt += f"*.{node}.app[{idx}].source.initialProductionOffset = {off_ms}\n"
            txt += f"*.{node}.app[{idx}].source.packetLength = {length}B\n"

            dport = client_port_map[node]           # puerto fijo por cliente
            txt  += f"*.{node}.app[{idx}].io.destPort = {dport}\n"

            pcp = _pcp_for_flow(node, seen_per_client)
            txt += (f"*.{node}.app[{idx}].bridging.streamIdentifier.identifier.mapping = "
                    f"[{{stream: \"{fl.flow_id}\", packetFilter: expr(udp.destPort == {dport})}}]\n")
            txt += (f"*.{node}.app[{idx}].bridging.streamCoder.encoder.mapping = "
                    f"[{{stream: \"{fl.flow_id}\", pcp: {pcp}}}]\n\n")

    # ------- habilitar outgoing streams por cliente -----------
    for node in by_src.keys():
        txt += f"*.{node}.hasOutgoingStreams = true\n"

    # ------- habilitar egress shaping en cada switch ----------
    sw_nodes = [n.lower() for n, d in network.graph.nodes(data=True)
                if d.get("node_type") == "SW"]
    for sw in sw_nodes:
        txt += f"*.{sw}.hasEgressTrafficShaping = true\n"
        txt += (f"*.{sw}.bridging.directionReverser.reverser."
                f"excludeEncapsulationProtocols = [\"ieee8021qctag\"]\n")

    txt += "\n#########################################################\n"
    txt += "#  SINKS                                                 #\n"
    txt += "#########################################################\n"
    for node, list_flows in by_dst.items():
        txt += f"*.{node}.numApps         = {len(list_flows)}\n"
        txt += f"*.{node}.app[*].typename = \"UdpSinkApp\"\n"
        for idx, _ in enumerate(list_flows):
            txt += f"*.{node}.app[{idx}].io.localPort = {2000+idx}\n"
        txt += "\n"

    return txt

def _ini_gcl(
    gcl: Dict["Link", List[Tuple[int, int]]],
    network: "Network",
    gate_port: int = 0,           # â† puerto declarado en **.gateFilter
) -> str:
    """
    Genera la secciÃ³n de programaciÃ³n Time-Aware para cada puerto de switch.
    Si `gcl` estÃ¡ vacÃ­o simplemente devuelve cadena vacÃ­a.
    """
    if not gcl:
        return ""

    txt = "#########################################################\n"
    txt += "#  TIME-AWARE TRAFFIC SHAPING                           #\n"
    txt += "#########################################################\n"

    for link, table in gcl.items():
        src = link.link_id[0] if isinstance(link.link_id, tuple) else link.link_id.split('-')[0]
        dst = link.link_id[1] if isinstance(link.link_id, tuple) else link.link_id.split('-')[1]
        # Usa el mismo Ã­ndice que figura en **.gateFilter
        port = gate_port

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  UNA sola cola "video" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        txt += f"*.{src.lower()}.eth[{port}].macLayer.queue.numTrafficClasses = 1\n"
        txt += f"*.{src.lower()}.eth[{port}].macLayer.queue.*[0].display-name = \"video\"\n"

        # Gate Ãºnico TC0 â€“ usar **la tabla que ya imprimiÃ³ ResAnalyzer**.
        #     â®‘  No se vuelven a ordenar ni a recalcular tiempos.
        # âŠ  HiperperÃ­odo = m.c.m. de los perÃ­odos de *toda la red*
        import math as _m
        hyperperiod = 1
        for _f in network.flows:
            hyperperiod = _m.lcm(hyperperiod, _f.period)

        # â–º LA TABLA LLEGA TAL CUAL SE IMPRIMIO EN
        #   "TABLA GCL GENERADA (t, estado)"  â—„
        #   â”€ no se toca, sÃ³lo se ordena por tiempo.
        # â”€â”€ tabla "corta" que llega desde ResAnalyzer â”€â”€
        events = sorted(table, key=lambda x: x[0])  # ya viene filtrada

        # Ordenar la tabla generada y *reemplazar* su PRIMER registro
        #     por "abierto en 0 Âµs" (no se aÃ±ade, se sustituye).
        if events:
            events[0] = (0, 1)      # primer registro forzado
        else:
            events = [(0, 1)]       # salvaguarda: lista vacÃ­a

        # Elimina duplicados consecutivos (p.ej. â€¦, (0,1), (0,1), â€¦)
        compact: list[tuple[int, int]] = []
        for t, s in events:
            if not compact or compact[-1][1] != s:
                compact.append((t, s))

        durations = [(compact[(i+1)%len(compact)][0] - t) % hyperperiod
                     for i, (t, _) in enumerate(compact)]

        txt += f"*.{src.lower()}.eth[{port}].macLayer.queue.transmissionGate[0].offset = 0ms\n"
        txt += f"*.{src.lower()}.eth[{port}].macLayer.queue.transmissionGate[0].durations = "
        txt += f"[{', '.join(_us_to_ms_str(d) for d in durations)}]\n"

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        #  DEBUG   Comparar la tabla original (events) con la reconstruida
        #          a partir de las durations que vamos a escribir.
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        #
        # 1.  Tabla "original" (la que viene de ResAnalyzer) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        orig_table = sorted(table, key=lambda x: x[0])    # [(t,state), ...]

        # 2.  Reconstruir tabla a partir de durations  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        recon_table: list[tuple[int,int]] = []
        t_acc = 0
        state = 1                      # siempre arrancamos "abierto"
        for d in durations:
            recon_table.append((t_acc, state))
            t_acc = (t_acc + d) % hyperperiod
            state = 1 - state          # alternar 1â†”0

        # 3.  Normalizar y ordenar para comparar
        recon_norm = sorted(recon_table, key=lambda x: x[0])
        orig_norm  = orig_table

        # Si las tablas no coinciden, registra un mensaje breve solo en nivel DEBUG.
        # AsÃ­ evitamos saturar la salida estÃ¡ndar durante `ui/test.py`.
        if recon_norm != orig_norm:
            import logging
            logging.getLogger(__name__).debug(
                f"[GCL-CHECK] Diferencia detectada en {src}.eth[{port}] "
                "(detalles omitidos; habilita DEBUG para ver el diff)."
            )

    return txt


def write_ned(network: Network, outfile: os.PathLike, pkg: str, net_name: str) -> None:
    """Generate a .ned file representing *exactly* the passed network."""

    # â”€â”€ Simple â€“ we only need node names and edges.  Use a naÃ¯ve layout: â”€â”€
    # â€¢ Endâ€‘stations on the left (x â‰ˆ 300)  â€“ spread vertically in steps
    # â€¢ Switches   in the middle (x â‰ˆ 500)
    # â€¢ Servers    on the right (x â‰ˆ 700)

    es_nodes = [n for n, d in network.graph.nodes(data=True) if d.get("node_type") == "ES"]
    sw_nodes = [n for n, d in network.graph.nodes(data=True) if d.get("node_type") == "SW"]

    server_nodes = [n for n in es_nodes if n.startswith("SRV")]
    client_nodes = [n for n in es_nodes if n not in server_nodes]

    # Coordinates
    step = 80
    submod_lines: List[str] = []

    for i, name in enumerate(client_nodes):
        submod_lines.append(_ned_node_line(name.lower(), "TsnDevice", 300, 200 + i * step))

    for i, name in enumerate(sw_nodes):
        submod_lines.append(_ned_node_line(name.lower(), "TsnSwitch", 500, 250 + i * step))

    for i, name in enumerate(server_nodes):
        submod_lines.append(_ned_node_line(name.lower(), "TsnDevice",
                                         700, 250 + i * step))

    # Connections â€“ iterate original directed edges and create <--> pairs.
    conn_lines: List[str] = []
    added = set()
    for src, dst, edata in network.graph.edges(data=True):
        # Create deterministic id for unordered pair to avoid duplicates
        key = tuple(sorted((src, dst)))
        if key in added:
            continue
        added.add(key)

        # Mantener el mismo alias en las conexiones
        def _alias(n: str) -> str:
            return n.lower()
        conn_lines.append(
            f"        {_alias(src)}.ethg++ <--> EthernetLink <--> {_alias(dst)}.ethg++;\n"
        )
        
    net_name = Path(outfile).stem          # p.ej.  Â«unidirÂ»
    # Detectar bitrate (asumimos que todos los enlaces comparten valor)
    try:
        first_edge = next(iter(network.graph.edges(data=True)))
        link_rate = first_edge[2].get("link_rate", 100)
    except StopIteration:
        link_rate = 100

    ned = (
        f"package {pkg};\n\n"
        "import inet.node.ethernet.EthernetLink;\n"
        "import inet.node.contract.IEthernetNetworkNode;\n"
        "import inet.node.tsn.TsnDevice;\n"
        "import inet.node.tsn.TsnSwitch;\n"
        "import inet.networks.base.TsnNetworkBase;\n\n"
        f"network {net_name.lower()} extends TsnNetworkBase\n"  # Force lowercase here
        "{\n"
        "    parameters:\n"
        f"        *.eth[*].bitrate = default({link_rate}Mbps);\n"
        "    submodules:\n"
        + "".join(submod_lines)
        + "\n    connections:\n"
        + "".join(conn_lines)
        + "}\n"
    )

    Path(outfile).write_text(ned, encoding="utf-8")

def write_ini(
    network: Network,
    schedule_res: ScheduleRes,
    gcl_tables: Dict[Link, List[Tuple[int, int]]],
    outfile: os.PathLike,
    net_name: str,
    gate_port: int,
) -> None:
    """Generate an .ini file reproducing traffic & GCL schedule.
    
    This function never fails completely - if problems occur during generation,
    it will still produce at least a basic INI file with the [General] section.
    """
    try:
        flows = list(network.flows)
        num_flows = len(flows)
        
        parts: List[str] = [_ini_general(net_name, gate_port)]
        try:
            parts.append(_ini_flows(flows, schedule_res, network))
        except Exception as exc:
            import logging
            logging.warning(f"[omnet_export] Error generating FLOWS section: {exc}")
            parts.append(f"#  (ERROR generando secciÃ³n SOURCES / SINKS: {exc})\n")

        try:
            parts.append(_ini_gcl(gcl_tables, network, gate_port))
        except Exception as exc:
            import logging
            logging.warning(f"[omnet_export] Error generating GCL section: {exc}")
            parts.append(f"#  (ERROR generando secciÃ³n GCL: {exc})\n")

        Path(outfile).write_text("".join(parts), encoding="utf-8")
            
    except Exception as e:
        import logging
        logging.error(f"[omnet_export] Critical error generating INI file: {e}")
        # Ensure at least a minimal INI file is written
        minimal_ini = "[General]\nnetwork = inet.showcases.tsn.generated.GeneratedTsnNetwork\n"
        Path(outfile).write_text(minimal_ini, encoding="utf-8")



def export_omnet_files(network: Network, schedule_res: ScheduleRes, gcl_tables: Dict[Link, List[Tuple[int, int]]], label: str, out_dir: os.PathLike, gate_port:int=2) -> Tuple[Path, Path]:
    """Create *.ned & *.ini in *out_dir* (overwriting any previous version)."""
    os.makedirs(out_dir, exist_ok=True)

    # Create a subdirectory for this topology if it doesn't exist
    topo_dir = Path(out_dir) / label
    topo_dir.mkdir(exist_ok=True, parents=True)

    ned_path = topo_dir / f"{label}.ned"
    ini_path = topo_dir / f"{label}.ini"

    pkg = "inet.showcases.tsn.trafficshaping.Pruebas_tesis.Red_" + label
    net_name = label.lower()

    write_ned(network, ned_path, pkg, net_name)
    write_ini(network, schedule_res, gcl_tables, ini_path, net_name, gate_port)

    print(f"[omnet_export]  generated â†’ {ned_path}, {ini_path}")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    #  Resumen global de planificaciÃ³n (ahora SÃ con variables)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    total_flows = len(network.flows)
    scheduled_ids = {
        f.flow_id
        for ops in schedule_res.values()
        for f, _ in ops
    }                                        # â† Ãºnicos
    ok_flows = len(scheduled_ids)

    msg = f"Programados con Ã©xito: {ok_flows}/{total_flows} flujos"
    print(msg)           # â† visible siempre
    logging.info(msg)    # â† para quien tenga el logger a nivel INFO
    return ned_path, ini_path
