# ===================================

## ARCHIVO: __init__.py
## ==================================================



import path_setup  # Carga las rutas definidas

__version__ = "0.1.0"





## ARCHIVO: core/__init__.py
## ==================================================




## ARCHIVO: core/learning/__init__.py
## ==================================================

"""
Entornos y componentes de aprendizaje por refuerzo
"""

from .environment import NetEnv  # La misma exportaci√≥n para mantener compatibilidad





## ARCHIVO: core/learning/encoder.py
## ==================================================

import torch
import torch.nn as nn
import gymnasium as gym 

from stable_baselines3.common.torch_layers import BaseFeaturesExtractor

# ------------------------------------------------------------------
# Utilidades
# ------------------------------------------------------------------
class _FourierEncoding(nn.Module):
    """Devuelve [x, sin(2^k œÄx), cos(2^k œÄx)] para k < num_bands."""
    def __init__(self, num_bands: int = 4):
        super().__init__()
        self.register_buffer("freq_bands", 2 ** torch.arange(num_bands).float() * torch.pi)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Expande cada caracter√≠stica con pares seno/coseno a distintas
        frecuencias, preservando la forma (*batch*, features).
        """
        enc = [x]
        for f in self.freq_bands:          # type: ignore[attr-defined]
            enc.append(torch.sin(f * x))
            enc.append(torch.cos(f * x))
        return torch.cat(enc, dim=-1)


class _ResidualBlock(nn.Module):
    def __init__(self, dim: int, p: float = 0.1):
        super().__init__()
        self.net = nn.Sequential(
            nn.LayerNorm(dim),
            nn.SiLU(),
            nn.Linear(dim, dim),
            nn.SiLU(),
            nn.Dropout(p),
            nn.Linear(dim, dim),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return x + self.net(x)

# ------------------------------------------------------------------
# Extractores
# ------------------------------------------------------------------

class SimpleExtractor(BaseFeaturesExtractor):
    """MLP de referencia (64 dims)."""
    def __init__(self, observation_space: gym.spaces.Box):
        super().__init__(observation_space, features_dim=64)
        inp = observation_space.shape[0]
        self.network = nn.Sequential(
            nn.Linear(inp, 128), nn.ReLU(),
            nn.Linear(128, 128), nn.ReLU(),
            nn.Linear(128, 64),
        )

    def forward(self, obs: torch.Tensor):
        return self.network(obs)


class AdvancedExtractor(BaseFeaturesExtractor):
    """
    ¬∑ Codificaci√≥n de Fourier (4 bandas) ‚Üí 36 dims<br>
    ¬∑ Proyecci√≥n a 128 + 2 bloques residuales + LayerNorm/SiLU<br>
    Devuelve un vector de 128 dims.
    """
    def __init__(self, observation_space: gym.spaces.Box, num_bands: int = 4):
        super().__init__(observation_space, features_dim=128)
        self.encode = _FourierEncoding(num_bands)
        self.project = nn.Linear((1 + 2 * num_bands) * observation_space.shape[0], 128)
        self.blocks = nn.Sequential(
            _ResidualBlock(128), _ResidualBlock(128),
            nn.LayerNorm(128), nn.SiLU(),
        )

    def forward(self, obs: torch.Tensor) -> torch.Tensor:
        x = self.encode(obs)
        x = self.project(x)
        return self.blocks(x)


# ‚úî Hacemos que el entrenamiento utilice el extractor avanzado por defecto.
FeaturesExtractor = AdvancedExtractor



## ARCHIVO: core/learning/env_actions.py
## ==================================================

import math
import logging
from typing import Dict, List, Tuple, Any, Optional

import numpy as np
from core.network.operation import Operation
from core.network.net import Net

from core.learning.env_utils import SchedulingError, ErrorType, find_next_event_time, check_temp_operations

# --------------------------------------------------------------------------- #
# ‚Üí NUEVA filosof√≠a de GCL ‚Üê                                                 #
#   ‚ñ∏ Cada vez que un hop *espera* en el egress-switch                       #
#     se a√±aden **exactamente 2 entradas** (close n).                   #
#   ‚ñ∏ La longitud del ciclo no cambia (se mantiene en 1 para indicar que     #
#     gestionaremos las reglas como eventos independientes).                 #
# --------------------------------------------------------------------------- #

# Todas las utilidades relacionadas con la reserva din√°mica de GCL se han
# eliminado.  A partir de ahora la lista se calcula *a posteriori* en el
# analizador de resultados.

pass  # <-- mantener el archivo, sin l√≥gica de GCL

# --------------------------------------------------------------------------- #
# Funciones para la implementaci√≥n del m√©todo step                            #
# --------------------------------------------------------------------------- #
def process_step_action(self, action):
    """Procesa la acci√≥n en el m√©todo step"""
    # -------------------------------------------------------------- #
    # 0. Interpretar la acci√≥n con componentes reducidos              #
    # -------------------------------------------------------------- #
    # La acci√≥n ahora es un array con 3 componentes
    guard_factor_idx  = int(action[0])
    switch_gap_idx    = int(action[1])
    flow_selection    = int(action[2])
    
    # Convertir √≠ndices en valores reales para usar en el algoritmo
    offset_us = 0
    guard_factor_values = [0.5, 0.75, 1.0, 1.5, 2.0]
    guard_factor = guard_factor_values[guard_factor_idx]
    switch_gap_values = [0.5, 1.0, 1.5, 2.0]
    switch_gap = switch_gap_values[switch_gap_idx]
    
    # Registrar las decisiones del agente para visualizaci√≥n
    flow = self.current_flow()
    
    # Almacenar todas las decisiones del agente (sin gcl_strategy)
    self.agent_decisions = {
        'guard_factor': guard_factor,
        'switch_gap': switch_gap,
        'flow_selection': flow_selection
    }
    
    # NUEVO: M√©tricas de operaci√≥n para el an√°lisis
    self.last_operation_info = {
        'bandwidth_used': 0,
        'bandwidth_total': 0,
        'wait_breakdown': {
            'switch': 0,
            'gap': 0,
            'total': 0
        }
    }

    flow = self.current_flow()
    hop_idx = self.flow_progress[self.current_flow_idx]
    link = self.link_dict[flow.path[hop_idx]]
    gating = True
    trans_time = link.transmission_time(flow.payload)
    
    # Guard time ahora usa el factor elegido por el agente
    base_guard_time = link.interference_time()
    guard_time = base_guard_time * guard_factor
    
    # Registrar las decisiones del agente para visualizaci√≥n posterior
    self.guard_time_selected = guard_time
    self.switch_gap_selected = switch_gap

    # Si el ORIGEN del enlace es un switch ‚áí este hop ES un egress
    def _get_src(node_pair):
        return node_pair[0] if isinstance(node_pair, tuple) \
               else node_pair.split('-')[0]

    sw_src = _get_src(link.link_id)
    is_egress_from_switch = sw_src.startswith('S') and not sw_src.startswith('SRV')
    
    # Use fixed conservative GCL strategy
    gcl_strategy = 0
    
    return (flow, hop_idx, link, gating, trans_time,
            guard_time, guard_factor,               # ‚ûä  NUEVO
            offset_us, switch_gap, sw_src,
            is_egress_from_switch, gcl_strategy)



## ARCHIVO: core/learning/env_utils.py
## ==================================================

import math
import os
import random
import logging
from collections import defaultdict
from dataclasses import dataclass
from enum import Enum, auto
from typing import Optional, List, Dict, Any

import numpy as np
import gymnasium as gym
from gymnasium import spaces

from core.network.operation import Operation, check_operation_isolation
from core.network.net import Net, Network, generate_flows, generate_simple_topology, FlowGenerator, UniDirectionalFlowGenerator

# --------------------------------------------------------------------------- #
#  Definiciones de error                                                      #
# --------------------------------------------------------------------------- #
class ErrorType(Enum):
    PeriodExceed = auto()

class SchedulingError(Exception):
    def __init__(self, error_type: ErrorType, msg: str):
        super().__init__(f"Error: {msg}")
        self.error_type = error_type
        self.msg = msg

# Funciones auxiliares para NetEnv
def find_next_event_time(link_busy_until, switch_busy_until, current_time):
    """Encuentra el siguiente tiempo de evento programado despu√©s de current_time"""
    next_event_time = float('inf')
    
    # Buscar en todos los tiempos de ocupaci√≥n de enlaces
    for time in link_busy_until.values():
        if time > current_time and time < next_event_time:
            next_event_time = time
            
    # Buscar en todos los tiempos de ocupaci√≥n de switches
    for time in switch_busy_until.values():
        if time > current_time and time < next_event_time:
            next_event_time = time
    
    return next_event_time if next_event_time < float('inf') else None

def check_valid_link(link, operation, current_flow, links_operations):
    """Comprueba si una operaci√≥n es v√°lida en un enlace"""
    for f_rhs, op_rhs in links_operations[link]:
        offset = check_operation_isolation(
            (operation, current_flow.period), (op_rhs, f_rhs.period)
        )
        if offset is not None:
            return offset
    return None

def check_temp_operations(temp_operations, links_operations, current_flow):
    """Verifica todas las operaciones temporales"""
    for link, op in temp_operations:
        offset = check_valid_link(link, op, current_flow, links_operations)
        if offset is not None:
            return offset
    return None



## ARCHIVO: core/learning/environment.py
## ==================================================

import math
import os
import random
import logging
from collections import defaultdict
from dataclasses import dataclass
from enum import Enum, auto
from typing import Optional, List, Dict, Any

import numpy as np
import gymnasium as gym
from gymnasium import spaces

from tools.definitions import LOG_DIR
from core.network.operation import Operation, check_operation_isolation
from core.network.net import Net, Network, generate_flows, generate_simple_topology, FlowGenerator, UniDirectionalFlowGenerator

# Importar desde los m√≥dulos auxiliares
from core.learning.env_utils import ErrorType, SchedulingError, find_next_event_time
from core.learning.env_utils import check_valid_link, check_temp_operations
from core.learning.env_actions import process_step_action

# --------------------------------------------------------------------------- #
#  Entorno TSN / DRL                                                          #
# --------------------------------------------------------------------------- #
class NetEnv(gym.Env):
    """Entorno de simulaci√≥n TSN para aprendizaje por refuerzo."""
    
    # Usar la constante centralizada en Net
    MIN_SWITCH_GAP = Net.SWITCH_GAP_MIN

    @dataclass
    class GclInfo:
        gcl_cycle: int = 1
        gcl_length: int = 0

    # --------------------------------------------------------------------- #
    #  Inicializaci√≥n                                                       #
    # --------------------------------------------------------------------- #
    def __init__(self, network: Optional[Network] = None, 
                curriculum_enabled: bool = True,
                initial_complexity: float = 0.25,
                curriculum_step: float = 0.05) -> None:
        super().__init__()

        # --- Curriculum Learning Adaptativo ---
        self.curriculum_enabled = curriculum_enabled
        self.initial_complexity = initial_complexity 
        self.curriculum_step = curriculum_step
        self.current_complexity = initial_complexity
        self.consecutive_successes = 0
        self.original_flows = []  # Store the original complete set of flows
        
        # Si se proporciona una red, guardar su estructura original
        if network is not None:
            self.total_flows = len(network.flows)
            self.base_graph = network.graph
            # Guardar todos los flujos originales
            self.original_flows = list(network.flows)
            
            # Registro expl√≠cito para depuraci√≥n del modo curriculum
            self.logger = logging.getLogger(f"{__name__}.{os.getpid()}")
            self.logger.setLevel(logging.INFO)
            self.logger.info(f"Inicializando entorno con {self.total_flows} flujos (curriculum: {curriculum_enabled}, complejidad inicial: {initial_complexity})")
            
            if curriculum_enabled and initial_complexity < 1.0:
                # En modo curriculum, reducir el n√∫mero inicial de flujos
                active_flows = int(self.total_flows * self.initial_complexity)
                active_flows = max(5, active_flows)  # M√≠nimo 5 flujos para empezar
                
                # Seleccionar subset de flujos para el nivel de complejidad actual
                active_flows_list = self.original_flows[:active_flows]
                network = Network(network.graph, active_flows_list)
                self.logger.info(f"Modo curriculum ACTIVADO: usando {active_flows}/{self.total_flows} flujos inicialmente")
            else:
                # Si curriculum est√° desactivado o complejidad es 1.0, usar todos los flujos
                self.logger.info(f"Modo curriculum DESACTIVADO: usando todos los {self.total_flows} flujos")
                self.current_complexity = 1.0  # Forzar complejidad completa
        
        # Si no se entrega una red, construir topolog√≠a y flujos sencillos
        if network is None:
            g = generate_simple_topology()
            f = generate_flows(g, 10)
            network = Network(g, f)
            self.total_flows = len(network.flows)
            self.base_graph = g
            # Crear generador de flujos apropiado
            self.flow_generator = FlowGenerator(g)

        # --- Estructuras base -------------------------------------------- #
        self.graph = network.graph
        self.flows = list(network.flows)               # lista estable
        self.line_graph, self.link_dict = (
            network.line_graph,
            network.links_dict,
        )

        # --- Estados internos ------------------------------------------- #
        self.num_flows: int = len(self.flows)
        # Reloj de referencia global (solo para la observaci√≥n)
        # Se calcula siempre como el pr√≥ximo evento m√°s cercano
        self.global_time: int = 0
        self.flow_progress: List[int] = [0] * self.num_flows  # hop en curso de cada flujo
        self.flow_completed: List[bool] = [False] * self.num_flows
        self.flow_first_tx: List[int | None] = [None] * self.num_flows
        self.current_flow_idx: int = 0                 # para round‚Äërobin

        self.links_operations = defaultdict(list)

        #  üîÄ  Las estructuras ligadas a GCL ya no se utilizan.  Mantener
        #  √∫nicamente la planificaci√≥n de operaciones; el c√°lculo de las
        #  tablas se traslada al *ResAnalyzer*.

        self.temp_operations: List[tuple] = []         # op en construcci√≥n

        # üîπ Placeholder para mantener compatibilidad con c√≥digo heredado.
        #   Ya no se llena ni se usa, pero evita AttributeError.
        self.links_gcl: dict = {}

        # ‚è±Ô∏è  NUEVO: reloj "ocupado‚Äëhasta" por enlace
        #    (cu√°ndo queda libre cada enlace)
        self.link_busy_until = defaultdict(int)
        # ‚è±Ô∏è‚è±Ô∏è reloj "ocupado‚Äëhasta" por switch **s√≥lo para EGRESOS**
        self.switch_busy_until = defaultdict(int)
        
        # üìä NUEVO: Registro del √∫ltimo tiempo de llegada por switch
        # Para garantizar separaci√≥n m√≠nima entre paquetes
        self.switch_last_arrival = defaultdict(int)

        # ‚è≤Ô∏è  √öltimo instante en que **se cre√≥** (primer hop) un paquete
        #     ‚Äì solo se usa para imponer la separaci√≥n en el PRIMER enlace
        self.last_packet_start = -Net.PACKET_GAP_EXTRA

        # üö¶ NUEVO: secci√≥n cr√≠tica global ‚Äì "una sola cola"
        self.global_queue_busy_until = 0

        # --- Espacios de observaci√≥n y acci√≥n --------------------------- #
        self._setup_spaces()

        # --- Logger ------------------------------------------------------ #
        self.logger = logging.getLogger(f"{__name__}.{os.getpid()}")
        self.logger.setLevel(logging.INFO)

        # Cache: ¬´¬øes nodo final?¬ª
        self._es_node_cache: Dict[Any, bool] = {}
        
        # Variable para datos de operaci√≥n
        self.last_operation_info = {}
        self.agent_decisions = {}

        # Orden FIFO inmutable: simplemente el √≠ndice de creaci√≥n del flujo
        # (flows ya est√° en el mismo orden en que se generaron).
        self._fifo_order = list(range(self.num_flows))

    # ----------------------------------------------------------------- #
    #  Helper est√°tico (picklable) para muestrear la separaci√≥n global  #
    # ----------------------------------------------------------------- #
    @staticmethod
    def _next_packet_gap() -> int:
        """
        Devuelve la separaci√≥n entre paquetes (¬µs) delegando la l√≥gica
        √≠ntegramente a ``Net.sample_packet_gap``.  
        Esta versi√≥n elimina c√≥digo muerto y evita ramas nunca alcanzadas.
        """
        return Net.sample_packet_gap()

    # --------------------------------------------------------------------- #
    #  Configuraci√≥n de gymnasium                                           #
    # --------------------------------------------------------------------- #
    def _default_gcl_info(self):
        return self.GclInfo()

    def _setup_spaces(self):
        # MODIFICAR LA OBSERVACI√ìN: Incluir informaci√≥n de m√∫ltiples flujos (hasta 5)
        # Para cada flujo: [per√≠odo_norm, payload_norm, progreso, tiempo_espera_norm, es_seleccionable]
        # Adem√°s de las caracter√≠sticas globales originales
        FLUJOS_OBSERVABLES = 5  # N√∫mero de flujos que podemos observar a la vez
        FEATURES_POR_FLUJO = 5  # Caracter√≠sticas por flujo
        FEATURES_GLOBALES = 4   # Caracter√≠sticas globales (tiempo, ocupaci√≥n GCL, etc.)
        
        self.observation_space = spaces.Box(
            low=0.0, 
            high=1.0, 
            shape=(FEATURES_GLOBALES + FLUJOS_OBSERVABLES * FEATURES_POR_FLUJO,), 
            dtype=np.float32
        )
        
        # NUEVO: Almacenar constantes para uso posterior
        self.NUM_FLUJOS_OBSERVABLES = FLUJOS_OBSERVABLES
        self.FEATURES_POR_FLUJO = FEATURES_POR_FLUJO
        self.FEATURES_GLOBALES = FEATURES_GLOBALES

        # ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
        # ‚ïë  ESPACIO DE ACCI√ìN (3 DIMENSIONES)                                    ‚ïë
        # ‚ïë  0. Guard factor [0-4]                                                ‚ïë
        # ‚ïë  1. Gap m√≠nimo en switch [0-3]                                        ‚ïë
        # ‚ïë  2. Selecci√≥n de flujo candidato [0-4]                                ‚ïë
        # ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
        self.action_space = spaces.MultiDiscrete([
            5,                 # Guard factor
            4,                 # Gap m√≠nimo switch
            FLUJOS_OBSERVABLES # Selecci√≥n de flujo
        ])

    # --------------------------------------------------------------------- #
    #  Utilidades de selecci√≥n de flujo / enlace                            #
    # --------------------------------------------------------------------- #
    def select_next_flow_by_agent(self, flow_selection):
        """
        Permite que el agente RL seleccione el pr√≥ximo flujo a programar.
        
        Args:
            flow_selection: √çndice del flujo seleccionado por el agente (0-4)
        """
        # Usar el √≠ndice de flujo seleccionado por el agente si est√° disponible
        if hasattr(self, 'current_candidate_flows') and self.current_candidate_flows:
            if 0 <= flow_selection < len(self.current_candidate_flows):
                selected_idx = self.current_candidate_flows[flow_selection]
                if not self.flow_completed[selected_idx]:
                    self.current_flow_idx = selected_idx
                    return

        # Si la selecci√≥n directa falla, usar FIFO como fallback
        now = self.global_time
        chosen = None  # (idx, wait_time)

        for idx, done in enumerate(self.flow_completed):
            if done:
                continue
            prog = self.flow_progress[idx]
            if prog == 0 or prog >= len(self.flows[idx].path):
                continue
            next_link = self.flows[idx].path[prog]
            if not next_link[0].startswith('S'):
                continue

            prev_link_id = self.flows[idx].path[prog-1]
            prev_ops = self.links_operations[self.link_dict[prev_link_id]]
            if not prev_ops:
                continue  # el hop previo a√∫n no fue programado
            prev_op = prev_ops[-1][1]
            wait_time = now - prev_op.reception_time
            if chosen is None or wait_time > chosen[1]:
                chosen = (idx, wait_time)

        if chosen:
            self.current_flow_idx = chosen[0]
            return
                
        # Si no se encontr√≥ un flujo adecuado, buscar cualquier flujo no completado
        if self.flow_completed[self.current_flow_idx]:
            for idx, completed in enumerate(self.flow_completed):
                if not completed:
                    self.current_flow_idx = idx
                    return

    def current_flow(self):
        return self.flows[self.current_flow_idx]

    def current_link(self):
        idx = self.flow_progress[self.current_flow_idx]
        flow = self.current_flow()
        return self.link_dict[flow.path[idx]]

    # ----------------------------------------------------------------- #
    #  FIFO helper                                                      #
    # ----------------------------------------------------------------- #
    def _next_fifo_idx(self) -> int | None:
        """
        Devuelve el √≠ndice del **√∫nico** flujo que debe programarse ahora
        seg√∫n FIFO estricto (el que m√°s tiempo lleva esperando).
        Si no hay flujos pendientes, devuelve None.
        
        Prioridad: 
        1. Paquetes que est√°n esperando en un switch (para transmisi√≥n inmediata)
        2. Paquetes con mayor tiempo de espera
        """
        best: tuple[int, int, bool, int] | None = None   # (wait_time, -idx, is_in_switch, hop_idx)
        best_idx: int | None = None

        now = self.global_time
        for idx, done in enumerate(self.flow_completed):
            if done:
                continue
            hop_idx = self.flow_progress[idx]
            if hop_idx >= len(self.flows[idx].path):
                continue

            # --- tiempo desde que el paquete est√° "listo" ---
            if hop_idx == 0:
                ready_t = 0                          # nunca se ha transmitido
                is_in_switch = False
            else:
                prev_link_id = self.flows[idx].path[hop_idx - 1]
                prev_ops = self.links_operations.get(self.link_dict[prev_link_id], [])
                if not prev_ops:
                    continue        # hop previo a√∫n no programado ‚áí no listo
                
                prev_op = prev_ops[-1][1]
                ready_t = prev_op.reception_time
                
                # Determinar si el paquete est√° esperando en un switch
                # Si el destino del enlace anterior es un switch y el origen del siguiente enlace
                # coincide con ese switch, entonces el paquete est√° esperando en un switch
                dst_node = prev_link_id[1] if isinstance(prev_link_id, tuple) else prev_link_id.split('-')[1]
                is_in_switch = dst_node.startswith('S') and not dst_node.startswith('SRV')

            wait = now - ready_t
            fifo_rank = -idx               # menor √≠ndice ‚áí m√°s antiguo
            
            # Prioridad: 1) paquetes en switch, 2) tiempo de espera, 3) √≠ndice m√°s bajo
            cand = (int(is_in_switch), wait, fifo_rank, hop_idx)

            if (best is None) or (cand > best):
                best = cand
                best_idx = idx

        return best_idx

    # --------------------------------------------------------------------- #
    #  Reinicio                                                             #
    # --------------------------------------------------------------------- #
    def reset(self, *, seed=None, options=None):
        super().reset(seed=seed)
        
        # Si es necesario incrementar la complejidad
        if self.curriculum_enabled and self.consecutive_successes >= 3:
            if self.increase_complexity():
                # Calcular n√∫mero de flujos seg√∫n complejidad actual
                num_flows = int(self.total_flows * self.current_complexity)
                num_flows = max(5, min(self.total_flows, num_flows))
                
                if self.original_flows:
                    # Usar un subset consistente de los flujos originales
                    active_flows_list = self.original_flows[:num_flows]
                    network = Network(self.base_graph, active_flows_list)
                    
                    # Actualizar las estructuras con la nueva red
                    self.graph = network.graph
                    self.flows = list(network.flows)
                    self.line_graph, self.link_dict = (
                        network.line_graph,
                        network.links_dict,
                    )
                    self.num_flows = len(self.flows)
                    
                    self.logger.info(f"Curriculum: Incrementando a {num_flows}/{self.total_flows} flujos (complejidad: {self.current_complexity:.2f})")
                
            self.consecutive_successes = 0

        self.global_time = 0
        self.flow_progress = [0] * self.num_flows
        self.flow_completed = [False] * self.num_flows
        self.flow_first_tx = [None] * self.num_flows
        self.current_flow_idx = 0

        self.links_operations.clear()
        self.links_gcl = {}   # reiniciar placeholder
        self.temp_operations.clear()
        self._es_node_cache.clear()
        self.link_busy_until.clear()
        self.switch_busy_until.clear()
        self.switch_last_arrival.clear()    # NUEVO: Limpiar tiempos de llegada
        self.global_queue_busy_until = 0
        self.last_packet_start = -Net.PACKET_GAP_EXTRA
                                           
        return self._get_observation(), {}

    # --------------------------------------------------------------------- #
    #  Observaci√≥n                                                          #
    # --------------------------------------------------------------------- #
    def _get_observation(self):
        # Creamos una observaci√≥n que contenga informaci√≥n sobre m√∫ltiples flujos
        # La observaci√≥n tendr√°: [caracter√≠sticas_globales, caracter√≠sticas_flujo1, caracter√≠sticas_flujo2, ...]
        
        # 1. Caracter√≠sticas globales de la red
        global_time_norm = self.global_time / 10000  # Normalizar tiempo global
        
        # Ya no hay GCL din√°mico ‚Üí utilizaci√≥n 0 siempre
        gcl_util_norm = 0.0
        
        # Calcular porcentaje de flujos completados
        completion_rate = sum(self.flow_completed) / self.num_flows
        
        # Nivel de curriculum
        curriculum_norm = self.current_complexity
        
        # Vector de caracter√≠sticas globales
        global_features = [global_time_norm, gcl_util_norm, completion_rate, curriculum_norm]
        
        # 2. Obtener una lista de flujos candidatos para programar
        candidatos = []
        for idx, completed in enumerate(self.flow_completed):
            if not completed:
                flow = self.flows[idx]
                hop_idx = self.flow_progress[idx]
                
                # Verificar que el flujo tiene un hop v√°lido para programar
                if hop_idx >= len(flow.path):
                    continue
                    
                # Calcular cu√°nto tiempo ha estado esperando (si aplica)
                wait_time = 0
                if hop_idx > 0:
                    prev_link_id = flow.path[hop_idx-1]
                    prev_ops = self.links_operations.get(self.link_dict[prev_link_id], [])
                    if prev_ops:
                        prev_op = prev_ops[-1][1]
                        wait_time = max(0, self.global_time - prev_op.reception_time)
                
                # Normalizar valores - convertir a caracter√≠sticas significativas 
                period_norm = flow.period / 10000  # Per√≠odos m√°s cortos ‚Üí valores m√°s peque√±os
                payload_norm = flow.payload / Net.MTU  # Payloads m√°s peque√±os ‚Üí valores m√°s peque√±os
                hop_progress = self.flow_progress[idx] / len(flow.path)
                wait_time_norm = min(wait_time / flow.period, 1.0)  # Normalizar por periodo
                
                # Calcular urgencia basada en plazo pr√≥ximo
                # Cuanto menor sea el tiempo hasta el deadline, mayor urgencia (1.0 = muy urgente)
                deadline_remaining = (flow.period - (self.global_time % flow.period)) / flow.period
                urgency = 1.0 - deadline_remaining  # 0.0 = acaba de empezar, 1.0 = casi vencido
                
                # Log del flujo candidato con sus caracter√≠sticas para depuraci√≥n
                self.logger.debug(f"Candidato: {flow.flow_id}, Period: {period_norm:.2f}, Payload: {payload_norm:.2f}, Wait: {wait_time_norm:.2f}, Urgency: {urgency:.2f}")
                
                # A√±adir a candidatos con sus caracter√≠sticas
                candidatos.append((idx, [period_norm, payload_norm, hop_progress, wait_time_norm, urgency]))
        
        # 3. Si no hay candidatos, devolver una observaci√≥n con ceros
        if not candidatos:
            return np.zeros(self.observation_space.shape, dtype=np.float32)
        
        # 4. Ordenar candidatos de forma significativa para el agente
        # Considerar m√∫ltiples factores: tiempo espera (FIFO), urgencia, tama√±o payload
        candidatos = sorted(candidatos, key=lambda x: (
            x[1][3],  # Tiempo de espera (mayor primero - FIFO)
            x[1][4],  # Urgencia (mayor primero)
            -x[1][1]  # Payload (menor primero - m√°s r√°pido)
        ), reverse=True)
        
        # Asegurar que tenemos exactamente NUM_FLUJOS_OBSERVABLES candidatos
        if len(candidatos) > self.NUM_FLUJOS_OBSERVABLES:
            candidatos = candidatos[:self.NUM_FLUJOS_OBSERVABLES]
        
        while len(candidatos) < self.NUM_FLUJOS_OBSERVABLES:
            candidatos.append((-1, [0.0, 0.0, 0.0, 0.0, 0.0]))
        
        # 5. Actualizar los √≠ndices de flujos candidatos para recuperarlos despu√©s
        self.current_candidate_flows = [idx for idx, _ in candidatos if idx >= 0]
        
        # Registro para depuraci√≥n
        if self.current_candidate_flows:
            self.logger.debug(f"Candidatos ordenados: {[self.flows[idx].flow_id for idx in self.current_candidate_flows]}")
        
        # 6. Construir la observaci√≥n completa concatenando caracter√≠sticas
        obs = np.array(global_features + [feat for _, feats in candidatos for feat in feats], dtype=np.float32)
        
        return obs

    # ------------------------------------------------------------------ #
    #  M√°scaras de acci√≥n                                                #
    # ------------------------------------------------------------------ #
    def action_masks(self):
        """
        Genera m√°scaras para el espacio de acci√≥n MultiDiscreto.
        """
        # Calcular tama√±o total de la m√°scara sumando todas las dimensiones
        mask_size = sum(self.action_space.nvec)
        
        # Crear una m√°scara plana donde todas las acciones est√°n permitidas (0 = permitida)
        mask = np.zeros(mask_size, dtype=np.int8)
        
        try:
            if self.current_flow_idx < len(self.flows) and not self.flow_completed[self.current_flow_idx]:
                flow = self.current_flow()
                hop_idx = self.flow_progress[self.current_flow_idx]
                
                if hop_idx < len(flow.path):
                    link = self.link_dict[flow.path[hop_idx]]
                    
                    # Si es el primer hop, los factores de guard time altos podr√≠an desperdiciarse
                    if hop_idx == 0:
                        # √çndice para guard time alto
                        offset = 0  # No offset needed anymore
                        mask[offset + 3] = 1  # Desactivar valores 3 y 4 (factores altos)
                        mask[offset + 4] = 1
        except Exception as e:
            self.logger.warning(f"Error generando m√°scaras de acci√≥n: {e}")
            
        return mask

    # --------------------------------------------------------------------- #
    #  Comprobaciones de aislamiento y GCL                                  #
    # --------------------------------------------------------------------- #
    def _is_es_source(self, link_id):
        """
        Devuelve True si el **origen** de link_id es una End-Station (ES).
        1) Primero consulta el atributo ``node_type`` del grafo.
        2) Si no existe, usa la convenci√≥n de nombres:
           E*, C*  = clientes/ES gen√©ricas  
           SRV*    = servidores (tambi√©n ES)  
           S*      = switches
        """
        if link_id in self._es_node_cache:
            return self._es_node_cache[link_id]

        src = link_id[0] if isinstance(link_id, tuple) else link_id.split('-')[0]

        # a) Metadata del grafo (m√°s robusta)
        if self.graph.nodes.get(src, {}).get("node_type") == "ES":
            self._es_node_cache[link_id] = True
            return True

        # b) Convenci√≥n de nombres
        is_es = src.startswith(("E", "C", "SRV"))
        self._es_node_cache[link_id] = is_es
        return is_es

    #  ‚õî  Retirado.  La agrupaci√≥n de GCL dej√≥ de ser necesaria.

    def _check_valid_link(self, link, operation):
        return check_valid_link(link, operation, self.current_flow(), self.links_operations)

    def _check_temp_operations(self):
        return check_temp_operations(self.temp_operations, self.links_operations, self.current_flow())

    def _find_next_event_time(self, current_time):
        """Encuentra el siguiente tiempo de evento programado despu√©s de current_time"""
        return find_next_event_time(self.link_busy_until, self.switch_busy_until, current_time)

    # M√©todo obsoleto: generaci√≥n din√°mica de GCL eliminada.
    # Se mantiene para compatibilidad pero no hace nada y devuelve False.
    def add_gating_with_grouping(self, *args, **kwargs):
        return False

    def increase_complexity(self):
        """Incrementa el nivel de dificultad del entorno"""
        if not self.curriculum_enabled or self.current_complexity >= 1.0:
            return False
            
        # Incrementar complejidad gradualmente
        self.current_complexity = min(1.0, self.current_complexity + self.curriculum_step)
        return True

    # --------------------------------------------------------------------- #
    #  M√©todo principal step() - delegado al m√≥dulo environment_impl         #
    # --------------------------------------------------------------------- #
    from core.learning.environment_impl import step  # (sigue igual, pero sin GCL)

    # --------------------------------------------------------------------- #
    #  gym stubs                                                             #
    # --------------------------------------------------------------------- #
    def render(self):
        pass

    def close(self):
        pass



## ARCHIVO: core/learning/environment_impl.py
## ==================================================

import math
import numpy as np
from core.learning.env_utils import ErrorType, SchedulingError, find_next_event_time
from core.network.net import Net
from core.network.operation import Operation
# import directo (la funci√≥n sigue existiendo)
from core.learning.env_actions import process_step_action
# Add missing import for logging
import logging

def step(self, action):
    """
    Realiza un paso en el entorno seg√∫n la acci√≥n proporcionada.
    
    Este m√©todo implementa la l√≥gica principal para:
    - Procesar la acci√≥n del agente
    - Calcular tiempos de transmisi√≥n
    - Manejar conflictos
    - Actualizar el estado del entorno
    - Calcular la recompensa
    
    Args:
        action: Acci√≥n multidimensional del agente RL
        
    Returns:
        Tuple: (observaci√≥n, recompensa, terminado, truncado, info)
    """
    try:
        # NUEVO: Extraer la selecci√≥n de flujo de la acci√≥n y aplicarla ANTES de procesar
        flow_selection = int(action[-1])  # La √∫ltima dimensi√≥n es la selecci√≥n de flujo
        
        # IMPORTANTE: Aplicar la selecci√≥n de flujo inmediatamente
        flow_reward_adj = 0.0
        original_flow_idx = self.current_flow_idx  # Guardar para comprobar si cambi√≥
        
        if hasattr(self, 'current_candidate_flows') and self.current_candidate_flows:
            if 0 <= flow_selection < len(self.current_candidate_flows):
                selected_idx = self.current_candidate_flows[flow_selection]
                if not self.flow_completed[selected_idx]:
                    # Verificar que el flujo seleccionado tiene un hop v√°lido para programar
                    if self.flow_progress[selected_idx] < len(self.flows[selected_idx].path):
                        self.current_flow_idx = selected_idx
                        
                        # A√±adir informaci√≥n de debug para seguimiento
                        self.logger.info(f"Agente seleccion√≥ flujo {self.flows[selected_idx].flow_id} (√≠ndice {selected_idx}) de candidatos: {[self.flows[idx].flow_id for idx in self.current_candidate_flows]}")
                        
                        # Evaluar si la selecci√≥n fue buena bas√°ndose en caracter√≠sticas
                        selected_flow = self.flows[selected_idx]
                        
                        # Calcular recompensa por selecci√≥n inteligente
                        period_factor   = 1.0 - (selected_flow.period / 10000)
                        payload_factor  = 1.0 - (selected_flow.payload / Net.MTU)
                        remaining       = len(selected_flow.path) - self.flow_progress[selected_idx]
                        progress_factor = remaining / len(selected_flow.path)

                        flow_reward_adj = (period_factor * 0.15 +
                                           payload_factor * 0.15 +
                                           progress_factor * 0.15)
                        
                        self.logger.debug(f"Flow selection: {flow_selection} ‚Üí candidate #{selected_idx} (Flow {selected_flow.flow_id})")
                        self.logger.debug(f"Flow reward adjustment: +{flow_reward_adj:.4f}")
        
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        #  ENFORCE PRIORITY SCHEDULING - IMMEDIATE FORWARDING FROM SWITCHES
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        fifo_idx = self._next_fifo_idx()
        if fifo_idx is None:
            # No quedan flujos pendientes ‚Äì deber√≠a terminar normalmente
            return self._get_observation(), 0, True, False, {"success": True}

        if self.current_flow_idx != fifo_idx:
            # Priorizar la transmisi√≥n desde switches
            prev_flow = self.flows[self.current_flow_idx]
            new_flow = self.flows[fifo_idx]
            prev_hop_idx = self.flow_progress[self.current_flow_idx]
            new_hop_idx = self.flow_progress[fifo_idx]
            
            # Verificar si es un cambio a un flujo que est√° en un switch esperando
            if new_hop_idx > 0:
                prev_link_id = new_flow.path[new_hop_idx - 1]
                dst_node = prev_link_id[1] if isinstance(prev_link_id, tuple) else prev_link_id.split('-')[1]
                is_at_switch = dst_node.startswith('S') and not dst_node.startswith('SRV')
                
                if is_at_switch:
                    self.logger.debug(f"Priorizando transmisi√≥n inmediata desde switch: flujo {new_flow.flow_id}")
            
            # Actualizar el flujo actual
            self.current_flow_idx = fifo_idx

        # A partir de aqu√≠ TODO el c√≥digo sigue igual: ya trabajamos con el
        # flujo correcto; si posteriormente no cabe en el per√≠odo, se lanzar√°
        # el SchedulingError habitual y el episodio terminar√° "con error".
        
        # Procesar la acci√≥n y obtener la informaci√≥n necesaria
        # ‚ûã  Desestructuramos el nuevo elemento `guard_factor`
        (flow, hop_idx, link, gating, trans_time,
         guard_time, guard_factor,                # ‚Üê aqu√≠
         offset_us, switch_gap, sw_src,
         is_egress_from_switch, gcl_strategy) = process_step_action(self, action)

        # ------------------------------------------------------------ #
        # 1. Calcular tiempos                                          #
        # ------------------------------------------------------------ #
        if hop_idx == 0:        # ---------- primer hop ----------
            # Si no se entrega una red, construir topolog√≠a y flujos sencillos
            if self.flow_first_tx[self.current_flow_idx] is None:
                self.flow_first_tx[self.current_flow_idx] = self.global_time
            # ‚ù∂  Primer hop: basta con que el enlace est√© libre
            # ‚û°Ô∏è  S√≥lo el *primer* enlace respeta Net.PACKET_GAP_EXTRA
            earliest = max(
                self.link_busy_until[link],
                self.global_queue_busy_until,
                self.last_packet_start + self._next_packet_gap()
            )  # Removed offset_us
            # Obtener el switch de destino para este paquete
            dst_node = link.link_id[1] if isinstance(link.link_id, tuple) else link.link_id.split('-')[1]
            if dst_node.startswith('S'):
                # Asegurar separaci√≥n m√≠nima de 1Œºs entre llegadas al switch
                # Calcula el tiempo de llegada *potencial* al switch
                potential_arrival_time = earliest + trans_time + Net.DELAY_PROP
                min_arrival_time = self.switch_last_arrival[dst_node] + switch_gap
                if potential_arrival_time < min_arrival_time:
                    # Ajustar el tiempo de inicio para garantizar 1Œºs de separaci√≥n en destino
                    delay = min_arrival_time - potential_arrival_time
                    earliest += delay
                    # Actualizar el tiempo de llegada real
                    arrival_time = min_arrival_time
                else:
                    arrival_time = potential_arrival_time
                # Actualizar el √∫ltimo tiempo de llegada registrado para este switch
                self.switch_last_arrival[dst_node] = arrival_time

            # Re-calcular la ventana tras cualquier retraso aplicado
            latest = earliest + Net.SYNC_ERROR

            # Para el primer hop desde ES, no hay gating. Dequeue es el inicio m√°s temprano.
            offset   = 0            # sin margen de sincron√≠a
            dequeue  = earliest     # comienza tan pronto el enlace queda libre
            end_time = dequeue + trans_time
            if end_time > flow.period:
                 # Primer hop (sale de una ES): no hay switch para crear espera
                 raise SchedulingError(ErrorType.PeriodExceed, "Excedi√≥ per√≠odo")

            # --- Crear objeto Operation para hop_idx == 0 ---
            op_start_time = earliest
            op_gating_time = None # No hay gating desde ES
            op_latest_time = latest # Usamos latest calculado
            op_end_time = end_time
            # Crear la operaci√≥n AHORA para que 'op' est√© definida
            op = Operation(op_start_time, op_gating_time, op_latest_time, op_end_time)

            # ‚ûï GUARDAR datos que el visualizador necesita
            op.guard_factor      = guard_factor      # decisi√≥n RL
            op.min_gap_value     = switch_gap        # decisi√≥n RL
            op.guard_time        = guard_time        # longitud real del guard-band

        else:                   # ---------- hops siguientes ----------
            # ‚ù∑  Resto de hops:
            prev_link_id = flow.path[hop_idx - 1]
            prev_link = self.link_dict[prev_link_id]
            prev_op = self.links_operations[prev_link][-1][1]

            # Tiempo base: cuando el paquete est√° listo en el nodo actual
            packet_ready_time = prev_op.reception_time

            # Earliest possible start considerando s√≥lo llegada y disponibilidad del ENLACE
            # ‚ö†Ô∏è  En hops posteriores **no** aplicamos la separaci√≥n global:
            earliest_possible_start_on_link = max(packet_ready_time,
                                         self.link_busy_until[link],
                                         self.global_queue_busy_until)  # Removed offset_us

            # Earliest start considerando tambi√©n la disponibilidad del PUERTO del SWITCH (si aplica)
            if is_egress_from_switch:
                # FCFS se mantiene, pero no registramos la espera (no la decide el agente)
                final_earliest_start = max(earliest_possible_start_on_link,
                                            self.switch_busy_until[sw_src])
            else:
                final_earliest_start = earliest_possible_start_on_link

            # Calcular la ventana 'latest' basada en el inicio m√°s temprano real
            latest = final_earliest_start + Net.SYNC_ERROR

            # Determinar el tiempo real de DEQUEUE (inicio de transmisi√≥n)
            offset   = 0            # sin margen de sincron√≠a
            dequeue  = final_earliest_start
            
            # Calcular tiempo de fin de transmisi√≥n
            end_time = dequeue + trans_time
            if end_time > flow.period:
                # Si no cabe en su periodo, abortar sin intentos de recolocaci√≥n
                raise SchedulingError(ErrorType.PeriodExceed, "Excedi√≥ per√≠odo")

            # --- Crear objeto Operation ---
            # start_time: Cu√°ndo podr√≠a haber empezado (llegada + disponibilidad enlace)
            # gating_time: Cu√°ndo empez√≥ realmente (dequeue), si aplica gating
            # latest_time: L√≠mite superior de la ventana para gating
            # end_time: Cu√°ndo termin√≥ la transmisi√≥n
            op_start_time = earliest_possible_start_on_link
            op_gating_time = dequeue if gating and is_egress_from_switch else None 
            # Importante: Si hay gating, latest_time debe ser igual a gating_time
            if gating and is_egress_from_switch:
                op_latest_time = op_gating_time  # Si hay gating, ambos deben ser iguales
            else:
                op_latest_time = latest  # Sin gating, latest_time mantiene su valor normal

            op_end_time = end_time

            op = Operation(op_start_time, op_gating_time, op_latest_time, op_end_time)

            # ‚ûï Actualizar tambi√©n en la rama "hops > 0"
            op.guard_factor  = guard_factor
            op.min_gap_value = switch_gap
            op.guard_time    = guard_time
            # No guardamos esperas que no sean decisi√≥n del agente

            # ‚îÄ‚îÄ Nuevo: garantizar separaci√≥n m√≠nima entre llegadas al switch destino ‚îÄ‚îÄ
            dst_node = link.link_id[1] if isinstance(link.link_id, tuple) \
                       else link.link_id.split('-')[1]
            if dst_node.startswith('S'):                       # s√≥lo switches reales
                arrival = op.reception_time - Net.DELAY_PROC_RX
                min_arrival = self.switch_last_arrival[dst_node] + switch_gap
                if arrival < min_arrival:
                    delay = min_arrival - arrival
                    op.add(delay)              # ajusta *todos* los tiempos de la operaci√≥n
                    op.min_gap_wait += delay   # registrar espera por gap m√≠nimo
                    dequeue   += delay
                    end_time  += delay
                    op_start_time += delay
                    op_end_time   += delay
                    arrival = min_arrival
                # Registrar llegada para el siguiente paquete
                self.switch_last_arrival[dst_node] = arrival

        # --- Regla *un‚Äësolo‚Äëpaquete‚Äëswitch* ---
        # 2. Crear operaci√≥n temporal                                  #
        # ------------------------------------------------------------ #
        # op ya est√° creado con los tiempos correctos
        self.temp_operations.append((link, op))

        # Resolver conflictos por desplazamiento
        offset = self._check_temp_operations()
        max_iter = 16  # salvaguarda contra bucles infinitos
        while offset is not None and max_iter:
            # Desplazar la operaci√≥n seg√∫n el offset de conflicto
            op_start_time += offset
            
            # Actualizar TODAS las propiedades temporales
            if op_gating_time is not None:
                # Con gating, todo se desplaza por igual
                op_gating_time += offset
                op_latest_time += offset  # latest siempre alineado con gating
            else:
                # Sin gating, latest avanza con start (son independientes)
                op_latest_time += offset
            
            # Tiempo final siempre se recalcula respecto al inicio real
            op_end_time = (op_gating_time if op_gating_time is not None else op_start_time) + trans_time

            # Recrear la operaci√≥n con los nuevos tiempos
            op = Operation(op_start_time, op_gating_time, op_latest_time, op_end_time)
            
            # Si hay gating, validar que a√∫n est√° dentro del per√≠odo del flujo
            if op_gating_time is not None and op_end_time > flow.period:
                # El inicio real ocurrir√≠a despu√©s del final del per√≠odo
                raise SchedulingError(
                    ErrorType.PeriodExceed, 
                    "Flow cycles into next period"
                )
            
            # Recrear el array de operaciones temporales con la actualizada
            self.temp_operations = [(link, op)]
            
            # Volver a verificar conflictos
            offset = self._check_temp_operations()
            max_iter -= 1
            
            # Use default fixed conflict resolution strategy
            # (previous conflict_strategy action dimension was removed)
            # Apply a small minimum offset to ensure progress
            if offset is not None and offset == 0:
                offset = max(1, int(switch_gap * Net.SWITCH_GAP_MIN))

        if max_iter == 0 and offset is not None:
            raise SchedulingError(
                ErrorType.PeriodExceed,
                "Failed to resolve conflict after 16 iterations"
            )

        #  ‚õî  Ya no se generan ni reservan reglas GCL durante el scheduling.

        # ---------- REWARD SHAPING ----------
        GB_PEN      = 0.05   # guard-band (s√≠ la decide RL)

        reward = 1.0
        reward -= GB_PEN * (guard_time / flow.period)

        # NUEVO: A√±adir el ajuste de recompensa por selecci√≥n inteligente de flujo
        reward += flow_reward_adj

    except SchedulingError as e:
        self.logger.info(f"Fallo: {e.msg} (flujo {self.current_flow_idx})")
        return self._get_observation(), -1, True, False, {"success": False}

    # ------------------------------------------------------------ #
    # 3. Avanzar progreso del flujo                                #
    # ------------------------------------------------------------ #
    self.links_operations[link].append((flow, op))
    self.temp_operations.clear()

    # üåê Registrar s√≥lo si es el *primer* hop del flujo
    if hop_idx == 0:
        self.last_packet_start = op_start_time

    # ‚ù∑  Marcar el enlace como ocupado hasta que el paquete est√© completamente recibido Y PROCESADO
    # Usar reception_time que ya incluye DELAY_PROP + DELAY_PROC_RX
    self.link_busy_until[link] = op.reception_time  # En lugar de op.end_time + Net.DELAY_PROP
    # üîí Mantener la secci√≥n cr√≠tica ocupada hasta que el frame se recibe
    self.global_queue_busy_until = op.reception_time

    if is_egress_from_switch:                 # ‚ù∑ liberar switch al terminar
        # El switch se considera ocupado solo hasta que termina de transmitir el paquete
        # Sin guard_time adicional para la ocupaci√≥n del switch
        self.switch_busy_until[sw_src] = op.end_time  # CORREGIDO: Eliminar guard_time
        
        # El guard_time solo afecta a cu√°ndo puede empezar otra transmisi√≥n por el mismo puerto,
        # pero el switch ya termin√≥ su trabajo con este paquete en end_time

    self.flow_progress[self.current_flow_idx] += 1

    # ‚ù∏  El "reloj" global se redefine como el evento m√°s temprano pendiente
    next_events = [*self.link_busy_until.values(),
                  *self.switch_busy_until.values()]
    # Si no quedan eventos pendientes, mantenemos el reloj en lugar de "rebobinar" a 0
    self.global_time = min(next_events, default=self.global_time)

    # ¬øTermin√≥ este flujo?
    if self.flow_progress[self.current_flow_idx] == len(flow.path):
        self.flow_completed[self.current_flow_idx] = True
        # ---------- verificaci√≥n latencia extremo-a-extremo ----------
        fst = self.flow_first_tx[self.current_flow_idx]
        e2e_latency = op.reception_time - fst if fst is not None else 0
        
        # NUEVO: Guardar latencia e2e para an√°lisis
        self.last_operation_info['e2e_latency'] = e2e_latency
        
        # *Incluir* la propagaci√≥n y el procesamiento de RX en el presupuesto
        # üí° Tomar el peor‚Äëcaso acumulativo sobre la ruta completa
        hops = len(flow.path)
        e2e_budget = (flow.e2e_delay +                      # presupuesto nominal
                      Net.DELAY_PROP   * hops +            # propagaci√≥n
                      Net.DELAY_PROC_RX * hops +           # procesado RX
                      guard_time        * (hops - 1))      # guard‚Äëband por hop
        if e2e_latency > e2e_budget:
            raise SchedulingError(ErrorType.PeriodExceed,
                                  f"E2E delay {e2e_latency} > {e2e_budget}")
        reward += 2

    # ¬øTermin√≥ episodio?
    done = all(self.flow_completed)
    
    # Despu√©s de procesar un hop de un flujo, si el destino es un switch,
    # inmediatamente preparar el siguiente hop para transmisi√≥n
    if not done and hop_idx < len(flow.path) - 1:
        dst_node = link.link_id[1] if isinstance(link.link_id, tuple) else link.link_id.split('-')[1]
        if dst_node.startswith('S') and not dst_node.startswith('SRV'):
            # Este paquete lleg√≥ a un switch, marcar como alta prioridad
            # para ser procesado en el siguiente paso
            arrival_time = op.reception_time
            self.switch_last_arrival[dst_node] = min(arrival_time, self.switch_last_arrival[dst_node])
            
            # Actualizar el reloj global para favorecer el procesamiento inmediato
            # de este paquete que acaba de llegar al switch
            if arrival_time < self.global_time:
                next_events = [*self.link_busy_until.values(), *self.switch_busy_until.values(), arrival_time]
                self.global_time = min(next_events)

    # Gestionar el curriculum learning
    if done and all(self.flow_completed):
        # Episodio exitoso: incrementar contador de √©xitos consecutivos
        self.consecutive_successes += 1
        # A√±adir bonificaci√≥n de recompensa proporcional al nivel de complejidad
        reward += 5.0 * self.current_complexity
        
        # Mostrar informaci√≥n del progreso del curriculum
        if self.curriculum_enabled:
            self.logger.info(f"√âxito con {len(self.flows)}/{self.total_flows} flujos (complejidad: {self.current_complexity:.2f}, √©xitos: {self.consecutive_successes}/3)")
    
    info = {
        "success": done,
        "ScheduleRes": self.links_operations.copy() if done else None,
        "curriculum_level": self.current_complexity,
        "num_flows": len(self.flows),
        # NUEVO: A√±adir informaci√≥n sobre selecci√≥n de flujos
        "flow_selection": {
            "current_flow_idx": self.current_flow_idx,
            "available_candidates": getattr(self, 'current_candidate_flows', []),
            "selected_option": flow_selection,
            "reward_adj": flow_reward_adj
        }
    }

    return self._get_observation(), reward, done, False, info



## ARCHIVO: core/learning/training_metrics.py
## ==================================================

# This file is intentionally empty. All metrics functionality has been removed.



## ARCHIVO: core/network/__init__.py
## ==================================================

"""
M√≥dulos de modelado de red y topolog√≠as
"""

from .net import Flow, Link, Net, Network, generate_graph, generate_flows, FlowGenerator
from .operation import Operation, check_operation_isolation





## ARCHIVO: core/network/net.py
## ==================================================

import logging
import math
import networkx as nx
import numpy as np
import random
import typing

# Conjunto de per√≠odos disponibles
PERIOD_SET = [2000, 4000, 8000]

class Net:
    # Configuraci√≥n de red y temporizaciones
    MTU = 1518
    # ‚û°Ô∏è Nueva: separaci√≥n m√≠nima entre llegadas sucesivas al mismo switch (¬µs)
    SWITCH_GAP_MIN = 1

    # ‚û°Ô∏è **NUEVO** ‚Äì par√°metros para la separaci√≥n probabil√≠stica entre creaciones
    #     de paquetes (s√≥lo se eval√∫a en el PRIMER hop de cada flujo)
    #
    #     ‚Ä¢ PACKET_GAP_MODE:
    #         ‚îÄ fixed        ‚Üí separaci√≥n constante = PACKET_GAP_EXTRA
    #         ‚îÄ uniform      ‚Üí U[min,max]  definido en PACKET_GAP_UNIFORM
    #         ‚îÄ exponential  ‚Üí Exp(Œª)  con media = PACKET_GAP_EXTRA
    #         ‚îÄ gaussian     ‚Üí N(Œº,œÉ¬≤)  definido en PACKET_GAP_GAUSS
    #         ‚îÄ pareto       ‚Üí Pareto(Œ±,xm) definido en PACKET_GAP_PARETO
    #     ‚Ä¢ Los valores se configuran desde CLI en ui/train.py / ui/test.py.
    #
    PACKET_GAP_MODE    = "fixed"
    PACKET_GAP_EXTRA   = 0
    PACKET_GAP_UNIFORM = (0, 0)       # s√≥lo usado si mode == uniform
    # Nuevos contenedores para gaussian y pareto
    PACKET_GAP_GAUSS   = (0.0, 1.0)   # Œº, œÉ
    PACKET_GAP_PARETO  = (2.0, 1.0)   # Œ±, x_m

    # ------------------------------------------------------------------
    #  Configuraci√≥n centralizada desde CLI
    # ------------------------------------------------------------------
    @staticmethod
    def set_gap_distribution(dist: str, params: list[float] | None = None):
        """
        Ajusta la distribuci√≥n global del gap entre creaciones de paquetes.

        ‚îÄ dist = fixed        ‚Üí params = [gap]
        ‚îÄ dist = uniform      ‚Üí params = [min, max]
        ‚îÄ dist = exponential  ‚Üí params = [mean]
        ‚îÄ dist = gaussian     ‚Üí params = [mean, sigma]
        ‚îÄ dist = pareto       ‚Üí params = [alpha, x_m]
        """
        params = params or []
        Net.PACKET_GAP_MODE = dist

        if dist == "fixed":
            Net.PACKET_GAP_EXTRA = int(params[0]) if params else 0

        elif dist == "uniform":
            assert len(params) == 2, "--dist-params necesita MIN MAX"
            Net.PACKET_GAP_UNIFORM = (int(params[0]), int(params[1]))

        elif dist == "exponential":
            Net.PACKET_GAP_EXTRA = int(params[0]) if params else 1

        elif dist == "gaussian":
            assert len(params) >= 2, "--dist-params necesita MU SIGMA"
            Net.PACKET_GAP_GAUSS = (float(params[0]), float(params[1]))

        elif dist == "pareto":
            assert len(params) == 2, "--dist-params necesita ALPHA XM"
            Net.PACKET_GAP_PARETO = (float(params[0]), float(params[1]))

        else:
            raise ValueError(f"Distribuci√≥n desconocida: {dist}")

    @staticmethod
    def sample_packet_gap() -> int:
        """Devuelve la separaci√≥n (¬µs) seg√∫n la configuraci√≥n global."""
        if Net.PACKET_GAP_MODE == "fixed":
            return Net.PACKET_GAP_EXTRA
        elif Net.PACKET_GAP_MODE == "uniform":
            lo, hi = Net.PACKET_GAP_UNIFORM
            return random.randint(lo, hi) if hi > lo else lo
        elif Net.PACKET_GAP_MODE == "exponential":
            Œº = max(Net.PACKET_GAP_EXTRA, 1)      # evita Œª=0
            return int(random.expovariate(1/Œº))
        elif Net.PACKET_GAP_MODE == "gaussian":
            Œº, œÉ = Net.PACKET_GAP_GAUSS
            # recortamos a ‚â•0 y redondeamos
            return max(0, int(random.gauss(Œº, œÉ)))
        elif Net.PACKET_GAP_MODE == "pareto":
            Œ±, xm = Net.PACKET_GAP_PARETO
            # random.paretovariate devuelve xm*(1-U)^(-1/Œ±) con xm=1
            return int(random.paretovariate(Œ±)*xm)
        else:
            raise ValueError(f"Modo de gap desconocido: {Net.PACKET_GAP_MODE}")
  
    DELAY_PROC_RX = 1  # Nuevo: tiempo de procesamiento de recepci√≥n
    SYNC_ERROR = 0  # Sin incertidumbre: relojes perfectamente alineados
    DELAY_PROP = 1
    GCL_CYCLE_MAX = 128000
    # Longitud m√°xima t√≠pica de un Gate Control List en hardware TSN (open/close)
    GCL_LENGTH_MAX = 256

class Link:
    embedding_length = 3

    def __init__(self, link_id, link_rate):
        self.link_id = link_id
        self.gcl_capacity = Net.GCL_LENGTH_MAX
        self.link_rate = link_rate

    def __hash__(self):
        return hash(self.link_id)

    def __eq__(self, other):
        return isinstance(other, Link) and self.link_id == other.link_id

    def __repr__(self):
        return f"Link{self.link_id}"

    def interference_time(self):
        return self.transmission_time(Net.MTU)

    def transmission_time(self, payload):
        # 12B para IFG, 8B para pre√°mbulo
        return math.ceil((payload + 12 + 8) * 8 / self.link_rate)

class Flow:
    def __init__(self, flow_id, src_id, dst_id, path, period=2000, payload=100, e2e_delay=None):
        self.flow_id = flow_id
        self.src_id = src_id
        self.dst_id = dst_id
        self.path = path
        # Validar per√≠odo
        assert period in PERIOD_SET, f"Per√≠odo inv√°lido {period}"
        self.period = period
        self.payload = payload
        self.e2e_delay = period if e2e_delay is None else e2e_delay
        # Jitter completamente eliminado

    def __hash__(self):
        return hash(self.flow_id)

    def __eq__(self, other):
        return isinstance(other, Flow) and self.flow_id == other.flow_id

    def __repr__(self):
        return f"Flow(id='{self.flow_id}', src='{self.src_id}', dst='{self.dst_id}', period={self.period})"

# Funciones para generar topolog√≠as simplificadas
def generate_graph(topo, link_rate=100):
    """Genera diferentes topolog√≠as de grafo seg√∫n el tipo especificado"""
    if topo == "SIMPLE":
        return generate_simple_topology(link_rate)
    elif topo == "UNIDIR":
        return generate_unidirectional_topology(link_rate)
    else:
        raise ValueError(f"Topolog√≠a desconocida: {topo}. Solo se soportan 'SIMPLE' y 'UNIDIR'")

def generate_simple_topology(link_rate: int = 100):
    """Genera una topolog√≠a S-E-S m√≠nima con identificadores **tuple**.

    Mantener el mismo tipo de ``link_id`` en toda la base de c√≥digo
    evita las m√∫ltiples llamadas a ``split('-')`` y simplifica las
    comprobaciones de si un nodo es *switch* o *end-station*.
    """
    graph = nx.DiGraph()
    graph.add_node("S1", node_type="SW")

    for node in ("C1", "C2", "SRV1"):
        graph.add_node(node, node_type="ES")
        # Enlaces bidireccionales (src, dst)
        graph.add_edge(node, "S1", link_id=(node, "S1"), link_rate=link_rate)
        graph.add_edge("S1", node, link_id=("S1", node), link_rate=link_rate)

    return graph

def generate_unidirectional_topology(link_rate=100):
    """Genera una topolog√≠a unidireccional donde los datos fluyen solo de clientes a servidor"""
    graph = nx.DiGraph()
    # Crear nodos
    graph.add_node("S1", node_type="SW")
    for node in ["C1", "C2", "SRV1"]:
        graph.add_node(node, node_type="ES")
    # Crear enlaces unidireccionales
    for client in ["C1", "C2"]:
        graph.add_edge(client, "S1", link_id=(client, "S1"), link_rate=link_rate)
    graph.add_edge("S1", "SRV1", link_id=("S1", "SRV1"), link_rate=link_rate)
    return graph

def _transform_line_graph(graph):
    """Transforma un grafo en su l√≠nea de grafo y crea diccionario de enlaces"""
    line_graph = nx.line_graph(graph)
    links_dict = {node: Link(node, graph.edges[node]['link_rate']) for node in line_graph.nodes}
    return line_graph, links_dict

class Network:
    def __init__(self, graph, flows):
        self.graph = graph
        self.flows = flows
        # Construir l√≠nea de grafo y diccionario de enlaces
        self.line_graph, self.links_dict = _transform_line_graph(graph)

    def disable_gcl(self, num_nodes):
        """Deshabilita la capacidad GCL para un n√∫mero espec√≠fico de nodos"""
        list_nodes = random.sample(list(self.graph.nodes), num_nodes)
        list_links = []
        for node in list_nodes:
            list_links.extend([link for link in self.links_dict.values() if node == link.link_id[0]])
        for link in list_links:
            link.gcl_capacity = 0

    def set_gcl(self, num_gcl):
        """Establece la capacidad GCL para todos los enlaces"""
        for link in self.links_dict.values():
            link.gcl_capacity = num_gcl

class FlowGenerator:
    def __init__(self, graph, seed=None, period_set=None, min_payload=1500, max_payload=1518): # A√±adir min/max payload
        self.graph = graph
        # Inicializar semilla para reproducibilidad
        if seed is not None:
            random.seed(seed)
        # Inicializar conjunto de per√≠odos usando el global PERIOD_SET como default
        self.period_set = period_set if period_set is not None else PERIOD_SET
        for period in self.period_set:
            assert isinstance(period, int) and period > 0
        # Jitters eliminado
        # Identificar nodos finales
        self.es_nodes = [n for n, d in graph.nodes(data=True) if d['node_type'] == 'ES']
        self.num_generated_flows = 0
        # Guardar rango de payload
        self.min_payload = min_payload
        self.max_payload = max_payload
        assert self.min_payload <= self.max_payload, "min_payload debe ser <= max_payload"

    def _generate_flow(self):
        """Genera un √∫nico flujo aleatorio"""
        # Seleccionar dos nodos aleatorios
        src_id, dst_id = random.sample(self.es_nodes, 2)
        # Calcular ruta m√°s corta
        path = nx.shortest_path(self.graph, src_id, dst_id)
        path = [(path[i], path[i+1]) for i in range(len(path)-1)]
        # Seleccionar per√≠odo aleatorio
        period = random.choice(self.period_set)

        # Crear flujo con payload aleatorio dentro del rango especificado
        flow = Flow(
            f"F{self.num_generated_flows}", src_id, dst_id, path,
            payload=random.randint(self.min_payload, self.max_payload), # Usar el rango
            period=period
        )
        self.num_generated_flows += 1
        return flow

    def __call__(self, num_flows=1):
        """Genera un conjunto de flujos aleatorios"""
        return [self._generate_flow() for _ in range(num_flows)]

class UniDirectionalFlowGenerator(FlowGenerator):
    def _generate_flow(self):
        """Genera un flujo unidireccional desde clientes a servidores"""
        # Identificar nodos cliente y servidor
        client_nodes = [n for n in self.es_nodes if n.startswith('C')]
        server_nodes = [n for n in self.es_nodes if n.startswith('SRV')]
        
        # Si no hay clientes o servidores, usar generaci√≥n normal
        if not client_nodes or not server_nodes:
            return super()._generate_flow()
            
        # Seleccionar origen y destino
        src_id = random.choice(client_nodes)
        dst_id = random.choice(server_nodes)
        
        # Calcular ruta
        path = nx.shortest_path(self.graph, src_id, dst_id)
        path = [(path[i], path[i+1]) for i in range(len(path)-1)]
        
        # Seleccionar par√°metros
        period = random.choice(self.period_set)
        
        # Crear flujo con payload aleatorio dentro del rango especificado
        flow = Flow(
            f"F{self.num_generated_flows}", src_id, dst_id, path,
            payload=random.randint(self.min_payload, self.max_payload), # Usar el rango
            period=period
        )
        self.num_generated_flows += 1
        return flow

def generate_flows(graph, num_flows=50, seed=None, period_set=PERIOD_SET, unidirectional=False, min_payload=1500, max_payload=1518): # A√±adir min/max payload
    """Funci√≥n para generar flujos con configuraci√≥n espec√≠fica"""
    generator_class = UniDirectionalFlowGenerator if unidirectional else FlowGenerator
    # Pasar el rango de payload al constructor del generador
    generator = generator_class(graph, seed, period_set, min_payload, max_payload)
    return generator(num_flows)



## ARCHIVO: core/network/operation.py
## ==================================================

import copy
from dataclasses import dataclass
import math
from typing import Optional
import numpy as np

from core.network.net import Net  # A√±adido para acceder a Net.DELAY_PROC_RX

@dataclass
class Operation:
    start_time: int
    gating_time: Optional[int]
    latest_time: int  # must equal to gating time if enable gating
    end_time: int
    # Nuevos campos para an√°lisis y visualizaci√≥n
    reception_time: Optional[int] = None
    
    # Solo se conserva la espera por *min-gap* (controlada por RL)
    min_gap_wait: int = 0
    
    # Campos para decisiones de RL (eliminado gcl_cycle_opt)
    guard_factor: float = 1.0
    min_gap_value: float = 1.0
    conflict_strategy: int = 0

    def __post_init__(self):
        if self.gating_time is not None:
            assert self.start_time <= self.gating_time == self.latest_time < self.end_time, \
                   "Invalid Operation: desajuste temporal"
        else:
            assert self.start_time <= self.latest_time < self.end_time, "Invalid Operation"
        
        # Calcular autom√°ticamente el tiempo de recepci√≥n completa si no se proporciona
        if self.reception_time is None:
            # Tomar en cuenta retardo de propagaci√≥n y procesamiento de recepci√≥n
            self.reception_time = self.end_time + Net.DELAY_PROP + Net.DELAY_PROC_RX

    def add(self, other: int | np.integer):
        """
        Desplaza la operaci√≥n en ``other`` ¬µs.
        Acepta enteros python normales **o** cualquier subtipo de ``np.integer``.
        """
        # Convertir numpy.int* a int nativo para evitar fallos de tipo
        if isinstance(other, np.integer):
            other = int(other)
        assert isinstance(other, int), "Operation.add() espera un entero"

        self.start_time += other
        if self.gating_time is not None:
            self.gating_time += other
        self.latest_time += other
        self.end_time += other
        # earliest_time ya no existe o se alinea con start_time
        # Actualizar reception_time tambi√©n
        if self.reception_time is not None:
             self.reception_time += other
        return self

    def __repr__(self):
        if self.gating_time is not None:
            return (f"Operation(start={self.start_time}, gate={self.gating_time}, "
                    f"end={self.end_time}, rcv={self.reception_time})")
        return (f"Operation(start={self.start_time}, latest={self.latest_time}, "
                f"end={self.end_time}, rcv={self.reception_time})")

    # ‚ûï Utilidad
    @property
    def duration(self):
        """Duraci√≥n efectiva de la transmisi√≥n (end ‚àí start)"""
        return self.end_time - self.start_time

    # Nueva propiedad para obtener el desglose de esperas
    @property
    def wait_breakdown(self):
        """Retorna un diccionario con el desglose de las causas de espera"""
        if self.gating_time is None or self.gating_time <= self.start_time:
            return {}
        
        total_wait = self.gating_time - self.start_time
        result = {
            'total': total_wait,
            'min_gap': self.min_gap_wait,
            'other': total_wait - self.min_gap_wait
        }
        return result

    # Eliminar la propiedad jitter_percent
    
    # M√©todo para registrar el per√≠odo asociado
    def set_period(self, period: int):
        """Almacena el per√≠odo asociado a esta operaci√≥n para c√°lculos posteriores"""
        self._period = period
        return self

#
def check_operation_isolation(operation1: tuple[Operation, int],
                              operation2: tuple[Operation, int]) -> Optional[int]:
    """

    :param operation1:
    :param operation2:
    :return: None if isolation constraint is satisfied,
             otherwise, it returns the offset that `operation1` should add.
             Notice that the adding the returned offset might make `operation` out of period.
    """
    operation1, period1 = operation1
    operation2, period2 = operation2

    assert (operation1.start_time >= 0) and (operation1.end_time <= period1)
    assert (operation2.start_time >= 0) and (operation2.end_time <= period2)

    hyper_period = math.lcm(period1, period2)
    alpha = hyper_period // period1
    beta = hyper_period // period2

    operation_lhs = copy.deepcopy(operation1)

    for _ in range(alpha):

        operation_rhs = copy.deepcopy(operation2)
        for _ in range(beta):
            if (operation_lhs.start_time <= operation_rhs.start_time < operation_lhs.end_time) or \
                    (operation_rhs.start_time <= operation_lhs.start_time < operation_rhs.end_time):
                return operation_rhs.end_time - operation_lhs.start_time
            operation_rhs.add(period2)

        operation_lhs.add(period1)
    return None



## ARCHIVO: core/omnet_export.py
## ==================================================

"""core.omnet_export
---------------------------------
Utilities to export a `Network` plus a scheduling result (`ScheduleRes`)
obtained with `ui/test.py` into two artefacts ready to be consumed by
OMNeT++ NET‚ÄëTSN:

* **<name>.ned** ‚Äì  the physical topology as a `network` definition.
* **<name>.ini** ‚Äì  simulation configuration that reproduces the exact
  cyclic traffic (period, payload, initial offset) *and* programmes the
  time‚Äëaware shaping gate according to the static GCL derived during the
  DRL scheduling step.

Files are *always* overwritten if they already exist.

This module is completely decoupled from the rest of the code‚Äëbase ‚Äì it
only relies on public attributes of `core.network` objects, on the final
`ScheduleRes`, and on the ¬´GCL tables¬ª returned by `ResAnalyzer`.
"""

from __future__ import annotations

import os
import math
from pathlib import Path
from typing import Dict, List, Tuple
from collections import defaultdict

from core.network.net import Network, Flow, Link
from core.scheduler.scheduler import ScheduleRes

# ---------------------------------------------------------------------------
#  UTILIDADES DE FORMATO  (‚Üì se usan en todo el m√≥dulo, por eso van primero)
# ---------------------------------------------------------------------------
Œºs = 1e-6

def _us_to_ms_str(us: int) -> str:
    """¬µs ‚Üí "x.xxms" sin ceros sobrantes."""
    return f"{us * Œºs * 1_000:.3f}ms".rstrip("0").rstrip(".")

# ---------- helpers para puertos y PCP ----------
def _build_period_order(flows: list["Flow"]) -> dict[int, int]:
    """Mapa per√≠odo‚Üí√≠ndice incremental (2000 ¬µs‚áí0 ‚Üí UDP 2000, 4000 ¬µs‚áí1 ‚Üí 2001‚Ä¶)."""
    unique = sorted({f.period for f in flows})
    return {p: i for i, p in enumerate(unique)}

def _flow_port(period: int, period_order: dict[int, int]) -> int:
    return 2000 + period_order[period]

# PCP: 4 para primer flujo por cliente, 7 el resto
def _pcp_for_flow(node_name: str, seen: dict[str, int]) -> int:
    pcp = 4 if seen[node_name] == 0 else 7
    seen[node_name] += 1
    return pcp

def _first_tx_offset_us(schedule_res: "ScheduleRes", fl: "Flow") -> int:
    """
    Devuelve el **start_time** del primer hop del flujo `fl`.

    üî∏ La comparaci√≥n se hace por `flow_id` (==) en lugar de identidad
       de objetos, porque los `Flow` en `schedule_res` pueden ser copias.
    """
    first_link_id = fl.path[0]            # (src, dst) del primer salto
    for link, ops in schedule_res.items():
        if link.link_id != first_link_id:
            continue
        for f, op in ops:
            if f.flow_id == fl.flow_id:   # ‚Üê comparaci√≥n robusta
                return op.start_time
    # fallback ‚Äì no deber√≠a ocurrir
    return 0

# ---------------------------------------------------------------------------
#  Small helpers                                                               
# ---------------------------------------------------------------------------

# (las tres funciones ya se han adelantado)


def _indent(n: int) -> str:
    return " " * n


# ---------------------------------------------------------------------------
#  NED generation                                                             
# ---------------------------------------------------------------------------


def _ned_node_line(node_name: str, node_type: str, xpos: int, ypos: int) -> str:
    """Return a *submodule* line for a node with display coordinates."""
    return (
        f"        {node_name}: <default(\"{node_type}\")> like IEthernetNetworkNode {{\n"
        f"            @display(\"p={xpos},{ypos}\");\n"
        f"        }}\n"
    )


#  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
#  INI generation
#  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def _ini_general(net_name: str, gate_port: int) -> str:
    return (
        "#########################################################\n"
        "#  AUTOGENERATED BY gatingdrl omnet_export.py            #\n"
        "#########################################################\n"
        "[General]\n"
        f"network = {net_name}\n"
        "sim-time-limit   = 24ms\n"
        'description      = "Auto-imported schedule"\n\n'
        "**.displayGateSchedules = true\n"
        # Mostrar por defecto todos los puertos (el filtro se puede afinar despu√©s)
        f'**.gateFilter = "**.eth[{gate_port}].**"\n'
        "**.gateScheduleVisualizer.height = 16\n"
        '**.gateScheduleVisualizer.placementHint = "top"\n\n'
    )

def _ini_flows(
        flows: List["Flow"],
        schedule_res: ScheduleRes,
        network: "Network"           # ‚Üê necesitamos acceso al grafo
) -> str:
    """
    Construye la secci√≥n SOURCES + SINKS a partir de la lista completa de flujos.
    No debe lanzar excepci√≥n ‚Äì si un flujo carece de alg√∫n dato, lo salta y sigue.
    """
    by_src: Dict[str, List["Flow"]] = {}
    by_dst: Dict[str, List["Flow"]] = {}
    for f in flows:
        by_src.setdefault(f.src_id.lower(), []).append(f)
        by_dst.setdefault(f.dst_id.lower(), []).append(f)

    txt = "#########################################################\n"
    txt += "#  SOURCES                                               #\n"
    txt += "#########################################################\n"
    # ‚ñ∏ Puerto UDP √∫nico por cliente (2000 + √≠ndice del cliente)
    client_port_map: Dict[str, int] = {
        n: 2000 + i for i, n in enumerate(sorted(by_src.keys()))
    }
    seen_per_client: Dict[str, int] = defaultdict(int)
    for node, list_flows in by_src.items():
        txt += f"*.{node}.numApps           = {len(list_flows)}\n"
        txt += f"*.{node}.app[*].typename   = \"UdpSourceApp\"\n"
        txt += f"*.{node}.app[*].io.destAddress = \"srv1\"\n\n"

        for idx, fl in enumerate(list_flows):
            # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ NUEVO: etiqueta legible para el flujo ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            # Usa el flow_id ('F0', 'F1'‚Ä¶) en min√∫sculas  ‚Üí  "f0", "f1", ‚Ä¶
            # Si se prefiere "video-1" basta con reemplazar la siguiente l√≠nea
            display_name = fl.flow_id.lower()
            txt += f"*.{node}.app[{idx}].display-name   = \"{display_name}\"\n"

            # ‚ûã Offset = ¬´Disponible (start time)¬ª ‚áí op.start_time
            off_us = _first_tx_offset_us(schedule_res, fl)
            off_ms = _us_to_ms_str(off_us)
            
            # ‚ûä Tama√±o de trama = payload mostrado en ¬´INFORMACI√ìN DE FLUJOS¬ª
            #    (no restamos 54 B de cabeceras; es exactamente el valor listado).
            length = max(64, fl.payload)

            txt += f"*.{node}.app[{idx}].source.productionInterval = {fl.period*Œºs*1_000:.0f}ms\n"
            txt += f"*.{node}.app[{idx}].source.initialProductionOffset = {off_ms}\n"
            txt += f"*.{node}.app[{idx}].source.packetLength = {length}B\n"

            dport = client_port_map[node]           # puerto fijo por cliente
            txt  += f"*.{node}.app[{idx}].io.destPort = {dport}\n"

            pcp = _pcp_for_flow(node, seen_per_client)
            txt += (f"*.{node}.app[{idx}].bridging.streamIdentifier.identifier.mapping = "
                    f"[{{stream: \"{fl.flow_id}\", packetFilter: expr(udp.destPort == {dport})}}]\n")
            txt += (f"*.{node}.app[{idx}].bridging.streamCoder.encoder.mapping = "
                    f"[{{stream: \"{fl.flow_id}\", pcp: {pcp}}}]\n\n")

    # ------- habilitar outgoing streams por cliente -----------
    for node in by_src.keys():
        txt += f"*.{node}.hasOutgoingStreams = true\n"

    # ------- habilitar egress shaping en cada switch ----------
    sw_nodes = [n.lower() for n, d in network.graph.nodes(data=True)
                if d.get("node_type") == "SW"]
    for sw in sw_nodes:
        txt += f"*.{sw}.hasEgressTrafficShaping = true\n"
        txt += (f"*.{sw}.bridging.directionReverser.reverser."
                f"excludeEncapsulationProtocols = [\"ieee8021qctag\"]\n")

    txt += "\n#########################################################\n"
    txt += "#  SINKS                                                 #\n"
    txt += "#########################################################\n"
    for node, list_flows in by_dst.items():
        txt += f"*.{node}.numApps         = {len(list_flows)}\n"
        txt += f"*.{node}.app[*].typename = \"UdpSinkApp\"\n"
        for idx, _ in enumerate(list_flows):
            txt += f"*.{node}.app[{idx}].io.localPort = {2000+idx}\n"
        txt += "\n"

    return txt

def _ini_gcl(
    gcl: Dict["Link", List[Tuple[int, int]]],
    network: "Network",
    gate_port: int = 0,           # ‚Üê puerto declarado en **.gateFilter
) -> str:
    """
    Genera la secci√≥n de programaci√≥n Time-Aware para cada puerto de switch.
    Si `gcl` est√° vac√≠o simplemente devuelve cadena vac√≠a.
    """
    if not gcl:
        return ""

    txt = "#########################################################\n"
    txt += "#  TIME-AWARE TRAFFIC SHAPING                           #\n"
    txt += "#########################################################\n"

    for link, table in gcl.items():
        src = link.link_id[0] if isinstance(link.link_id, tuple) else link.link_id.split('-')[0]
        dst = link.link_id[1] if isinstance(link.link_id, tuple) else link.link_id.split('-')[1]
        # Usa el mismo √≠ndice que figura en **.gateFilter
        port = gate_port

        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  UNA sola cola "video" ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        txt += f"*.{src.lower()}.eth[{port}].macLayer.queue.numTrafficClasses = 1\n"
        txt += f"*.{src.lower()}.eth[{port}].macLayer.queue.*[0].display-name = \"video\"\n"

        # Gate √∫nico TC0 ‚Äì usar **la tabla que ya imprimi√≥ ResAnalyzer**.
        #     ‚Æë  No se vuelven a ordenar ni a recalcular tiempos.
        # ‚ûä  Hiperper√≠odo = m.c.m. de los per√≠odos de *toda la red*
        import math as _m
        hyperperiod = 1
        for _f in network.flows:
            hyperperiod = _m.lcm(hyperperiod, _f.period)

        # ‚ñ∫ LA TABLA LLEGA TAL CUAL SE IMPRIMIO EN
        #   "TABLA GCL GENERADA (t, estado)"  ‚óÑ
        #   ‚îÄ no se toca, s√≥lo se ordena por tiempo.
        # ‚îÄ‚îÄ tabla "corta" que llega desde ResAnalyzer ‚îÄ‚îÄ
        events = sorted(table, key=lambda x: x[0])  # ya viene filtrada

        # Ordenar la tabla generada y *reemplazar* su PRIMER registro
        #     por "abierto en 0 ¬µs" (no se a√±ade, se sustituye).
        if events:
            events[0] = (0, 1)      # primer registro forzado
        else:
            events = [(0, 1)]       # salvaguarda: lista vac√≠a

        # Elimina duplicados consecutivos (p.ej. ‚Ä¶, (0,1), (0,1), ‚Ä¶)
        compact: list[tuple[int, int]] = []
        for t, s in events:
            if not compact or compact[-1][1] != s:
                compact.append((t, s))

        durations = [(compact[(i+1)%len(compact)][0] - t) % hyperperiod
                     for i, (t, _) in enumerate(compact)]

        txt += f"*.{src.lower()}.eth[{port}].macLayer.queue.transmissionGate[0].offset = 0ms\n"
        txt += f"*.{src.lower()}.eth[{port}].macLayer.queue.transmissionGate[0].durations = "
        txt += f"[{', '.join(_us_to_ms_str(d) for d in durations)}]\n"

        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        #  DEBUG   Comparar la tabla original (events) con la reconstruida
        #          a partir de las durations que vamos a escribir.
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        #
        # 1.  Tabla "original" (la que viene de ResAnalyzer) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        orig_table = sorted(table, key=lambda x: x[0])    # [(t,state), ...]

        # 2.  Reconstruir tabla a partir de durations  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        recon_table: list[tuple[int,int]] = []
        t_acc = 0
        state = 1                      # siempre arrancamos "abierto"
        for d in durations:
            recon_table.append((t_acc, state))
            t_acc = (t_acc + d) % hyperperiod
            state = 1 - state          # alternar 1‚Üî0

        # 3.  Normalizar y ordenar para comparar
        recon_norm = sorted(recon_table, key=lambda x: x[0])
        orig_norm  = orig_table

        if recon_norm != orig_norm:
            import textwrap, logging
            logging.warning(
                "\n".join([
                    "",
                    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ",
                    f"GCL CHECK  ‚üπ  DIFERENCIA EN PUERTO {src}.eth[{port}]",
                    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ",
                    "ORIGINAL:          RECONSTRUIDA DESDE durations:",
                    *[
                        f"{o[0]:>6} ¬µs | {o[1]}        ||      "
                        f"{r[0]:>6} ¬µs | {r[1]}"
                        for o, r in zip(
                            orig_norm + [('', '')]*(len(recon_norm)-len(orig_norm)),
                            recon_norm + [('', '')]*(len(orig_norm)-len(recon_norm))
                        )
                    ],
                    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ",
                    "Revisa la l√≥gica que genera ¬´durations¬ª: puede que se",
                    "est√© filtrando/eliminando alg√∫n evento o que el orden",
                    "de los mismos no sea exactamente el original.",
                    ""
                ])
            )

    return txt


def write_ned(network: Network, outfile: os.PathLike, pkg: str, net_name: str) -> None:
    """Generate a .ned file representing *exactly* the passed network."""

    # ‚îÄ‚îÄ Simple ‚Äì we only need node names and edges.  Use a na√Øve layout: ‚îÄ‚îÄ
    # ‚Ä¢ End‚Äëstations on the left (x ‚âà 300)  ‚Äì spread vertically in steps
    # ‚Ä¢ Switches   in the middle (x ‚âà 500)
    # ‚Ä¢ Servers    on the right (x ‚âà 700)

    es_nodes = [n for n, d in network.graph.nodes(data=True) if d.get("node_type") == "ES"]
    sw_nodes = [n for n, d in network.graph.nodes(data=True) if d.get("node_type") == "SW"]

    server_nodes = [n for n in es_nodes if n.startswith("SRV")]
    client_nodes = [n for n in es_nodes if n not in server_nodes]

    # Coordinates
    step = 80
    submod_lines: List[str] = []

    for i, name in enumerate(client_nodes):
        submod_lines.append(_ned_node_line(name.lower(), "TsnDevice", 300, 200 + i * step))

    for i, name in enumerate(sw_nodes):
        submod_lines.append(_ned_node_line(name.lower(), "TsnSwitch", 500, 250 + i * step))

    for i, name in enumerate(server_nodes):
        submod_lines.append(_ned_node_line(name.lower(), "TsnDevice",
                                         700, 250 + i * step))

    # Connections ‚Äì iterate original directed edges and create <--> pairs.
    conn_lines: List[str] = []
    added = set()
    for src, dst, edata in network.graph.edges(data=True):
        # Create deterministic id for unordered pair to avoid duplicates
        key = tuple(sorted((src, dst)))
        if key in added:
            continue
        added.add(key)

        # Mantener el mismo alias en las conexiones
        def _alias(n: str) -> str:
            return n.lower()
        conn_lines.append(
            f"        {_alias(src)}.ethg++ <--> EthernetLink <--> {_alias(dst)}.ethg++;\n"
        )
        
    net_name = Path(outfile).stem          # p.ej.  ¬´unidir¬ª
    # Detectar bitrate (asumimos que todos los enlaces comparten valor)
    try:
        first_edge = next(iter(network.graph.edges(data=True)))
        link_rate = first_edge[2].get("link_rate", 100)
    except StopIteration:
        link_rate = 100

    ned = (
        f"package {pkg};\n\n"
        "import inet.node.ethernet.EthernetLink;\n"
        "import inet.node.contract.IEthernetNetworkNode;\n"
        "import inet.node.tsn.TsnDevice;\n"
        "import inet.node.tsn.TsnSwitch;\n"
        "import inet.networks.base.TsnNetworkBase;\n\n"
        f"network {net_name.lower()} extends TsnNetworkBase\n"  # Force lowercase here
        "{\n"
        "    parameters:\n"
        f"        *.eth[*].bitrate = default({link_rate}Mbps);\n"
        "    submodules:\n"
        + "".join(submod_lines)
        + "\n    connections:\n"
        + "".join(conn_lines)
        + "}\n"
    )

    Path(outfile).write_text(ned, encoding="utf-8")

def write_ini(
    network: Network,
    schedule_res: ScheduleRes,
    gcl_tables: Dict[Link, List[Tuple[int, int]]],
    outfile: os.PathLike,
    net_name: str,
    gate_port: int,
) -> None:
    """Generate an .ini file reproducing traffic & GCL schedule.
    
    This function never fails completely - if problems occur during generation,
    it will still produce at least a basic INI file with the [General] section.
    """
    try:
        flows = list(network.flows)
        num_flows = len(flows)
        
        parts: List[str] = [_ini_general(net_name, gate_port)]
        try:
            parts.append(_ini_flows(flows, schedule_res, network))
        except Exception as exc:
            import logging
            logging.warning(f"[omnet_export] Error generating FLOWS section: {exc}")
            parts.append(f"#  (ERROR generando secci√≥n SOURCES / SINKS: {exc})\n")

        try:
            parts.append(_ini_gcl(gcl_tables, network, gate_port))
        except Exception as exc:
            import logging
            logging.warning(f"[omnet_export] Error generating GCL section: {exc}")
            parts.append(f"#  (ERROR generando secci√≥n GCL: {exc})\n")

        Path(outfile).write_text("".join(parts), encoding="utf-8")
            
    except Exception as e:
        import logging
        logging.error(f"[omnet_export] Critical error generating INI file: {e}")
        # Ensure at least a minimal INI file is written
        minimal_ini = "[General]\nnetwork = inet.showcases.tsn.generated.GeneratedTsnNetwork\n"
        Path(outfile).write_text(minimal_ini, encoding="utf-8")

def export_omnet_files(network: Network, schedule_res: ScheduleRes, gcl_tables: Dict[Link, List[Tuple[int, int]]], label: str, out_dir: os.PathLike, gate_port:int=2) -> Tuple[Path, Path]:
    """Create *.ned & *.ini in *out_dir* (overwriting any previous version)."""
    os.makedirs(out_dir, exist_ok=True)

    # Create a subdirectory for this topology if it doesn't exist
    topo_dir = Path(out_dir) / label
    topo_dir.mkdir(exist_ok=True, parents=True)

    ned_path = topo_dir / f"{label}.ned"
    ini_path = topo_dir / f"{label}.ini"

    pkg = "inet.showcases.tsn.trafficshaping.Pruebas_tesis.Red_" + label
    net_name = label.lower()

    write_ned(network, ned_path, pkg, net_name)
    write_ini(network, schedule_res, gcl_tables, ini_path, net_name, gate_port)

    print(f"[omnet_export]  generated ‚Üí {ned_path}, {ini_path}")
    return ned_path, ini_path



## ARCHIVO: core/scheduler/__init__.py
## ==================================================

"""
Algoritmos de scheduling para Time Sensitive Networks
"""

from .scheduler import DrlScheduler, ResAnalyzer, ScheduleRes





## ARCHIVO: core/scheduler/scheduler.py
## ==================================================

import logging
import os
from typing import Dict, List, Tuple
import math
from collections import defaultdict

import numpy as np
from sb3_contrib import MaskablePPO
# Usamos el mismo extractor que durante el entrenamiento
from core.learning.encoder import FeaturesExtractor
from stable_baselines3.common.vec_env import DummyVecEnv

from tools.definitions import OUT_DIR
from core.network.net import Flow, Link, Network, Net
from core.network.operation import Operation
from core.learning.environment import NetEnv

# Tipo para resultados
ScheduleRes = Dict[Link, List[Tuple[Flow, Operation]]]

class DrlScheduler:
    """Scheduler TSN usando Deep Reinforcement Learning con MaskablePPO"""
    
    def __init__(self, network: Network, num_envs=1, timeout_s=300, use_curriculum=False):
        """Inicializa el scheduler con una red y opcionalmente n√∫mero de entornos"""
        self.network = network
        self.num_flows = len(network.flows)
        self.num_envs = num_envs
        self.timeout_s = timeout_s
        # Explicitly pass curriculum_enabled=False for testing
        self.env = DummyVecEnv([
            lambda: NetEnv(network, curriculum_enabled=use_curriculum, initial_complexity=1.0) 
            for _ in range(num_envs)
        ])
        # Mantener consistencia con el extractor usado al entrenar.
        policy_kwargs = dict(features_extractor_class=FeaturesExtractor)
        self.model = MaskablePPO(
            "MlpPolicy",
            self.env,
            verbose=0,
            policy_kwargs=policy_kwargs,
        )
        self.res = None
        logging.info(f"Scheduler inicializado con {self.num_flows} flujos (curriculum: {use_curriculum})")
        
    def load_model(self, filepath: str, alg='MaskablePPO'):
        """Carga un modelo pre-entrenado"""
        if filepath.endswith('.zip'):
            filepath = filepath[:-4]
        if not os.path.isfile(f"{filepath}.zip"):
            logging.error(f"Modelo no encontrado: {filepath}.zip")
            return False
        try:
            self.model = MaskablePPO.load(filepath, self.env)
            logging.info(f"Modelo cargado: {filepath}")
            return True
        except Exception as e:
            logging.error(f"Error cargando modelo: {e}")
            return False
        
    def schedule(self) -> bool:
        """Ejecuta el scheduling usando el modelo DRL"""
        self.res = None
        
        # Intentar hasta 100 episodios como m√°ximo
        for _ in range(100):   # +100 % intentos ‚áí mayor tasa de √©xito
            obs = self.env.reset()
            done = False
            
            # Procesar un episodio completo
            while not done:
                # Predecir acci√≥n con m√°scara
                # `env_method` devuelve una lista de arrays (uno por entorno);
                # apilamos para obtener shape (n_envs, n_actions).
                action_masks = np.vstack(self.env.env_method('action_masks'))
                # Asegurar dtype int8 para plena compatibilidad
                action, _ = self.model.predict(
                    obs, deterministic=True, action_masks=action_masks.astype(np.int8)
                )
                
                # Ejecutar acci√≥n
                obs, _, dones, infos = self.env.step(action)

                # ‚¨áÔ∏è  Propagar correctamente la terminaci√≥n del episodio
                done = any(dones)

                if done:
                    for i, is_done in enumerate(dones):
                        if is_done and infos[i].get('success'):
                            self.res = infos[i].get('ScheduleRes')
                            return True      # √©xito ‚áí abandonar bucle de episodios
                    # Reiniciar si termin√≥ sin √©xito
                    obs = self.env.reset()
                    done = False
            
        # Si llegamos aqu√≠, no se encontr√≥ soluci√≥n
        return False
    
    def get_res(self) -> ScheduleRes:
        """Retorna el resultado del scheduling"""
        return self.res

class ResAnalyzer:
    """Analiza y guarda resultados del scheduling"""
    
    # Threshold for GCL entry generation - easily modifiable class variable
    DEFAULT_GCL_GAP_THRESHOLD = 30
    
    def __init__(self, network: Network, results: ScheduleRes):
        """
        Inicializa el analizador y guarda resultados
        
        Args:
            network: Red TSN
            results: Resultado del scheduling
        """
        self.network = network
        self.results = results
        self.analyzer_id = id(self) # Generar un ID √∫nico para el analizador
        
        # Set instance variable from class default
        self.gap_threshold_us = self.DEFAULT_GCL_GAP_THRESHOLD
        
        # --- NUEVO: Calcular y almacenar tablas GCL est√°ticas ---
        self._gcl_tables = self._calculate_gcl_tables(self.gap_threshold_us)
        # --- FIN NUEVO ---

        # Guardar resultados a archivo
        if results:
            # Asegurarse de que el directorio OUT_DIR existe
            os.makedirs(OUT_DIR, exist_ok=True)
            
            # Usar el formato "by_link" consistentemente
            filename = os.path.join(OUT_DIR, f'schedule_res_by_link_{self.analyzer_id}.log')
            try:
                with open(filename, 'w') as f:
                    for link, operations in results.items():
                        f.write(f"Enlace: {link}\n")
                        for flow, op in operations:
                            f.write(f"  Flujo: {flow.flow_id}, Op: {op}\n")
                        f.write("\n")
                logging.info(f"Resultados guardados en {filename}")
            except Exception as e:
                logging.error(f"Error al guardar resultados: {e}")

    # --- NUEVO: M√©todo para calcular las tablas GCL --------------------------
    def _calculate_gcl_tables(self, gap_thr_us: int = None) -> Dict[Link, List[Tuple[int, int]]]:
        """
        Genera la tabla GCL (lista de pares ¬´tiempo, estado¬ª) para cada
        puerto-switch.

        ‚ñ∏ S√≥lo se insertan 0/1 cuando el hueco entre la recepci√≥n de un paquete
          y el comienzo del siguiente supera `gap_thr_us` ¬µs (default: valor de self.gap_threshold_us).
        ‚ñ∏ Se enlaza el √∫ltimo paquete del hiperper√≠odo con el primero para que
          el cierre final tambi√©n quede reflejado.
        ‚ñ∏ IMPORTANTE: Para cada par de eventos, se a√±ade un 0 (cierre) y un 1 (apertura)
        """
        from math import lcm
        
        # El umbral viene del atributo de instancia (por defecto 30 ¬µs,
        # o el que se haya pasado v√≠a --gcl-threshold).  **NO** lo
        # sobre-escribimos aqu√≠; as√≠ generamos la "tabla corta".
        gcl_tables: Dict[Link, List[Tuple[int, int]]] = {}
        if not self.results:
            return gcl_tables

        for link, ops in self.results.items():
            # S√≥lo puertos cuyo ORIGEN es un switch ("S‚Ä¶", excluyendo "SRV‚Ä¶")
            src = link.link_id[0] if isinstance(link.link_id, tuple) else link.link_id.split("-")[0]
            if not (src.startswith("S") and not src.startswith("SRV")):
                continue

            # 1Ô∏è‚É£  Ordenar operaciones por inicio real (gating_time o start_time)
            ops_sorted = sorted(ops, key=lambda p: (p[1].gating_time or p[1].start_time))
            n = len(ops_sorted)
            if n == 0:
                continue

            # 2Ô∏è‚É£  Calcular hiperper√≠odo de ese puerto
            gcl_cycle = 1
            for f, _ in ops_sorted:
                gcl_cycle = lcm(gcl_cycle, f.period)

            # 3Ô∏è‚É£  Analizar cada operaci√≥n ‚Äì reset de listas por-link
            all_transmission_times: list[tuple[int, str]] = []
            all_reception_times:    list[tuple[int, str]] = []
            
            hyperperiod_link = gcl_cycle  # Hiperperiodo para este enlace

            #     Crear un par de eventos 0/1 por paquete
            for i in range(n):
                f_curr, op_curr = ops_sorted[i]
                
                # √çndice del siguiente paquete (con wraparound)
                next_idx = (i + 1) % n
                f_next, op_next = ops_sorted[next_idx]

                # Tiempo cuando termina de llegar este paquete (necesitamos cerrar el gate)
                close_time = op_curr.reception_time
                
                # Tiempo cuando inicia la transmisi√≥n del siguiente paquete (reabrimos el gate)
                open_time = op_next.gating_time or op_next.start_time
                
                # Si es el √∫ltimo paquete, a√±adir un per√≠odo para el wraparound
                if i == n - 1:
                    open_time += f_next.period

                # Calcular el gap entre recepci√≥n y siguiente transmisi√≥n
                gap = open_time - close_time
                
                # Para cada paquete, repetirlo durante todo el hiperper√≠odo
                repetitions = hyperperiod_link // f_curr.period
                for rep in range(repetitions):
                    offset = rep * f_curr.period
                    # Guardar tiempo de inicio y recepci√≥n (normalizado al hiperperiodo)
                    tx_t = (op_curr.start_time + offset) % hyperperiod_link
                    rx_t = (op_curr.reception_time + offset) % hyperperiod_link
                    all_transmission_times.append((tx_t, f_curr.flow_id))
                    all_reception_times.append((rx_t, f_curr.flow_id))

            # Ordenar los tiempos
            all_transmission_times.sort(key=lambda x: x[0])
            all_reception_times.sort(key=lambda x: x[0])
            
            # PASO 2: Generar eventos GCL con la tabla COMPLETA
            gcl_close_events: list[tuple[int,str,int,str]] = []
            
            # Buscar gaps significativos entre recepci√≥n y siguiente transmisi√≥n
            for rx_time, rx_flow in all_reception_times:
                
                # ‚ûä Iniciar variables cada iteraci√≥n
                next_tx_time: int | None = None
                next_tx_flow: str | None = None

                # Buscar la siguiente transmisi√≥n > rx_time
                for tx_time, tx_flow in all_transmission_times:
                    if tx_time > rx_time:
                        next_tx_time = tx_time
                        next_tx_flow = tx_flow
                        break

                # Si no hay ninguna (wraparound) usa la primera del ciclo + hiperper√≠odo
                if next_tx_time is None and all_transmission_times:
                    first_tx_time, first_tx_flow = all_transmission_times[0]
                    next_tx_time = first_tx_time + hyperperiod_link
                    next_tx_flow = first_tx_flow
                
                # Protecci√≥n extra ‚Äì si, aun as√≠, no existe TX, saltar este RX
                if next_tx_time is None:
                    continue
                 
                # Calcular el gap siempre (tabla completa ‚Äì sin filtro)
                gap = (next_tx_time - rx_time) % hyperperiod_link

                #  S√≥lo a√±adimos el par 0/1 cuando el hueco supera
                #  el threshold definido por el usuario.
                if gap > gap_thr_us:
                    # A√±adir eventos de cierre/apertura
                    gcl_close_events.append(
                        (rx_time, rx_flow, next_tx_time, next_tx_flow)
                    )

            # PASO 3: Generar los pares de eventos 0/1 para cada gap significativo
            events: List[Tuple[int, int]] = []
            for rx_time, rx_flow, next_tx_time, next_tx_flow in gcl_close_events:
                # A√±adir evento de cierre (0) en el tiempo de recepci√≥n
                events.append((rx_time, 0))
                    
                # A√±adir evento de apertura (1) cuando empieza el siguiente paquete
                events.append((next_tx_time % hyperperiod_link, 1))

            # 4Ô∏è‚É£  Ordenar todos los eventos por tiempo
            events.sort(key=lambda x: (x[0], x[1]))
            
            # 5Ô∏è‚É£  Eliminar estados duplicados o redundantes consecutivos
            final_table: List[Tuple[int, int]] = []
            last_state: int | None = None
            for t, s in events:
                if s != last_state:  # Solo a√±adir si cambia el estado
                    final_table.append((t, s))
                    last_state = s

            # 6Ô∏è‚É£  Garantizar que la tabla empiece "abierta" en t = 0 ¬µs
            if not final_table or final_table[0][0] != 0:
                final_table.insert(0, (0, 1))
            elif final_table[0][0] == 0 and final_table[0][1] == 0:
                # Si el primer evento es cerrar en t=0, a√±adir apertura en t=0 antes
                final_table.insert(0, (0, 1))

            gcl_tables[link] = final_table

        return gcl_tables

    # --- NUEVO: M√©todo para recalcular GCL con threshold diferente ---
    def recalculate_gcl_tables(self, new_threshold_us: int) -> Dict[Link, List[Tuple[int, int]]]:
        """
        Recalcula las tablas GCL con un nuevo threshold.
        
        Args:
            new_threshold_us: Nuevo valor de threshold en ¬µs
            
        Returns:
            Diccionario con las nuevas tablas GCL
        """
        # Update instance threshold
        self.gap_threshold_us = new_threshold_us
        
        # Recalculate tables
        self._gcl_tables = self._calculate_gcl_tables(new_threshold_us)
        
        # Log the change
        print(f"GCL tables recalculated with threshold: {new_threshold_us}¬µs")
        
        return self._gcl_tables

    # --- NUEVO: M√©todo para imprimir informaci√≥n de los flujos ---
    def print_flow_info(self):
        """Imprime una tabla con informaci√≥n detallada de cada flujo, incluyendo tama√±os de paquete."""
        if not self.network or not self.network.flows:
            print("\nNo hay informaci√≥n de flujos disponible.")
            return
            
        print("\n" + "="*80)
        print("INFORMACI√ìN DE FLUJOS")
        print("="*80)
        
        # Definir formato de tabla
        format_str = "{:<8} | {:<8} | {:<8} | {:<10} | {:<12} | {:<6}"
        
        # Imprimir cabecera
        print(format_str.format("Flujo", "Origen", "Destino", "Per√≠odo (¬µs)", "Payload (B)", "Hops"))
        print("-"*8 + " | " + "-"*8 + " | " + "-"*8 + " | " + "-"*10 + " | " + "-"*12 + " | " + "-"*6)
        
        # Imprimir cada flujo
        scheduled_flows = set()
        if self.results:
            for link_ops in self.results.values():
                for flow, _ in link_ops:
                    scheduled_flows.add(flow.flow_id)
        
        for flow in self.network.flows:
            # Verificar si el flujo fue programado exitosamente
            status = "‚úì" if flow.flow_id in scheduled_flows else ""
            
            # Calcular n√∫mero de hops
            num_hops = len(flow.path)
            
            # Imprimir informaci√≥n
            print(format_str.format(
                flow.flow_id, 
                flow.src_id, 
                flow.dst_id, 
                flow.period, 
                flow.payload,
                f"{num_hops} {status}"
            ))
            
        print("="*80 + "\n")

    # --- NUEVO: M√©todo para imprimir las tablas GCL ---
    def print_gcl_tables(self):
        """
        Imprime las tablas GCL est√°ticas.
        Muestra solo la tabla GCL generada sin referencia a umbrales.
        """
        if not self._gcl_tables:
            print("\nNo se generaron tablas GCL (posiblemente no hubo gating o scheduling fall√≥).")
            return

        print("\n" + "="*80)
        print("TABLA GCL GENERADA (t, estado)")
        print("="*80)

        for link, table in self._gcl_tables.items():
            # Re-calcular gcl_cycle aqu√≠ para mostrarlo
            gated_ops = [(f, op) for f, op in self.results.get(link, []) if op.gating_time is not None]
            if not gated_ops: continue
            gcl_cycle = 1
            for f, _ in gated_ops:
                gcl_cycle = math.lcm(gcl_cycle, f.period)

            print(f"\n--- Enlace: {link.link_id} (Ciclo GCL: {gcl_cycle} ¬µs) ---")
            print(f"{'Tiempo (¬µs)':<12} | {'Estado':<6}")
            print(f"{'-'*12} | {'-'*6}")
            for time, state in table:
                print(f"{time:<12} | {state:<6}")

        print("="*80 + "\n")



## ARCHIVO: path_setup.py
## ==================================================

import os
import sys
import logging

# Obtener la ruta del directorio actual (donde est√° este script)
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))

# A√±adir al sys.path para permitir importaciones relativas
if PROJECT_ROOT not in sys.path:
    # Insertar al inicio para asegurar que tiene prioridad sobre otras instalaciones
    sys.path.insert(0, PROJECT_ROOT)
    logging.getLogger(__name__).debug(f"Added {PROJECT_ROOT} to PYTHONPATH")

# Diagn√≥stico: mostrar el path de Python completo




## ARCHIVO: project_analyzer.py
## ==================================================

#!/usr/bin/env python3

import os
import sys

# Archivos y directorios a excluir
EXCLUDED_FILES = {
  
}
EXCLUDED_DIRS = {
    '__pycache__', 
    '.git', 
    '.idea', 
    'venv', 
    'env', 
    'build', 
    'dist'
}

def collect_python_files(root_path):
    """Recopila todas las rutas de archivos Python en el proyecto"""
    py_files = []
    for root, dirs, files in os.walk(root_path):
        # Evitar directorios excluidos
        dirs[:] = [d for d in dirs if d not in EXCLUDED_DIRS]
        
        # Procesar archivos Python
        for file in files:
            if file.endswith('.py') and file not in EXCLUDED_FILES:
                full_path = os.path.join(root, file)
                rel_path = os.path.relpath(full_path, root_path)
                py_files.append((rel_path, full_path))
    
    # Ordenar los archivos por ruta relativa para mejor lectura
    return sorted(py_files)

def save_code_to_file(py_files, output_file='project_code.txt'):
    """Guarda el contenido de todos los archivos en un √∫nico archivo de texto"""
    with open(output_file, 'w', encoding='utf-8') as f:
        
        f.write("# ===================================\n\n")
        
        for rel_path, full_path in py_files:
            f.write(f"## ARCHIVO: {rel_path}\n")
            f.write("## " + "=" * 50 + "\n\n")
            
            try:
                with open(full_path, 'r', encoding='utf-8') as src_file:
                    content = src_file.read()
                    f.write(content)
                    
                # A√±adir l√≠neas en blanco para mejor separaci√≥n
                f.write("\n\n\n")
            except Exception as e:
                f.write(f"# ERROR: No se pudo leer el archivo: {e}\n\n")
    
    print(f"C√≥digo recopilado y guardado en: {output_file}")
    return output_file

def main():
    # Obtener la ruta del proyecto (directorio actual)
    project_path = os.path.dirname(os.path.abspath(__file__))
    output_file = os.path.join(project_path, "project_code.txt")
    
    print(f"Recopilando c√≥digo fuente del proyecto en: {project_path}")
    py_files = collect_python_files(project_path)
    print(f"Se encontraron {len(py_files)} archivos Python")
    
    saved_file = save_code_to_file(py_files, output_file)
    print(f"Proceso completado. El c√≥digo se ha guardado en: {saved_file}")

if __name__ == "__main__":
    main()



## ARCHIVO: setup.py
## ==================================================

from setuptools import setup, find_packages

setup(
    name="gatingdrl",
    version="0.1.0",
    description="Time Sensitive Networks Scheduling with Deep Reinforcement Learning",
    author="jpazussa",
    packages=find_packages(),
    install_requires=[
        "torch>=2.0.0",
        "torch-geometric>=2.3.0",
        "networkx>=3.0",
        "matplotlib",
        "pandas",
        "numpy",
        "gymnasium>=0.28.1",
        "sb3-contrib>=2.0.0",
        "stable-baselines3>=2.0.0",
        "plotly>=5.14.0",
    ],
    classifiers=[
        "Development Status :: 3 - Alpha",
        "Intended Audience :: Science/Research",
        "License :: OSI Approved :: MIT License",
        "Programming Language :: Python :: 3.10",
    ],
    python_requires=">=3.10",
)





## ARCHIVO: tools/__init__.py
## ==================================================




## ARCHIVO: tools/analyze_training.py
## ==================================================

#!/usr/bin/env python3

import os
import sys
import json
import argparse
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# A√±adir el directorio ra√≠z al path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from tools.definitions import OUT_DIR

def load_metrics(json_path=None):
    """Carga m√©tricas de un archivo .json de entrenamiento"""
    if json_path is None:
        # Buscar el √∫ltimo archivo metrics.json
        metrics_files = [f for f in os.listdir(OUT_DIR) if f.startswith('training_metrics_') and f.endswith('.json')]
        if not metrics_files:
            print("No se encontraron archivos de m√©tricas de entrenamiento")
            return None
        
        # Ordenar por fecha de modificaci√≥n y usar el m√°s reciente
        metrics_files.sort(key=lambda x: os.path.getmtime(os.path.join(OUT_DIR, x)), reverse=True)
        json_path = os.path.join(OUT_DIR, metrics_files[0])
    
    try:
        with open(json_path, 'r') as f:
            data = json.load(f)
        return data
    except Exception as e:
        print(f"Error cargando m√©tricas: {e}")
        return None

def analyze_rewards(metrics, save_dir=None):
    """Analiza y visualiza la evoluci√≥n de recompensas"""
    if not metrics or 'rewards' not in metrics:
        print("No hay datos de recompensas disponibles")
        return
    
    rewards = metrics['rewards']
    if not rewards:
        return
    
    plt.figure(figsize=(12, 6))
    
    # Gr√°fico de recompensa por episodio
    plt.plot(rewards, 'b-', alpha=0.6)
    
    # A√±adir suavizado (media m√≥vil)
    window = min(10, len(rewards) // 5 + 1)
    if window > 1:
        smooth_rewards = np.convolve(rewards, np.ones(window)/window, mode='valid')
        valid_idx = np.arange(len(smooth_rewards)) + window - 1
        plt.plot(valid_idx, smooth_rewards, 'r-', linewidth=2, label=f'Media m√≥vil ({window} eps)')
    
    # Estad√≠sticas de recompensas
    plt.axhline(y=np.mean(rewards), color='g', linestyle='--', label=f'Media: {np.mean(rewards):.2f}')
    
    # Tramos de an√°lisis
    n = len(rewards)
    if n >= 30:  # Solo si hay suficientes episodios
        first_third = np.mean(rewards[:n//3])
        mid_third = np.mean(rewards[n//3:2*n//3])
        last_third = np.mean(rewards[2*n//3:])
        
        plt.axhline(y=first_third, color='c', linestyle=':', alpha=0.7, 
                   label=f'Primer tercio: {first_third:.2f}')
        plt.axhline(y=last_third, color='m', linestyle=':', alpha=0.7,
                   label=f'√öltimo tercio: {last_third:.2f}')
        
        # Indicar tendencia
        if last_third > first_third * 1.1:
            trend = "‚ÜóÔ∏è Mejora"
        elif last_third < first_third * 0.9:
            trend = "‚ÜòÔ∏è Deterioro"
        else:
            trend = "‚Üí Estable"
            
        plt.text(0.02, 0.02, f"Tendencia: {trend}", transform=plt.gca().transAxes, 
                bbox=dict(facecolor='white', alpha=0.8))
    
    plt.title('Evoluci√≥n de Recompensas por Episodio', fontsize=14)
    plt.xlabel('Episodio')
    plt.ylabel('Recompensa')
    plt.grid(True, alpha=0.3)
    plt.legend()
    
    if save_dir:
        os.makedirs(save_dir, exist_ok=True)
        plt.savefig(os.path.join(save_dir, 'rewards_analysis.png'))
    
    plt.show()

def analyze_action_distribution(metrics, save_dir=None):
    """Analiza la distribuci√≥n de decisiones del agente"""
    if not metrics or 'action_distributions' not in metrics:
        print("No hay datos de distribuciones de acciones disponibles")
        return
    
    action_dist = metrics.get('action_distributions', {})
    if not action_dist:
        return
    
    # Crear un DataFrame para an√°lisis
    action_data = {}
    for action_name, dist in action_dist.items():
        if isinstance(dist, dict):  # Asegurar que es un diccionario
            action_data[action_name] = dist
    
    if not action_data:
        return
        
    # Analizar cada dimensi√≥n de acci√≥n
    plt.figure(figsize=(15, 10))
    
    n_actions = len(action_data)
    n_cols = 2
    n_rows = (n_actions + n_cols - 1) // n_cols
    
    action_names = {
        "offset": "Offset temporal (¬µs)",
        "gcl_strategy": "Estrategia GCL",
        "guard_factor": "Guard factor",
        "priority": "Prioridad",
        "switch_gap": "Gap m√≠nimo",
        "jitter": "Control jitter"
    }
    
    for i, (action, dist) in enumerate(action_data.items()):
        plt.subplot(n_rows, n_cols, i+1)
        
        # Extraer valores y frecuencias
        values = sorted(map(int, dist.keys()))
        counts = [dist.get(str(v), 0) for v in values]
        
        # Calcular entrop√≠a normalizada para medir aleatoriedad
        total = sum(counts)
        if total > 0:
            probs = [count/total for count in counts]
            entropy = -sum(p * np.log2(p) for p in probs if p > 0)
            max_entropy = np.log2(len(values)) if len(values) > 0 else 0
            norm_entropy = entropy / max_entropy if max_entropy > 0 else 0
            entropy_str = f"Entrop√≠a: {norm_entropy:.2f}"
            
            # An√°lisis de distribuci√≥n
            if norm_entropy > 0.95:
                analysis = "Muy uniforme - posible indecisi√≥n"
            elif norm_entropy > 0.85:
                analysis = "Bastante uniforme - poca preferencia"
            elif norm_entropy < 0.3:
                analysis = "Muy concentrada - fuerte preferencia"
            else:
                analysis = "Distribuci√≥n normal"
        else:
            entropy_str = ""
            analysis = "Datos insuficientes"
        
        # Crear gr√°fico
        plt.bar(values, counts)
        plt.title(f"{action_names.get(action, action)}\n{entropy_str}\n{analysis}")
        plt.xlabel("Valor")
        plt.ylabel("Frecuencia")
        plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if save_dir:
        os.makedirs(save_dir, exist_ok=True)
        plt.savefig(os.path.join(save_dir, 'action_distribution.png'))
    
    plt.show()

def analyze_correlation(metrics, save_dir=None):
    """Analiza correlaciones entre decisiones y resultados"""
    # Aqu√≠ se implementar√≠a el an√°lisis de correlaci√≥n entre dimensiones
    # y entre decisiones y rendimiento (recompensas)
    pass

def main():
    parser = argparse.ArgumentParser(description='Analiza m√©tricas de entrenamiento DRL')
    parser.add_argument('--file', type=str, help='Ruta al archivo JSON de m√©tricas (opcional)')
    parser.add_argument('--save-dir', type=str, help='Directorio donde guardar los gr√°ficos (opcional)')
    parser.add_argument('--no-plots', action='store_true', help='No mostrar gr√°ficos, solo guardarlos')
    
    args = parser.parse_args()
    
    # Cargar m√©tricas
    metrics = load_metrics(args.file)
    if not metrics:
        print("No se pudieron cargar las m√©tricas")
        return
    
    # Configurar backend de matplotlib
    if args.no_plots:
        plt.switch_backend('Agg')  # No mostrar gr√°ficos
    
    # Realizar an√°lisis
    print("Analizando recompensas...")
    analyze_rewards(metrics, args.save_dir)
    
    print("Analizando distribuciones de decisiones...")
    analyze_action_distribution(metrics, args.save_dir)
    
    print("An√°lisis completado")

if __name__ == "__main__":
    main()



## ARCHIVO: tools/definitions.py
## ==================================================

import os

ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))  # newtas root directory

OUT_DIR = os.path.join(ROOT_DIR, 'out')
CONFIG_DIR = os.path.join(ROOT_DIR, 'config')

LOG_DIR = os.path.join(OUT_DIR, 'log')
os.makedirs(LOG_DIR, exist_ok=True)




## ARCHIVO: tools/execute.py
## ==================================================

import argparse
import inspect
from typing import Any, Callable, List, Type, get_origin, get_args

def custom_list_type(item_type: Type) -> Callable[[str], List[Any]]:
    """
    Generates a function to convert a comma-separated string into a list of a specific type.

    Parameters
    ----------
    item_type : Type
        The type of the items in the resulting list.

    Returns
    -------
    Callable[[str], List[Any]]
        A function that converts a string to a list of the specified item type.
    """
    def convert(s: str) -> List[Any]:
        return [item_type(item) for item in s.split(',')]
    return convert



def execute_from_command_line(func: Callable):
    """
    Executes a given function using arguments provided from the command line.

    This function uses the inspect module to determine the required and optional
    arguments of the `func` function and their annotations. It then creates an
    ArgumentParser object and adds the function's arguments to it.

    The command-line arguments are expected to be provided in the format
    `--arg_name arg_value`. The function arguments can be either required or
    optional. If an optional argument is not provided, its default value from
    the function definition is used.

    After parsing the command-line arguments, this function calls `func` with
    the parsed arguments.

    Parameters
    ----------
    func : Callable
        The function to be executed. This function can have any number of
        required or optional arguments.

    Raises
    ------
    argparse.ArgumentError
        If a required argument is not provided in the command line.
    """
    # Get the signature of the function
    sig = inspect.signature(func)

    # Create the argument parser
    parser = argparse.ArgumentParser(description=func.__doc__)

    # Add arguments to the parser
    for name, param in sig.parameters.items():
        # Determine the type of the argument
        if param.annotation is not param.empty:
            arg_type = param.annotation
            # Check if the annotation is a generic type
            origin_type = get_origin(arg_type)
            if origin_type is list:
                # Get the inner type of the list
                inner_type = get_args(arg_type)[0]
                arg_type = custom_list_type(inner_type)
        else:
            arg_type = str

        if param.default is param.empty:  # it's a required argument
            parser.add_argument('--' + name, required=True, type=arg_type)
        else:  # it's an optional argument, use default value from function definition
            parser.add_argument('--' + name, default=param.default, type=arg_type)

    args = parser.parse_args()

    # Convert args to a dictionary
    args_dict = vars(args)

    # Call the function with the arguments
    return func(**args_dict)



## ARCHIVO: tools/log_config.py
## ==================================================

import logging

# 1. ‚úÖ Configura el logger de matplotlib.font_manager ANTES de importar matplotlib
logging.getLogger('matplotlib.font_manager').setLevel(logging.WARNING)
logging.getLogger('matplotlib.pyplot').setLevel(logging.WARNING)
logging.getLogger('matplotlib').setLevel(logging.WARNING)

# 2. ‚úÖ Configura la fuente por defecto
import matplotlib.pyplot as plt
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = ['DejaVu Sans']  # Aseg√∫rate de que esta fuente est√© instalada
# Eliminar el par√°metro inv√°lido font.cache_size
# Verificar si el par√°metro get_no_warn es v√°lido
try:
    plt.rcParams['font.get_no_warn'] = True
except KeyError:
    # Ignorar si este par√°metro tampoco es v√°lido
    pass

# 3. Funci√≥n para configurar el logging general
def log_config(filename, level=logging.DEBUG):
    # Configuraci√≥n b√°sica del logger ra√≠z
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(filename, mode='w'),
            logging.StreamHandler()
        ]
    )
    # 4. üö® Evita que matplotlib.font_manager herede el nivel DEBUG
    logging.getLogger('matplotlib').setLevel(logging.WARNING)
    logging.getLogger('matplotlib.font_manager').setLevel(logging.WARNING)
    logging.getLogger('matplotlib.pyplot').setLevel(logging.WARNING)
    logging.getLogger('PIL').setLevel(logging.WARNING)
    


## ARCHIVO: tools/schedule_report.py
## ==================================================




## ARCHIVO: ui/__init__.py
## ==================================================

"""
Interfaces de usuario y visualizaci√≥n
"""



## ARCHIVO: ui/plot_training.py
## ==================================================

import matplotlib.pyplot as plt
import os.path
import sys

# A√±adir el directorio ra√≠z al path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from stable_baselines3.common.results_plotter import plot_results
from tools.execute import execute_from_command_line
from tools.definitions import OUT_DIR


def plot_training_rewards(dirname: str):
    plot_results([dirname], None, 'timesteps', next((s for s in dirname.split(r'/') if '100' in s), None))
    filename = os.path.join(os.path.dirname(dirname), f"training_reward.png")
    print(f"saving the figure to {filename}")
    plt.savefig(filename)
    plt.show(block=False)
    plt.pause(3)  # Espera 3 segundos para mostrar la gr√°fica
    plt.close()


if __name__ == '__main__': 
    execute_from_command_line(plot_training_rewards)
    


## ARCHIVO: ui/show_schedule.py
## ==================================================

import argparse
import os
import glob
import sys

# A√±adir el directorio ra√≠z al path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from tools.definitions import OUT_DIR



def display_schedule_log(log_file=None, by_link=True):
    """
    Display the contents of a scheduling log file.
    
    Args:
        log_file: Path to the log file. If None, displays the most recent log file.
        by_link: If True, show logs organized by link, otherwise by flow.
    """
    if log_file is None:
        # Find the most recent schedule_res log file
        pattern = 'schedule_res_by_link_*.log' if by_link else 'schedule_res_*.log'
        log_files = glob.glob(os.path.join(OUT_DIR, pattern))
        if not log_files:
            # Try the other format if no logs found
            alt_pattern = 'schedule_res_*.log' if by_link else 'schedule_res_by_link_*.log'
            log_files = glob.glob(os.path.join(OUT_DIR, alt_pattern))
            if not log_files:
                print("No scheduling log files found in", OUT_DIR)
                return
        log_file = max(log_files, key=os.path.getmtime)
    
    try:
        with open(log_file, 'r') as f:
            content = f.read()
            
        print("\n" + "="*80)
        print(f"SCHEDULING DETAILS FROM: {os.path.basename(log_file)}")
        print("="*80)
        print(content)
        print("="*80 + "\n")
    except FileNotFoundError:
        print(f"Error: Log file not found: {log_file}")
    except Exception as e:
        print(f"Error reading log file: {e}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Display scheduling log file contents")
    parser.add_argument('--file', type=str, help='Path to specific log file (optional)')
    parser.add_argument('--by-flow', action='store_true', help='Show logs organized by flow instead of by link')
    
    args = parser.parse_args()
    display_schedule_log(args.file, not args.by_flow)



## ARCHIVO: ui/test.py
## ==================================================

import logging
import argparse
import os
import sys
import random  # A√±adido para corregir payloads
import multiprocessing  # A√±adido para detectar cores disponibles

# Configure Qt to use offscreen rendering by default
os.environ["QT_QPA_PLATFORM"] = "offscreen"

# A√±adir el directorio ra√≠z al path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from tools.execute import execute_from_command_line
from core.network.net import generate_flows, generate_graph, Network
from core.scheduler.scheduler import DrlScheduler, ResAnalyzer
from ui.tsn_visualizer_plotly import visualize_tsn_schedule_plotly  # Nuevo visualizador con Plotly
from core.omnet_export import export_omnet_files                    # ‚Üê NUEVO

DEFAULT_MIN_PAYLOAD = 64   # Valor por defecto m√≠nimo razonable
DEFAULT_MAX_PAYLOAD = 1518 # Valor por defecto m√°ximo MTU

def get_best_model_file(topo, alg='MaskablePPO'):
    """Retorna la ruta completa al archivo del mejor modelo para la topolog√≠a y algoritmo dados"""
    from tools.definitions import OUT_DIR
    return os.path.join(OUT_DIR, f"best_model_{topo}_{alg}", "best_model.zip")

def test(topo: str, num_flows: int, num_envs: int = 0,
         best_model_path: str = None, alg: str = 'MaskablePPO', link_rate: int = 100,
         min_payload: int = DEFAULT_MIN_PAYLOAD, max_payload: int = DEFAULT_MAX_PAYLOAD,
         visualize: bool = True, show_log: bool = True, gcl_threshold: int = 30):
    
    # Si num_envs es 0 o negativo, usar todos los cores disponibles
    if num_envs <= 0:
        num_envs = max(1, multiprocessing.cpu_count())
        logging.info(f"Usando {num_envs} entornos en paralelo (n√∫cleos CPU detectados)")
    
    # Configurar logging: INFO para consola, DEBUG para archivo
    from tools.log_config import log_config
    from tools.definitions import OUT_DIR
    log_config(os.path.join(OUT_DIR, f'test_{topo}_{num_flows}.log'), level=logging.INFO)

    # Siempre usar la ruta predeterminada para la topolog√≠a y algoritmo
    if best_model_path is None:
        best_model_path = get_best_model_file(topo, alg)
        logging.info(f"Usando modelo predeterminado: {best_model_path}")
    
    # Verificar si el archivo existe
    if not os.path.exists(best_model_path):
        logging.error(f"Error: Modelo no encontrado: {best_model_path}")
        return False

    graph = generate_graph(topo, link_rate)

    # Generar flujos usando el rango de payload especificado
    flows = generate_flows(graph, num_flows, unidirectional=(topo == "UNIDIR"), min_payload=min_payload, max_payload=max_payload) # Pasar min/max payload
    
    # Debug info: mostrar n√∫mero de flujos generados
    logging.info(f"Generados {len(flows)} flujos para topolog√≠a {topo} (solicitados: {num_flows})")
    
    # Create network with ALL flows - no curriculum learning in test mode
    network = Network(graph, flows)
    
    # Always use DrlScheduler with explicitly disabled curriculum learning
    scheduler = DrlScheduler(network, num_envs=num_envs, use_curriculum=False)
    
    if best_model_path:
        scheduler.load_model(best_model_path, alg)
    
    is_scheduled = scheduler.schedule()
    
    if is_scheduled:
        # Debug info: mostrar n√∫mero de flujos programados
        schedule_res = scheduler.get_res()
        scheduled_flows = set()
        for link_ops in schedule_res.values():
            for flow, _ in link_ops:
                scheduled_flows.add(flow.flow_id)
        
        logging.info(f"Flujos programados: {len(scheduled_flows)} de {num_flows} solicitados")
        
        # Analizar y guardar logs detallados del scheduling
        analyzer = ResAnalyzer(network, schedule_res)
        
        # Apply custom GCL threshold if provided
        if gcl_threshold != 30:  # If different from default
            analyzer.gap_threshold_us = gcl_threshold
            analyzer.recalculate_gcl_tables(gcl_threshold)
        
        log_file = f'schedule_res_by_link_{analyzer.analyzer_id}.log'  # Usar el ID almacenado en el analizador
        log_path = os.path.join(OUT_DIR, log_file)
        logging.info(f"Schedule details saved to {log_path}")

        # Imprimir informaci√≥n de flujos y tablas GCL est√°ticas
        analyzer.print_flow_info()  # Mostrar tabla de flujos independientemente
        
        # Usar el m√©todo actualizado que solo muestra la tabla GCL generada
        analyzer.print_gcl_tables()

        # Mostrar el contenido del log de scheduling en la consola si se solicita
        if show_log:
            try:
                if os.path.exists(log_path):
                    with open(log_path, 'r') as f:
                        log_content = f.read()
                    print("\n" + "="*80)
                    print("SCHEDULING DETAILS BY LINK:")
                    print("="*80)
                    print(log_content)
                    print("="*80 + "\n")
                else:
                    logging.error(f"Archivo de log no encontrado: {log_path}")
            except Exception as e:
                logging.error(f"Error reading schedule log: {e}")
        
        if visualize:
            # Try to visualize with error handling
            try:
                # Visualizar la programaci√≥n usando Plotly (m√°s estable)
                save_path = os.path.join(OUT_DIR, f'tsn_schedule_{topo}_{num_flows}.html')
                visualize_tsn_schedule_plotly(schedule_res, save_path)
                
                logging.info(f"Visualizaci√≥n interactiva guardada en {save_path}")
            except Exception as e:
                logging.error(f"Error durante la visualizaci√≥n: {e}")
                logging.info("Continuando sin visualizaci√≥n interactiva.")

        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        #  NUEVO: exportar .ned y .ini cada vez que haya scheduling OK
        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        try:
            ned_path, ini_path = export_omnet_files(
                network,
                schedule_res,
                analyzer._gcl_tables,   # tablas ya calculadas
                topo,
                OUT_DIR
            )
            logging.info(f"OMNeT++ files escritos:\n  ‚Ä¢ {ned_path}\n  ‚Ä¢ {ini_path}")
        except Exception as e:
            logging.error(f"Error exportando ficheros OMNeT++: {e}")
    else:
        logging.error("Fail to find a valid solution.")

    return is_scheduled


if __name__ == '__main__':
    # ¬´resolve¬ª evita choques de nombres si alg√∫n m√≥dulo a√±ade flags
    parser = argparse.ArgumentParser(conflict_handler="resolve")
    parser.add_argument('--topo', type=str, required=True)
    parser.add_argument('--num_flows', type=int, required=True)
    parser.add_argument('--alg', type=str, default='MaskablePPO')
    parser.add_argument('--link_rate', type=int, default=100)
    # A√±adir argumentos para min/max payload
    parser.add_argument('--min-payload', type=int, default=DEFAULT_MIN_PAYLOAD, help=f"Tama√±o m√≠nimo de payload en bytes (default: {DEFAULT_MIN_PAYLOAD})")
    parser.add_argument('--max-payload', type=int, default=DEFAULT_MAX_PAYLOAD, help=f"Tama√±o m√°ximo de payload en bytes (default: {DEFAULT_MAX_PAYLOAD})")
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ NUEVA interfaz unificada ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    parser.add_argument('--dist', type=str, default='fixed',
                        choices=['fixed', 'uniform', 'exponential', 'gaussian', 'pareto'],
                        help='Distribuci√≥n de separaci√≥n de paquetes')
    parser.add_argument('--dist-params', type=float, nargs='+', default=[],
                        help='Par√°metros de la distribuci√≥n (ver README)')
    parser.add_argument('--visualize', action='store_true', default=True,
                        help='Generar visualizaci√≥n TSN')
    parser.add_argument('--show-log', action='store_true', default=True, help='Mostrar detalles del scheduling en consola')
    parser.add_argument('--gcl-threshold', type=int, default=30, 
                        help='Threshold in ¬µs for GCL entry generation')
    # Se elimin√≥ completamente el par√°metro --best_model_path
    
    args = parser.parse_args()
    
    # Validar rango de payload
    if args.min_payload > args.max_payload:
        logging.error(f"Error: min-payload ({args.min_payload}) no puede ser mayor que max-payload ({args.max_payload})")
        sys.exit(1)
    if args.min_payload < 1 or args.max_payload < 1:
        logging.error("Error: min-payload y max-payload deben ser >= 1")
        sys.exit(1)

    # üëâ Configurar la distribuci√≥n global de separaci√≥n entre paquetes
    from core.network.net import Net
    try:
        Net.set_gap_distribution(args.dist, args.dist_params)
    except (AssertionError, ValueError) as e:
        logging.error(e)
        sys.exit(1)

    # La funci√≥n test ahora determinar√° autom√°ticamente la ruta del modelo y usar√° el rango de payload
    test(args.topo, args.num_flows, 0,
         None, args.alg, args.link_rate, 
         min_payload=args.min_payload, max_payload=args.max_payload,
         visualize=args.visualize, show_log=args.show_log,
         gcl_threshold=args.gcl_threshold)  # Pass the threshold



## ARCHIVO: ui/train.py
## ==================================================

import argparse
import logging
import matplotlib.pyplot as plt
import multiprocessing
import numpy as np
import random
import os
import sys
import shutil  # Para eliminar directorios recursivamente

# Add Qt platform environment variable before any imports that might use Qt
# This helps Qt find the correct platform plugin
os.environ["QT_QPA_PLATFORM"] = "offscreen"  # Use offscreen rendering by default

# Configurar el path antes de cualquier otra importaci√≥n
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
#print(f"Set Python path to include: {os.path.dirname(os.path.dirname(os.path.abspath(__file__)))}")

# Importar m√≥dulos desde rutas relativas
from ui.test import test
from tools.definitions import OUT_DIR, LOG_DIR  # Add LOG_DIR to import
from core.learning.encoder import FeaturesExtractor
# Importar MaskablePPO directamente
from sb3_contrib import MaskablePPO
from core.scheduler.scheduler import DrlScheduler
from core.learning.environment import NetEnv # Eliminar TrainingNetEnv
from tools.log_config import log_config
from core.network.net import FlowGenerator, UniDirectionalFlowGenerator, generate_graph, Network
from tools.definitions import OUT_DIR, LOG_DIR

from stable_baselines3.common.callbacks import EvalCallback
from stable_baselines3.common.results_plotter import load_results, ts2xy
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import SubprocVecEnv

TOPO = 'SIMPLE'  # Cambiado de 'CEV' a 'SIMPLE' para coincidir con el valor por defecto

# Usar siempre el n√∫mero m√°ximo de cores disponibles
NUM_ENVS = max(1, multiprocessing.cpu_count())
NUM_FLOWS = 50

# Cambiar la definici√≥n del algoritmo a una constante fija
DRL_ALG = 'MaskablePPO'

MONITOR_ROOT_DIR = os.path.join(OUT_DIR, "monitor")

DEFAULT_MIN_PAYLOAD = 64   # Valor por defecto m√≠nimo razonable
DEFAULT_MAX_PAYLOAD = 1518 # Valor por defecto m√°ximo MTU


def get_best_model_path(topo=TOPO, alg=DRL_ALG):
    """Retorna la ruta al modelo entrenado seg√∫n la topolog√≠a y algoritmo"""
    return os.path.join(OUT_DIR, f"best_model_{topo}_{alg}")

def get_best_model_file(topo=TOPO, alg=DRL_ALG):
    """Retorna la ruta completa al archivo del modelo (best_model.zip)"""
    return os.path.join(get_best_model_path(topo, alg), "best_model.zip")


def make_env(num_flows, rank: int, topo: str, monitor_dir, training: bool = True, link_rate: int = 100, 
             min_payload: int = DEFAULT_MIN_PAYLOAD, max_payload: int = DEFAULT_MAX_PAYLOAD,
             use_curriculum: bool = True):
    def _init():
        graph = generate_graph(topo, link_rate)

        # Simplificar - eliminar jitters
        # Use UniDirectionalFlowGenerator for UNIDIR topology
        is_unidir = topo == "UNIDIR"
        # Pasar el rango de payload al generador
        if is_unidir:
            flow_generator = UniDirectionalFlowGenerator(graph, min_payload=min_payload, max_payload=max_payload)
        else:
            flow_generator = FlowGenerator(graph, min_payload=min_payload, max_payload=max_payload)

        # Generar todos los flujos - asegurarse de crear exactamente el n√∫mero solicitado
        flows = flow_generator(num_flows)
        logging.info(f"Generados {len(flows)} flujos para {topo} (solicitados: {num_flows})")
        
        network = Network(graph, flows)
        
        # Crear entorno con curriculum learning adaptativo
        env = NetEnv(
            network, 
            curriculum_enabled=use_curriculum,  
            initial_complexity=0.25 if use_curriculum else 1.0,  # Si no hay curriculum, usar 100% de complejidad
            curriculum_step=0.05      # Incrementar 5% por cada √©xito
        )

        # Wrap the environment with Monitor
        env = Monitor(env, os.path.join(monitor_dir, f'{"train" if training else "eval"}_{rank}'))
        return env

    return _init


def train(topo: str, num_time_steps, monitor_dir, num_flows=NUM_FLOWS, pre_trained_model=None, link_rate=100, min_payload: int = DEFAULT_MIN_PAYLOAD, max_payload: int = DEFAULT_MAX_PAYLOAD, use_curriculum: bool = True):
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    #  NUEVO: Limpiar completamente el directorio de salida
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    if os.path.exists(OUT_DIR):
        logging.info(f"Limpiando directorio de salida: {OUT_DIR}")
        shutil.rmtree(OUT_DIR)
    
    # Recrear el directorio vac√≠o
    os.makedirs(OUT_DIR, exist_ok=True)
    os.makedirs(LOG_DIR, exist_ok=True)  # Tambi√©n recreamos LOG_DIR

    # Siempre usar todos los cores disponibles
    n_envs = NUM_ENVS
    logging.info(f"Usando {n_envs} entornos en paralelo (n√∫cleos CPU detectados: {multiprocessing.cpu_count()})")
    
    env = SubprocVecEnv([
        # Ya no hay distinci√≥n entre entornos de entrenamiento y evaluaci√≥n,
        # ambos usan la configuraci√≥n completa de num_flows desde el principio
        make_env(num_flows, i, topo, monitor_dir, link_rate=link_rate, min_payload=min_payload, max_payload=max_payload, use_curriculum=use_curriculum)  # Pasar flag de curriculum
        for i in range(n_envs)
        ])

    if pre_trained_model is not None:
        model = MaskablePPO.load(pre_trained_model, env)
    else:
        policy_kwargs = dict(
            features_extractor_class=FeaturesExtractor,
        )

        # Usar siempre MaskablePPO sin condicionales
        model = MaskablePPO("MlpPolicy", env, policy_kwargs=policy_kwargs, verbose=1)

    eval_env = SubprocVecEnv([
        # El entorno de evaluaci√≥n tambi√©n usa la misma configuraci√≥n
        make_env(num_flows, i, topo, monitor_dir, training=False, link_rate=link_rate, min_payload=min_payload, max_payload=max_payload, use_curriculum=False)  # Siempre desactivar curriculum en evaluaci√≥n
        for i in range(n_envs)
        ])
    
    # Crear callback de evaluaci√≥n y m√©tricas
    callbacks = [
        EvalCallback(eval_env, 
                   best_model_save_path=get_best_model_path(topo=topo, alg=DRL_ALG),
                   log_path=OUT_DIR, 
                   eval_freq=max(10000 // n_envs, 1))
    ]

    # Train the agent with just the eval callback
    model.learn(total_timesteps=num_time_steps, callback=callbacks)

    logging.info("Training complete.")

    # NUEVO: Ejecutar un episodio final y guardar los detalles del scheduling
    logging.info("Ejecutando episodio final para generar informe detallado...")
    obs = env.reset()
    done = False
    final_info = None
    
    # Ejecutar un episodio completo con el modelo entrenado
    while not done:
        action_masks = np.vstack(env.env_method('action_masks'))
        action, _ = model.predict(obs, deterministic=True, action_masks=action_masks.astype(np.int8))
        obs, _, dones, infos = env.step(action)
        done = any(dones)
        if done:
            # Buscar informaci√≥n de √©xito en alg√∫n entorno
            for i, is_done in enumerate(dones):
                if is_done and infos[i].get('success'):
                    final_info = infos[i]
                    break
    
    # Remove report generation code
    if final_info and final_info.get('ScheduleRes'):
        logging.info(f"Scheduling final exitoso con {len(final_info['ScheduleRes'])} enlaces programados")

    logging.info("------Finish learning------")
    return None  # Return None instead of metrics


def moving_average(values, window):
    """
    Smooth values by doing a moving average
    :param values: (numpy array)
    :param window: (int)
    :return: (numpy array)
    """
    weights = np.repeat(1.0, window) / window
    return np.convolve(values, weights, "valid")


def plot_results(log_folder, title="Learning Curve"):
    """
    plot the results

    :param log_folder: (str) the save location of the results to plot
    :param title: (str) the title of the task to plot
    """
    try:
        x, y = ts2xy(load_results(log_folder), "timesteps")
        y = moving_average(y, window=50)
        # Truncate x
        x = x[len(x) - len(y):]

        fig = plt.figure(title)
        plt.plot(x, y)
        plt.xlabel("Number of Timesteps")
        plt.ylabel("Rewards")
        plt.title(title + " Smoothed")
        plt.savefig(os.path.join(log_folder, "reward.png"))
        
        # Use non-blocking display and catch any errors
        try:
            plt.show(block=False)
            plt.pause(3)  # Espera 3 segundos para mostrar la gr√°fica
            plt.close()
        except Exception as e:
            logging.warning(f"Could not display plot interactively: {e}")
            logging.info("Plot saved to file, continuing without interactive display.")
    except Exception as e:
        logging.error(f"Error plotting results: {e}")
        logging.info("Continuing without plotting.")


def main():
    # Mover todas las declaraciones global al inicio de la funci√≥n
    global TOPO, NUM_ENVS, DRL_ALG
    
    # specify an existing model to train.
    parser = argparse.ArgumentParser(conflict_handler="resolve")
    parser.add_argument('--time_steps', type=int, required=True)
    parser.add_argument('--num_flows', type=int, nargs='?', default=NUM_FLOWS)
    parser.add_argument('--debug', action='store_true')
    # Eliminar la opci√≥n de especificar num_envs, ahora es autom√°tico
    parser.add_argument('--topo', type=str, default="SIMPLE", help="Topology type (e.g., SIMPLE, UNIDIR)")
    parser.add_argument('--link_rate', type=int, default=100)
    # A√±adir argumentos para min/max payload
    parser.add_argument('--min-payload', type=int, default=DEFAULT_MIN_PAYLOAD, help=f"Tama√±o m√≠nimo de payload en bytes (default: {DEFAULT_MIN_PAYLOAD})")
    parser.add_argument('--max-payload', type=int, default=DEFAULT_MAX_PAYLOAD, help=f"Tama√±o m√°ximo de payload en bytes (default: {DEFAULT_MAX_PAYLOAD})")
    # Cambio: Hacer --model opcional con un valor por defecto de None
    parser.add_argument('--model', type=str, default=None, 
                       help="Ruta opcional a un modelo pre-entrenado. Por defecto no carga ninguno.")
    # ------------- argumentos para la separaci√≥n probabil√≠stica -------------
    parser.add_argument('--gap-mode', type=str, default='fixed',
                        choices=['fixed', 'uniform', 'exponential', 'gaussian', 'pareto'],
                        help="Modo de c√°lculo del gap en ¬µs entre creaciones de paquetes")
    parser.add_argument('--pkt-gap', type=int, default=0,
                        help="‚Ä¢ fixed ‚áí valor constante\n"
                             "‚Ä¢ exponential ‚áí media Œº (Œª = 1/Œº)")
    parser.add_argument('--gap-uniform', type=int, nargs=2, metavar=('MIN', 'MAX'),
                        help="S√≥lo con --gap-mode uniform: intervalo [MIN,MAX]")
    parser.add_argument('--gap-gauss', type=int, nargs=2, metavar=('MEAN','STD'),
                        help="S√≥lo con --gap-mode gaussian: media Œº y desv√≠o œÉ (¬µs)")
    parser.add_argument('--gap-pareto', type=float, nargs=2, metavar=('ALPHA','XM'),
                        help="S√≥lo con --gap-mode pareto: shape Œ± y scale xm")
    # ------------- interfaz unificada de distribuci√≥n de gaps ---------------
    parser.add_argument('--dist', type=str, default='fixed',
                        choices=['fixed', 'uniform', 'exponential', 'gaussian', 'pareto'],
                        help="Tipo de distribuci√≥n de separaci√≥n de paquetes")
    parser.add_argument('--dist-params', type=float, nargs='+', default=[],
                        help="Par√°metros num√©ricos de la distribuci√≥n (ver doc)")
    parser.add_argument('--arrival-dist', type=str, default='set',
                        choices=['set', 'uniform', 'exponential'],
                        help="Distribuci√≥n para el per√≠odo de cada flujo")
    # A√±adir opci√≥n para controlar el curriculum learning
    parser.add_argument('--curriculum', action='store_true', default=True,
                      help='Usar curriculum learning adaptativo (por defecto activado)')
    parser.add_argument('--no-curriculum', action='store_false', dest='curriculum',
                      help='Desactivar curriculum learning adaptativo')
   
    args = parser.parse_args()


    if args.link_rate is not None:
        support_link_rates = [100, 1000]
        assert args.link_rate in support_link_rates, \
            f"Unknown link rate {args.link_rate}, which is not in supported link rates {support_link_rates}"

    # Validar rango de payload
    if args.min_payload > args.max_payload:
        logging.error(f"Error: min-payload ({args.min_payload}) no puede ser mayor que max-payload ({args.max_payload})")
        sys.exit(1)
    if args.min_payload < 1 or args.max_payload < 1:
        logging.error("Error: min-payload y max-payload deben ser >= 1")
        sys.exit(1)

    # Eliminar procesamiento de jitters
    TOPO = args.topo
    # NUM_ENVS = args.num_envs  # Esta l√≠nea se elimina

    # üëâ Aplicar el valor elegido antes de crear cualquier entorno
    from core.network.net import Net
    if args.pkt_gap < 0:
        logging.error("Error: pkt-gap debe ser >= 0")
        sys.exit(1)
    # -------- aplicar configuraci√≥n global del gap -------- #
    Net.PACKET_GAP_MODE = args.gap_mode
    if args.gap_mode == 'uniform':
        if args.gap_uniform is None or len(args.gap_uniform) != 2:
            logging.error("Debe proporcionar --gap-uniform MIN MAX con --gap-mode uniform")
            sys.exit(1)
        Net.PACKET_GAP_UNIFORM = tuple(args.gap_uniform)
    Net.PACKET_GAP_EXTRA = max(args.pkt_gap, 0)

    if args.gap_mode == 'gaussian':
        if args.gap_gauss is None or len(args.gap_gauss) != 2:
            logging.error("Debe proporcionar --gap-gauss MEAN STD con --gap-mode gaussian")
            sys.exit(1)
        Net.PACKET_GAP_GAUSS = tuple(args.gap_gauss)

    if args.gap_mode == 'pareto':
        if args.gap_pareto is None or len(args.gap_pareto) != 2:
            logging.error("Debe proporcionar --gap-pareto ALPHA XM con --gap-mode pareto")
            sys.exit(1)
        Net.PACKET_GAP_PARETO = tuple(args.gap_pareto)

    # -------- aplicar configuraci√≥n global del gap -------- #
    try:
        Net.set_gap_distribution(args.dist, args.dist_params)
    except AssertionError as e:
        logging.error(e)
        sys.exit(1)

    log_config(os.path.join(OUT_DIR, f"train.log"), logging.DEBUG)

    logging.info(args)

    done = False
    i = 0
    MONITOR_DIR = None
    while not done:
        try:
            MONITOR_DIR = os.path.join(MONITOR_ROOT_DIR, str(i))
            os.makedirs(MONITOR_DIR, exist_ok=False)
            done = True
        except OSError:
            i += 1
            continue
    assert MONITOR_DIR is not None

    logging.info("start training...")
    # metrics variable is ignored since it's now None
    train(args.topo, args.time_steps,
          MONITOR_DIR,  # Pasar MONITOR_DIR como par√°metro
          num_flows=args.num_flows,
          pre_trained_model=args.model,  # Usar args.model (que podr√≠a ser None)
          link_rate=args.link_rate,
          min_payload=args.min_payload, # Pasar min/max payload
          max_payload=args.max_payload,
          use_curriculum=args.curriculum)

    # Add try-except block around plotting
    try:
        plot_results(MONITOR_DIR)
    except Exception as e:
        logging.error(f"Error during plotting: {e}")
        logging.info("Continuing without plotting.")

    # Remove metrics summary code
    logging.info(f"Training completed successfully.")

    # Ya no necesitamos pasar la ruta del modelo, test la determinar√° autom√°ticamente
    test(args.topo, args.num_flows, NUM_ENVS, alg=DRL_ALG, link_rate=args.link_rate, min_payload=args.min_payload, max_payload=args.max_payload)


if __name__ == "__main__":
    main()



## ARCHIVO: ui/tsn_visualizer_plotly.py
## ==================================================

import plotly.graph_objects as go
from plotly.subplots import make_subplots
import numpy as np
from core.network.net import Net
import os
import math
import re
import colorsys
from collections import defaultdict
import webbrowser
import logging

from tools.definitions import OUT_DIR
from core.scheduler.scheduler import ScheduleRes

def visualize_tsn_schedule_plotly(schedule_res: ScheduleRes, save_path=None):
    """
    Genera una visualizaci√≥n interactiva de la programaci√≥n TSN usando Plotly.
    
    Args:
        schedule_res: Resultado de la programaci√≥n
        save_path: Ruta para guardar el HTML, por defecto es 'out/tsn_schedule_plotly.html'
    """
    if not schedule_res:
        print("No hay resultados de programaci√≥n para visualizar.")
        return
 
    
    # Organizar datos por enlace
    link_data = defaultdict(list)
    all_periods = set()
    all_flow_ids = set()
    max_end_time = 0
    
    print("\nProcesando datos para visualizaci√≥n Plotly...")
    
    # ----- NUEVO: Extraer informaci√≥n de ocupaci√≥n de switches -----
    switch_busy_periods = defaultdict(list)
    
    # Extraer y procesar datos
    for link, operations in schedule_res.items():
        link_str = str(link)
        match = re.search(r"Link\('([^']+)', '([^']+)'\)", link_str)
        if not match:
            print(f"Error: No se pudo extraer origen/destino de {link_str}")
            continue
        
        src, dst = match.group(1), match.group(2)
        link_name = f"{src} ‚Üí {dst}"
        
        flow_ids = [flow.flow_id for flow, _ in operations]
        print(f"Enlace: {link_name}, Flujos: {flow_ids}")
        
        for flow, operation in operations:
            all_flow_ids.add(flow.flow_id)
            all_periods.add(flow.period)
            
            # Calculate earliest_time on the fly based on the current Operation structure
            earliest_time = operation.start_time if operation.gating_time is None else operation.gating_time
            
            link_data[link_name].append({
                'flow_id': flow.flow_id,
                'period': flow.period,
                'start_time': operation.start_time,
                'earliest_time': earliest_time,  # Computed value
                'gating_time': operation.gating_time,
                'latest_time': operation.latest_time,
                'end_time': operation.end_time,
                'reception_time': operation.reception_time,  # visualizaci√≥n
                # ---- NEW: acci√≥n RL ----
                'offset_idx': getattr(operation, 'offset_idx', None),
                'offset_us' : getattr(operation, 'offset_us',  None),
            })
            
            max_end_time = max(max_end_time, operation.end_time)
            
            # ACTUALIZADO: Extraer ocupaci√≥n del switch si este enlace sale de un switch
            if src.startswith('S') and not src.startswith('SRV'):
                # El puerto del switch est√° ocupado durante la transmisi√≥n SOLAMENTE
                # El switch termina de estar ocupado cuando el paquete sale completamente
                guard_time = link.interference_time() if hasattr(link, "interference_time") else 1.22
                switch_busy_start = earliest_time
                switch_busy_end = operation.end_time  # CORREGIDO: Eliminar guard_time adicional
                
                # Almacenar per√≠odo de ocupaci√≥n para el switch
                switch_busy_periods[src].append({
                    'flow_id': flow.flow_id,
                    'start': switch_busy_start,
                    'end': switch_busy_end,  # El switch termina su trabajo cuando completa la transmisi√≥n
                    'period': flow.period
                })
    
    # Calcular el hiperper√≠odo
    hyperperiod = 1
    for period in all_periods:
        hyperperiod = math.lcm(hyperperiod, period)
    
    print(f"Hiperper√≠odo calculado: {hyperperiod}¬µs")
    
    # Ordenar enlaces para visualizaci√≥n
    # Considera como "switch-link" todo enlace cuyo **origen** sea S<n>
    switch_links = [lnk for lnk in link_data.keys() if re.match(r'^S\d+\s+‚Üí', lnk)]
    client_links = [lnk for lnk in link_data.keys() if lnk not in switch_links]
    sorted_links = sorted(switch_links) + sorted(client_links)
    
    # --- TODOS los hops de switch van con gate ‚áí un solo esquema de colores ---
    # IMPORTANTE: Definir flow_colors ANTES de cualquier referencia
    flow_colors = {}
    for i, flow_id in enumerate(sorted(all_flow_ids)):
        hue = (i * 0.618033988749895) % 1
        r, g, b = colorsys.hsv_to_rgb(hue, 0.7, 0.9)
        flow_colors[flow_id] = f'rgb({int(r*255)},{int(g*255)},{int(b*255)})'
    
    # ----- NUEVO: A√±adir switches a la lista de elementos a visualizar -----
    # Ordenar switches para mostrarlos primero
    sorted_switches = sorted(switch_busy_periods.keys())
    
    # Crear figura de Plotly con subplots compartiendo el eje X
    fig = make_subplots(
        rows=3, 
        cols=1,
        row_heights=[0.15, 0.70, 0.15],  # Proporciones ajustadas para incluir switches
        vertical_spacing=0.02,     # Reducir espacio entre gr√°ficas para mejor integraci√≥n visual
        shared_xaxes=True,         # Compartir eje X para que el zoom se sincronice
        subplot_titles=["Ocupaci√≥n de Switches", "Programaci√≥n de Flujos TSN", "Gate Control List (GCL)"]
    )
    
    # -- NUEVO: SECCI√ìN 1: OCUPACI√ìN DE SWITCHES --
    for i, switch_name in enumerate(sorted_switches):
        periods = switch_busy_periods[switch_name]
        
        # Replicar per√≠odos para todo el hiperper√≠odo
        for period_info in periods:
            flow_id = period_info['flow_id']
            flow_period = period_info['period']
            repetitions = hyperperiod // flow_period
            
            for rep in range(repetitions):
                time_offset = rep * flow_period
                start_time = period_info['start'] + time_offset
                end_time = period_info['end'] + time_offset
                duration = end_time - start_time
                
                # A√±adir barra para el per√≠odo ocupado
                fig.add_trace(
                    go.Bar(
                        x=[duration],
                        y=[switch_name],
                        orientation='h',
                        base=[start_time],
                        name=f"{switch_name} ocupado ({flow_id})",
                        marker=dict(
                            color='rgba(150,150,150,0.7)',
                            pattern=dict(
                                shape="x",
                                solidity=0.3,
                                fgcolor="black"
                            )
                        ),
                        showlegend=False,
                        hoverinfo='text',
                        hovertext=f"Switch: {switch_name}<br>Ocupado por flujo: {flow_id}<br>Inicio: {start_time}¬µs<br>Fin: {end_time}¬µs",
                    ),
                    row=1, col=1
                )
    
    # -- SECCI√ìN 2: GR√ÅFICO PRINCIPAL DE BARRAS (ahora en la segunda fila) --
    # Recopilar eventos de GCL
    gcl_events = []
    for link, operations in schedule_res.items():
        link_str = str(link)
        match = re.search(r"Link\('([^']+)', '([^']+)'\)", link_str)
        if not match:
            continue
        src, dst = match.group(1), match.group(2)
        link_name = f"{src} ‚Üí {dst}"
        
        # Solo procesar enlaces que salen de un switch
        if not src.startswith('S'):
            continue
        
        # Tratar todos los flujos como cr√≠ticos para un GCL peri√≥dico
        link_operations = operations
        if link_operations:
            print(f"Generando GCL peri√≥dico para enlace switch: {link_name}")
            
            # Calcular el hiperper√≠odo para todos los flujos
            periods = [flow.period for flow, _ in link_operations]
            hyperperiod_link = 1
            for period in periods:
                hyperperiod_link = math.lcm(hyperperiod_link, period)
                
            # Generar eventos GCL para todos los flujos con el mismo formato
            # ------------------------------------------------------------------
            # MODIFICACI√ìN: Generar eventos "0" (cerrar) para TODOS los paquetes
            # en su tiempo de recepci√≥n, sin filtrar por tama√±o de gap.
            # ------------------------------------------------------------------

            # 1) Ordenar operaciones por inicio real de transmisi√≥n
            ops_sorted = sorted(link_operations,
                                key=lambda p: (p[1].gating_time or p[1].start_time))
            n = len(ops_sorted)

            if n < 2:
                continue

            # 2) Hiperper√≠odo individual del enlace
            periods = [flow.period for flow, _ in ops_sorted]
            hyperperiod_link = 1
            for p in periods:
                hyperperiod_link = math.lcm(hyperperiod_link, p)

            # Variable para el umbral de gap (usado para filtrar qu√© eventos mostrar)
            gap_thr_us = 50  # umbral de espacio m√≠nimo para crear entradas GCL
            
            # PASO 1: Recopilar todos los tiempos de transmisi√≥n y recepci√≥n
            all_transmission_times = []
            all_reception_times = []
            
            # Recopilar todos los tiempos de transmisi√≥n y recepci√≥n
            for i in range(n):
                f_curr, op_curr = ops_sorted[i]
                tx_start = op_curr.gating_time if op_curr.gating_time is not None else op_curr.start_time
                # Para cada paquete, repetirlo durante todo el hiperper√≠odo
                repetitions = hyperperiod_link // f_curr.period
                for rep in range(repetitions):
                    offset = rep * f_curr.period
                    # Guardar tiempo de inicio y recepci√≥n (normalizado al hiperper√≠odo)
                    tx_t = (tx_start + offset) % hyperperiod_link
                    rx_t = (op_curr.reception_time + offset) % hyperperiod_link
                    all_transmission_times.append((tx_t, f_curr.flow_id))
                    all_reception_times.append((rx_t, f_curr.flow_id))
            
            # Ordenar los tiempos
            all_transmission_times.sort(key=lambda x: x[0])
            all_reception_times.sort(key=lambda x: x[0])
            
            # PASO 2: Generar eventos GCL analizando los gaps significativos
            gcl_close_events = []  # Lista temporal para eventos de cierre (0)
            
            # Buscar gaps significativos entre recepci√≥n y siguiente transmisi√≥n
            for i in range(len(all_reception_times)):
                rx_time, rx_flow = all_reception_times[i]
                
                # Encontrar el siguiente tiempo de transmisi√≥n despu√©s de esta recepci√≥n
                next_tx_time = None
                next_tx_flow = None
                
                for tx_time, tx_flow in all_transmission_times:
                    # B√∫squeda circular (considerando el wraparound del hiperper√≠odo)
                    if tx_time > rx_time:
                        # Caso normal: siguiente TX est√° despu√©s de RX en este ciclo
                        next_tx_time = tx_time
                        next_tx_flow = tx_flow
                        break
                
                # Si no se encontr√≥ ninguno, buscar el primero (wraparound)
                if next_tx_time is None and all_transmission_times:
                    next_tx_time = all_transmission_times[0][0] + hyperperiod_link
                    next_tx_flow = all_transmission_times[0][1]
                
                # Calcular el gap (si hay transmisiones)
                if next_tx_time is not None:
                    gap = next_tx_time - rx_time
                    if gap < 0:
                        gap += hyperperiod_link  # Ajustar para gaps negativos (wraparound)
                    
                    # Solo considerar gaps que superen el umbral
                    if gap > gap_thr_us:
                        # A√±adir eventos de cierre/apertura
                        gcl_close_events.append((rx_time, rx_flow, next_tx_time, next_tx_flow))
            
            # PASO 3: Generar los pares de eventos 0/1 para cada gap significativo
            for close_time, close_flow, next_tx_time, next_tx_flow in gcl_close_events:
                # Calcular repeticiones para todo el hiperper√≠odo
                repetitions = hyperperiod_link // hyperperiod_link  # Simplificado a 1
                
                for rep in range(repetitions):
                    offset = rep * hyperperiod_link
                    
                    # A√±adir evento de cierre (0) en el tiempo de recepci√≥n
                    close_t = (close_time + offset) % hyperperiod_link
                    gcl_events.append((close_t, 0, close_flow, link_name, True))
                    
                    # A√±adir evento de apertura (1) EXACTAMENTE cuando empieza el siguiente paquete
                    open_t = (next_tx_time + offset) % hyperperiod_link
                    gcl_events.append((open_t, 1, next_tx_flow, link_name, True))

    # Ordenar eventos por tiempo
    gcl_events.sort(key=lambda x: x[0])
    
    # Para barras muy estrechas o marcadores de ventana de transmisi√≥n, mejorar visibilidad
    for i, link_name in enumerate(sorted_links):
        if (link_name not in link_data):
            continue
            
        operations = link_data[link_name]
        
        for op_data in operations:
            flow_id = op_data['flow_id']
            flow_period = op_data['period']
            repetitions = hyperperiod // flow_period
            
            for rep in range(repetitions):
                # Calcular tiempos con el desplazamiento del per√≠odo
                time_offset = rep * flow_period
                # Usar los tiempos recalculados si hubo offset
                start_time = op_data['start_time'] + time_offset
                end_time = op_data['end_time'] + time_offset
                earliest_time = op_data['earliest_time'] + time_offset  # Use the computed value
                latest_time = op_data['latest_time'] + time_offset
                gating_time = op_data['gating_time'] + time_offset if op_data['gating_time'] is not None else None
                reception_time = op_data['reception_time'] + time_offset if op_data['reception_time'] is not None else None

                # Tiempo de transmisi√≥n real (desde gating_time o start_time si no hay gating)
                actual_start_time = gating_time if gating_time is not None else start_time
                transmission_duration = end_time - actual_start_time

                # --- NUEVO: Barra de Tiempo de Espera con distinci√≥n de tipos ---
                if gating_time is not None and gating_time > start_time:
                    wait_duration = gating_time - start_time
                    
                    # Solo dibujamos la barra base gris (Total)
                    fig.add_trace(
                        go.Bar(
                            x=[wait_duration],
                            y=[link_name],
                            orientation='h',
                            base=[start_time],
                            name="Espera Total",
                            marker=dict(
                                color='rgba(200,200,200,0.3)',
                                line=dict(width=1, color='black'),
                            ),
                            showlegend=False,
                            hoverinfo='none',
                        ),
                        row=2, col=1
                    )

                # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ BARRAS DE ESPERA DESGLOSADAS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                if gating_time is not None and gating_time > start_time:
                    wb = op.wait_breakdown   # dict con 'min_gap', 'other', 'total'

                    # Dibujar la base gris con la espera total
                    fig.add_trace(
                        go.Bar(
                            x=[wb['total']],
                            y=[link_name],
                            orientation='h',
                            base=[start_time],
                            name="Espera Total",
                            marker=dict(color='rgba(200,200,200,0.25)'),
                            showlegend=False,
                            hoverinfo='none',
                        ),
                        row=2, col=1
                    )

                    waits = [
                        # Solo las esperas controladas por RL
                        ('min_gap', 'rgba(220, 20, 60,0.8)', "\\", "Separaci√≥n m√≠nima"),
                        ('other',   'rgba(120,120,120,0.5)', "",   "Otros"),
                    ]

                    offset_acc = 0
                    for key, color, pattern, label in waits:
                        w = wb.get(key, 0)
                        if w == 0:
                            continue
                        fig.add_trace(
                            go.Bar(
                                x=[w],
                                y=[link_name],
                                orientation='h',
                                base=[start_time + offset_acc],
                                name=label,
                                marker=dict(color=color,
                                            pattern=dict(shape=pattern, solidity=0.35)),
                                showlegend=(rep == 0),
                                legendgroup=f"wait_{key}",
                                hoverinfo='text',
                                hovertext=f"{label}: {w}¬µs<br>Flujo: {flow_id}",
                            ),
                            row=2, col=1
                        )
                        offset_acc += w

                # Guard-band visualization removed

                # --- Barra principal para transmisi√≥n ---
                fig.add_trace(
                    go.Bar(
                        x=[transmission_duration],
                        y=[link_name],
                        orientation='h',
                        base=[actual_start_time],
                        name=flow_id,                    # sigue como texto interno
                        marker=dict(
                            color=flow_colors[flow_id],
                            opacity=0.8,
                            line=dict(width=1, color='black')
                        ),
                        text=flow_id,
                        textposition='inside',
                        insidetextanchor='middle',
                        hoverinfo='text',
                        hovertext=(
                            f"Flujo: {flow_id}"
                            f"<br>Per√≠odo: {flow_period}¬µs"
                            f"<br>Inicio Tx: {actual_start_time}¬µs"
                            f"<br>Fin Tx: {end_time}¬µs"
                            f"<br>Recibido: {reception_time}¬µs"
                        ),
                        showlegend=False,                # ‚Üê leyenda desactivada
                    ),
                    row=2, col=1
                )

                # Obtener detalles de las decisiones del agente si est√°n disponibles
                guard_factor = getattr(operation, 'guard_factor', 1.0)
                min_gap = getattr(operation, 'min_gap_value', 1.0)
                
                # Tooltip completo con par√°metros de RL
                hover_text = (
                    f"Flujo: {flow.flow_id}"
                    f"<br>Per√≠odo: {flow.period}¬µs"
                    f"<br>Inicio Tx: {actual_start_time}¬µs"
                    f"<br>Fin Tx: {end_time}¬µs"
                    f"<br>Recibido: {reception_time}¬µs"
                    f"<br>Guard Factor: {guard_factor:.2f}"
                    f"<br>Separaci√≥n M√≠n: {min_gap:.1f}¬µs"
                )

                # Marcadores ---
                # Marcador para start_time (cu√°ndo podr√≠a haber empezado)
                fig.add_trace(
                    go.Scatter(
                        x=[start_time],
                        y=[link_name],
                        mode='markers',
                        marker=dict(symbol='line-ns', size=15, color='green', line=dict(width=2)),
                        name='Start Time (Available)',
                        showlegend=False,
                        hoverinfo='text',
                        hovertext=f"Disponible (Start Time): {start_time}¬µs<br>Flujo: {flow_id}",
                    ),
                    row=2, col=1
                )

                # Marcador para gating_time (cu√°ndo empez√≥ realmente si hubo gating)
                if gating_time is not None:
                    fig.add_trace(
                        go.Scatter(
                            x=[gating_time],
                            y=[link_name],
                            mode='markers',
                            marker=dict(symbol='line-ns', size=18, color='red', line=dict(width=3)),
                            name='Gate Time (Actual Start)',
                            showlegend=False,
                            hoverinfo='text',
                            hovertext=f"Inicio Real (Gate Time): {gating_time}¬µs<br>Flujo: {flow_id}",
                        ),
                        row=2, col=1
                    )

                # Marcador para latest_time (l√≠mite ventana gating)
                # Solo relevante si hay gating
                if gating_time is not None:
                    fig.add_trace(
                        go.Scatter(
                            x=[latest_time],
                            y=[link_name],
                            mode='markers',
                            marker=dict(symbol='line-ns', size=15, color='orange', line=dict(width=2)),
                            name='Latest Time',
                            showlegend=False,
                            hoverinfo='text',
                            hovertext=f"Latest Time: {latest_time}¬µs<br>Flujo: {flow_id}",
                        ),
                        row=2, col=1
                    )
    
    # -- SECCI√ìN 3: REPRESENTACI√ìN DEL GCL (ahora en la tercera fila) --
    if gcl_events:
        print(f"Dibujando {len(gcl_events)} eventos GCL")
        
        # Crear representaci√≥n del GCL como un gr√°fico mejorado
        gcl_times = []
        gcl_values = []
        gcl_texts = []
        gcl_colors = []
        
        # Debug adicional para identificar el problema
        print("\nDETALLE DE EVENTOS GCL ORDENADOS:")
        print(f"{'Tiempo':8} | {'Estado':6} | {'Flujo':5} | {'Tipo':10}")
        print(f"{'-'*8} | {'-'*6} | {'-'*5} | {'-'*10}")
        
        # Mostrar eventos de cierre (0) Y apertura (1)
        for time, state, flow_id, link_name, is_gating in gcl_events:
            tipo = "Gated" if is_gating else "Non-Gated"
            print(f"{time:8} | {state:6} | {flow_id:5} | {tipo:10}")
            
            gcl_times.append(time)
            gcl_values.append(state)  # Incluir ambos estados: 0 y 1
            
            event_type = "Cierre" if state == 0 else "Apertura"
            gcl_texts.append(f"{event_type} de gate<br>Tiempo: {time}¬µs<br>Flujo: {flow_id}<br>Tipo: {tipo}")
            # Azul para estado 0 (cierre), Verde para estado 1 (apertura)
            gcl_colors.append('rgba(0,0,255,0.9)' if state == 0 else 'rgba(0,180,0,0.9)')

        # ------------------------------------------------------------------
        #  MOSTRAR TABLA GCL EN CONSOLA PARA VERIFICACI√ìN R√ÅPIDA
        # ------------------------------------------------------------------
        print("\n" + "="*60)
        print("TABLA GCL GENERADA (t, estado)")
        print("-"*60)
        for t, v in zip(gcl_times, gcl_values):
            print(f"{t:>8} ¬µs | {v}")
        print("="*60 + "\n")
        
        # L√≠nea para conectar los eventos GCL y hacerlos m√°s visibles
        fig.add_trace(
            go.Scatter(
                x=gcl_times,
                y=['GCL'] * len(gcl_times),
                mode='lines',
                line=dict(color='lightgray', width=1.5, dash='dot'),
                showlegend=False,
                hoverinfo='none'
            ),
            row=3, col=1
        )
        
        # Dibujar los eventos GCL con texto forzado para distinguir entre 0 y 1
        fig.add_trace(
            go.Scatter(
                x=gcl_times,
                y=['GCL'] * len(gcl_times),
                mode='markers+text',
                marker=dict(
                    size=15,  # Tama√±o fijo para todos los eventos
                    color=gcl_colors,
                    symbol='circle',
                    line=dict(width=2, color='black')
                ),
                text=[str(v) for v in gcl_values],  # Mostrar "0" o "1" seg√∫n el valor
                textposition='middle center',
                textfont=dict(color='white', size=10, family='Arial Black'),
                name='GCL Events',
                showlegend=False,
                hoverinfo='text',
                hovertext=gcl_texts,
            ),
            row=3, col=1
        )
    else:
        # Si no hay eventos GCL, mostrar un mensaje
        fig.add_annotation(
            x=hyperperiod/2,
            y=0,
            text="No hay eventos GCL para mostrar",
            showarrow=False,
            font=dict(size=12, color="gray"),
            row=3, col=1
        )
    
    # A√±adir l√≠nea vertical para el hiperper√≠odo
    fig.add_vline(
        x=hyperperiod,
        line_width=2,
        line_dash="solid",
        line_color="blue",
        annotation_text=f"Hiperper√≠odo: {hyperperiod}¬µs",
        annotation_position="top",
        annotation_font_size=12,
        annotation_font_color="blue"
    )
    
    # Configuraci√≥n de dise√±o mejorada para ocupar toda la p√°gina
    fig.update_layout(
        title=f"Programaci√≥n TSN de Flujos (Hiperper√≠odo: {hyperperiod}¬µs)",
        barmode='overlay',
        height=max(800, len(sorted_links) * 45 + 250),  # Altura ajustada para mejor visualizaci√≥n
        width=1400,                                     # Ancho aumentado para mejor visualizaci√≥n
        margin=dict(l=50, r=50, t=80, b=80),           # M√°rgenes reducidos
        legend=dict(
            orientation="h",
            yanchor="bottom",
            y=-0.18,                                    # Ajustar posici√≥n de leyenda
            xanchor="center",
            x=0.5,
            title="Flujos"
        ),
        plot_bgcolor='white',
        hovermode='closest',
    )
    
    # Agregar leyenda interactiva para filtrar por tipo de evento
    fig.update_layout(
        legend=dict(
            title="Tipo de evento",
            orientation="h",
            yanchor="bottom",
            y=-0.2,
            xanchor="center",
            x=0.5,
        )
    )
    
    # Vincular los ejes X para que el zoom se sincronice entre gr√°ficas
    tick_interval = max(100, hyperperiod // 20)  # m√°x. 20 ticks
    fig.update_xaxes(
        title="Tiempo (¬µs)",
        range=[-hyperperiod*0.02, hyperperiod*1.02],
        gridcolor='lightgray',
        griddash='dot',
        tickvals=list(range(0, hyperperiod + tick_interval, tick_interval)),
        row=1, col=1
    )
    
    fig.update_xaxes(
        title="Tiempo (¬µs)",
        range=[-hyperperiod*0.02, hyperperiod*1.02],
        showgrid=True,
        gridcolor='lightgray',
        griddash='dot',
        tickvals=list(range(0, hyperperiod + tick_interval, tick_interval)),
        row=2, col=1,
        matches='x'  # Esto sincroniza este eje con el eje X de la primera gr√°fica
    )
    
    fig.update_xaxes(
        title="Tiempo (¬µs)",
        range=[-hyperperiod*0.02, hyperperiod*1.02],
        showgrid=True,
        gridcolor='lightgray',
        griddash='dot',
        tickvals=list(range(0, hyperperiod + tick_interval, tick_interval)),
        row=3, col=1,
        matches='x'  # Esto sincroniza este eje con el eje X de la primera gr√°fica
    )
    
    # Mejorar el estilo de las etiquetas del eje Y
    fig.update_yaxes(
        title="Switches",
        row=1, col=1,
        linecolor='black',
        gridcolor='rgba(200,200,200,0.3)'
    )
    
    fig.update_yaxes(
        title="Enlaces",
        row=2, col=1,
        linecolor='black',
        gridcolor='rgba(200,200,200,0.3)'
    )
    
    # Manejo especial para el eje Y del GCL
    fig.update_yaxes(
        showticklabels=True,                # Mostrar etiquetas
        tickvals=['GCL'],                   # Establecer valores espec√≠ficos
        ticktext=['GCL'],                   # Establecer texto para esos valores
        row=3, col=1,
        linecolor='black',
        gridcolor='rgba(200,200,200,0.3)'
    )
    
    # Agregar leyenda adicional para s√≠mbolos
    symbols_legend = [
        dict(name="Disponible (Start)", marker=dict(color="green", symbol="line-ns", size=10)),
        dict(name="Inicio Real (Gate)", marker=dict(color="red", symbol="line-ns", size=10)),
        dict(name="√öltimo Inicio (Latest)", marker=dict(color="orange", symbol="line-ns", size=10)),
        dict(name="Espera por FCFS/Switch Ocupado", marker=dict(color="rgba(100,100,255,0.7)", symbol="square", size=10)),
        dict(name="Espera por Separaci√≥n M√≠nima", marker=dict(color="rgba(220,20,60,0.7)", symbol="square", size=10)),
    ]
    
    for item in symbols_legend:
        # Asegurarse de que el marcador no contenga 'pattern'
        marker_config = item.get('marker', {})
        if 'pattern' in marker_config:
             del marker_config['pattern'] # Eliminar si existe por error
                
        fig.add_trace(
            go.Scatter(
                x=[None],
                y=[None],
                mode='markers',
                marker=marker_config, # Usar la configuraci√≥n corregida
                name=item.get('name', ''),
                showlegend=True
            )
        )
    
    # Agregar leyenda para la ocupaci√≥n de switches
    fig.add_trace(
        go.Bar(
            x=[None],
            y=[None],
            name="Switch Ocupado",
            marker=dict(
                color='rgba(150,150,150,0.7)',
                pattern=dict(shape="x", solidity=0.3, fgcolor="black")
            ),
            showlegend=True
        )
    )
    
    # ‚îÄ‚îÄ leyendas limpiadas: s√≥lo min-gap (decisi√≥n RL) ‚îÄ‚îÄ
    fig.add_trace(
        go.Bar(
            x=[None], y=[None], name="Espera ‚ñ∏ min-gap",
            marker=dict(color='rgba(220, 20, 60,0.8)',
                        pattern=dict(shape="\\", solidity=0.35)),
            showlegend=True)
    )
    
    # Guardar como HTML interactivo con opciones para mejor visualizaci√≥n
    if save_path is None:
        save_path = os.path.join(OUT_DIR, 'tsn_schedule_plotly.html')
    
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    fig.write_html(
        save_path,
        include_plotlyjs='cdn',
        full_html=True,
        include_mathjax='cdn',
        config={
            'scrollZoom': True,            # Permitir zoom con rueda del rat√≥n
            'displayModeBar': True,        # Mostrar barra de herramientas
            'displaylogo': False,          # No mostrar logo de Plotly
            'toImageButtonOptions': {      # Configuraci√≥n para guardar imagen
                'format': 'png',
                'filename': 'tsn_schedule',
                'height': 1200,
                'width': 1800,
                'scale': 2                 # Alta resoluci√≥n
            }
        }
    )
    print(f"Visualizaci√≥n interactiva TSN guardada en: {save_path}")
    
    # Try to open in browser with error handling
    try:
        webbrowser.open('file://' + os.path.abspath(save_path))
    except Exception as e:
        logging.warning(f"Could not open browser automatically: {e}")
        print(f"Please open the visualization manually at: file://{os.path.abspath(save_path)}")
    
    return fig

def visualize_lcm_cycle_plotly(schedule_res: ScheduleRes, save_path=None):
    """
    Funci√≥n de compatibilidad que redirige a visualize_tsn_schedule_plotly
    """
    print("La funcionalidad de visualizaci√≥n del hiperper√≠odo est√° integrada en visualize_tsn_schedule_plotly.")
    print("Llamando a visualize_tsn_schedule_plotly...")
    return visualize_tsn_schedule_plotly(schedule_res, save_path)

if __name__ == "__main__":
    print("Este m√≥dulo debe ser importado y utilizado desde test.py")
    print("Ejemplo: visualize_tsn_schedule_plotly(scheduler.get_res())")



