# ===================================

## ARCHIVO: __init__.py
## ==================================================



import path_setup  # Carga las rutas definidas

__version__ = "0.1.0"






## ARCHIVO: core/__init__.py
## ==================================================




## ARCHIVO: core/learning/__init__.py
## ==================================================

"""
Entornos y componentes de aprendizaje por refuerzo
"""

from .environment import NetEnv  # La misma exportaciÃ³n para mantener compatibilidad
# Exporta ambos extractores
from .encoder import FeaturesExtractor          # GIN-baseline
from .hats_extractor import HATSExtractor       # NUEVO: HATS
from .maskable_sac import MaskableSAC           # NUEVO: SAC con mÃ¡scaras







## ARCHIVO: core/learning/encoder.py
## ==================================================

import torch
import torch.nn as nn
import gymnasium as gym 

from stable_baselines3.common.torch_layers import BaseFeaturesExtractor



# ------------------------------------------------------------------
# Utilidades
# ------------------------------------------------------------------
class _FourierEncoding(nn.Module):
    """Devuelve [x, sin(2^k Ï€x), cos(2^k Ï€x)] para k < num_bands."""
    def __init__(self, num_bands: int = 4):
        super().__init__()
        self.register_buffer("freq_bands", 2 ** torch.arange(num_bands).float() * torch.pi)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Expande cada caracterÃ­stica con pares seno/coseno a distintas
        frecuencias, preservando la forma (*batch*, features).
        """
        enc = [x]
        for f in self.freq_bands:          # type: ignore[attr-defined]
            enc.append(torch.sin(f * x))
            enc.append(torch.cos(f * x))
        return torch.cat(enc, dim=-1)


class _ResidualBlock(nn.Module):
    def __init__(self, dim: int, p: float = 0.1):
        super().__init__()
        self.net = nn.Sequential(
            nn.LayerNorm(dim),
            nn.SiLU(),
            nn.Linear(dim, dim),
            nn.SiLU(),
            nn.Dropout(p),
            nn.Linear(dim, dim),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return x + self.net(x)

# ------------------------------------------------------------------
# Extractores
# ------------------------------------------------------------------

class SimpleExtractor(BaseFeaturesExtractor):
    """MLP de referencia (64 dims)."""
    def __init__(self, observation_space: gym.spaces.Box):
        super().__init__(observation_space, features_dim=64)
        inp = observation_space.shape[0]
        self.network = nn.Sequential(
            nn.Linear(inp, 128), nn.ReLU(),
            nn.Linear(128, 128), nn.ReLU(),
            nn.Linear(128, 64),
        )

    def forward(self, obs: torch.Tensor):
        return self.network(obs)


class AdvancedExtractor(BaseFeaturesExtractor):
    """
    Â· CodificaciÃ³n de Fourier (4 bandas) â†’ 36 dims<br>
    Â· ProyecciÃ³n a 128 + 2 bloques residuales + LayerNorm/SiLU<br>
    Devuelve un vector de 128 dims.
    """
    def __init__(self, observation_space: gym.spaces.Box, num_bands: int = 4):
        super().__init__(observation_space, features_dim=128)
        self.encode = _FourierEncoding(num_bands)
        self.project = nn.Linear((1 + 2 * num_bands) * observation_space.shape[0], 128)
        self.blocks = nn.Sequential(
            _ResidualBlock(128), _ResidualBlock(128),
            nn.LayerNorm(128), nn.SiLU(),
        )

    def forward(self, obs: torch.Tensor) -> torch.Tensor:
        x = self.encode(obs)
        x = self.project(x)
        return self.blocks(x)


# âœ” Hacemos que el entrenamiento utilice el extractor avanzado por defecto.
FeaturesExtractor = AdvancedExtractor



## ARCHIVO: core/learning/env_actions.py
## ==================================================

import math
import logging
from typing import Dict, List, Tuple, Any, Optional

import numpy as np
from core.network.operation import Operation
from core.network.net import Net

from core.learning.env_utils import SchedulingError, ErrorType, find_next_event_time, check_temp_operations



# --------------------------------------------------------------------------- #
# â†’ NUEVA filosofÃ­a de GCL â†                                                 #
#   â–¸ Cada vez que un hop *espera* en el egress-switch                       #
#     se aÃ±aden **exactamente 2 entradas** (close n).                   #
#   â–¸ La longitud del ciclo no cambia (se mantiene en 1 para indicar que     #
#     gestionaremos las reglas como eventos independientes).                 #
# --------------------------------------------------------------------------- #

# Todas las utilidades relacionadas con la reserva dinÃ¡mica de GCL se han
# eliminado.  A partir de ahora la lista se calcula *a posteriori* en el
# analizador de resultados.

pass  # <-- mantener el archivo, sin lÃ³gica de GCL

# --------------------------------------------------------------------------- #
# Funciones para la implementaciÃ³n del mÃ©todo step                            #
# --------------------------------------------------------------------------- #
def process_step_action(self, action):
    """Procesa la acciÃ³n en el mÃ©todo step"""
    # -------------------------------------------------------------- #
    # 0. Interpretar la acciÃ³n con componentes reducidos              #
    # -------------------------------------------------------------- #
    # La acciÃ³n ahora es un array con 3 componentes
    guard_factor_idx  = int(action[0])
    switch_gap_idx    = int(action[1])
    flow_selection    = int(action[2])
    
    # Convertir Ã­ndices en valores reales para usar en el algoritmo
    offset_us = 0
    guard_factor_values = [0.5, 0.75, 1.0, 1.5, 2.0]
    guard_factor = guard_factor_values[guard_factor_idx]
    switch_gap_values = [0.5, 1.0, 1.5, 2.0]
    switch_gap = switch_gap_values[switch_gap_idx]
    
    # Registrar las decisiones del agente para visualizaciÃ³n
    flow = self.current_flow()
    
    # Almacenar todas las decisiones del agente (sin gcl_strategy)
    self.agent_decisions = {
        'guard_factor': guard_factor,
        'switch_gap': switch_gap,
        'flow_selection': flow_selection
    }
    
    # NUEVO: MÃ©tricas de operaciÃ³n para el anÃ¡lisis
    self.last_operation_info = {
        'bandwidth_used': 0,
        'bandwidth_total': 0,
        'wait_breakdown': {
            'switch': 0,
            'gap': 0,
            'total': 0
        }
    }

    flow = self.current_flow()
    hop_idx = self.flow_progress[self.current_flow_idx]
    link = self.link_dict[flow.path[hop_idx]]
    gating = True
    trans_time = link.transmission_time(flow.payload)
    
    # Guard time ahora usa el factor elegido por el agente
    base_guard_time = link.interference_time()
    guard_time = base_guard_time * guard_factor
    
    # Registrar las decisiones del agente para visualizaciÃ³n posterior
    self.guard_time_selected = guard_time
    self.switch_gap_selected = switch_gap

    # Si el ORIGEN del enlace es un switch â‡’ este hop ES un egress
    def _get_src(node_pair):
        return node_pair[0] if isinstance(node_pair, tuple) \
               else node_pair.split('-')[0]

    sw_src = _get_src(link.link_id)
    is_egress_from_switch = sw_src.startswith('S') and not sw_src.startswith('SRV')
    
    # Use fixed conservative GCL strategy
    gcl_strategy = 0
    
    return (flow, hop_idx, link, gating, trans_time,
            guard_time, guard_factor,               # âŠ  NUEVO
            offset_us, switch_gap, sw_src,
            is_egress_from_switch, gcl_strategy)



## ARCHIVO: core/learning/env_utils.py
## ==================================================

import math
import os
import random
import logging
from collections import defaultdict
from dataclasses import dataclass
from enum import Enum, auto
from typing import Optional, List, Dict, Any



import numpy as np
import gymnasium as gym
from gymnasium import spaces

from core.network.operation import Operation, check_operation_isolation
from core.network.net import Net, Network, generate_flows, generate_simple_topology, FlowGenerator, UniDirectionalFlowGenerator

# --------------------------------------------------------------------------- #
#  Definiciones de error                                                      #
# --------------------------------------------------------------------------- #
class ErrorType(Enum):
    PeriodExceed = auto()

class SchedulingError(Exception):
    def __init__(self, error_type: ErrorType, msg: str):
        super().__init__(f"Error: {msg}")
        self.error_type = error_type
        self.msg = msg

# Funciones auxiliares para NetEnv
def find_next_event_time(link_busy_until, switch_busy_until, current_time):
    """Encuentra el siguiente tiempo de evento programado despuÃ©s de current_time"""
    next_event_time = float('inf')
    
    # Buscar en todos los tiempos de ocupaciÃ³n de enlaces
    for time in link_busy_until.values():
        if time > current_time and time < next_event_time:
            next_event_time = time
            
    # Buscar en todos los tiempos de ocupaciÃ³n de switches
    for time in switch_busy_until.values():
        if time > current_time and time < next_event_time:
            next_event_time = time
    
    return next_event_time if next_event_time < float('inf') else None

def check_valid_link(link, operation, current_flow, links_operations):
    """Comprueba si una operaciÃ³n es vÃ¡lida en un enlace"""
    for f_rhs, op_rhs in links_operations[link]:
        offset = check_operation_isolation(
            (operation, current_flow.period), (op_rhs, f_rhs.period)
        )
        if offset is not None:
            return offset
    return None

def check_temp_operations(temp_operations, links_operations, current_flow):
    """Verifica todas las operaciones temporales"""
    for link, op in temp_operations:
        offset = check_valid_link(link, op, current_flow, links_operations)
        if offset is not None:
            return offset
    return None



## ARCHIVO: core/learning/environment.py
## ==================================================

import math
import os
import random
import logging
from collections import defaultdict
from dataclasses import dataclass
from enum import Enum, auto
from typing import Optional, List, Dict, Any


import numpy as np
import gymnasium as gym
from gymnasium import spaces

from tools.definitions import LOG_DIR
from core.network.operation import Operation, check_operation_isolation
from core.network.net import Net, Network, generate_flows, generate_simple_topology, FlowGenerator, UniDirectionalFlowGenerator

# Importar desde los mÃ³dulos auxiliares
from core.learning.env_utils import ErrorType, SchedulingError, find_next_event_time
from core.learning.env_utils import check_valid_link, check_temp_operations
from core.learning.env_actions import process_step_action

# --------------------------------------------------------------------------- #
#  Entorno TSN / DRL                                                          #
# --------------------------------------------------------------------------- #
class NetEnv(gym.Env):
    """Entorno de simulaciÃ³n TSN para aprendizaje por refuerzo."""
    
    # Usar la constante centralizada en Net
    MIN_SWITCH_GAP = Net.SWITCH_GAP_MIN

    @dataclass
    class GclInfo:
        gcl_cycle: int = 1
        gcl_length: int = 0    # --------------------------------------------------------------------- #
    #  InicializaciÃ³n                                                       #
    # --------------------------------------------------------------------- #
    def __init__(self, network: Optional[Network] = None, 
                curriculum_enabled: bool = True,
                initial_complexity: float = 0.25,
                curriculum_step: float = 0.05,
                use_graph_observation: bool = False) -> None:
        super().__init__()

        # --- Curriculum Learning Adaptativo ---
        self.curriculum_enabled = curriculum_enabled
        self.initial_complexity = initial_complexity 
        self.curriculum_step = curriculum_step
        self.current_complexity = initial_complexity
        self.consecutive_successes = 0
        self.original_flows = []  # Store the original complete set of flows
        
        # --- Graph observation support ---
        self.use_graph_observation = use_graph_observation
        
        # Si se proporciona una red, guardar su estructura original
        if network is not None:
            self.total_flows = len(network.flows)
            self.base_graph = network.graph
            # Guardar todos los flujos originales
            self.original_flows = list(network.flows)
            
            # Registro explÃ­cito para depuraciÃ³n del modo curriculum
            self.logger = logging.getLogger(f"{__name__}.{os.getpid()}")
            self.logger.setLevel(logging.INFO)
            self.logger.info(f"Inicializando entorno con {self.total_flows} flujos (curriculum: {curriculum_enabled}, complejidad inicial: {initial_complexity})")
            
            if curriculum_enabled and initial_complexity < 1.0:
                # En modo curriculum, reducir el nÃºmero inicial de flujos
                active_flows = int(self.total_flows * self.initial_complexity)
                active_flows = max(5, active_flows)  # MÃ­nimo 5 flujos para empezar
                
                # Seleccionar subset de flujos para el nivel de complejidad actual
                active_flows_list = self.original_flows[:active_flows]
                network = Network(network.graph, active_flows_list)
                self.logger.info(f"Modo curriculum ACTIVADO: usando {active_flows}/{self.total_flows} flujos inicialmente")
            else:
                # Si curriculum estÃ¡ desactivado o complejidad es 1.0, usar todos los flujos
                self.logger.info(f"Modo curriculum DESACTIVADO: usando todos los {self.total_flows} flujos")
                self.current_complexity = 1.0  # Forzar complejidad completa
        
        # Si no se entrega una red, construir topologÃ­a y flujos sencillos
        if network is None:
            g = generate_simple_topology()
            f = generate_flows(g, 10)
            network = Network(g, f)
            self.total_flows = len(network.flows)
            self.base_graph = g
            # Crear generador de flujos apropiado
            self.flow_generator = FlowGenerator(g)

        # --- Estructuras base -------------------------------------------- #
        self.graph = network.graph
        self.flows = list(network.flows)               # lista estable
        self.line_graph, self.link_dict = (
            network.line_graph,
            network.links_dict,
        )

        # --- Estados internos ------------------------------------------- #
        self.num_flows: int = len(self.flows)
        # Reloj de referencia global (solo para la observaciÃ³n)
        # Se calcula siempre como el prÃ³ximo evento mÃ¡s cercano
        self.global_time: int = 0
        self.flow_progress: List[int] = [0] * self.num_flows  # hop en curso de cada flujo
        self.flow_completed: List[bool] = [False] * self.num_flows
        self.flow_first_tx: List[int | None] = [None] * self.num_flows
        self.current_flow_idx: int = 0                 # para roundâ€‘robin

        self.links_operations = defaultdict(list)

        #  ğŸ”€  Las estructuras ligadas a GCL ya no se utilizan.  Mantener
        #  Ãºnicamente la planificaciÃ³n de operaciones; el cÃ¡lculo de las
        #  tablas se traslada al *ResAnalyzer*.

        self.temp_operations: List[tuple] = []         # op en construcciÃ³n

        # ğŸ”¹ Placeholder para mantener compatibilidad con cÃ³digo heredado.
        #   Ya no se llena ni se usa, pero evita AttributeError.
        self.links_gcl: dict = {}

        # â±ï¸  NUEVO: reloj "ocupadoâ€‘hasta" por enlace
        #    (cuÃ¡ndo queda libre cada enlace)
        self.link_busy_until = defaultdict(int)
        # â±ï¸â±ï¸ reloj "ocupadoâ€‘hasta" por switch **sÃ³lo para EGRESOS**
        self.switch_busy_until = defaultdict(int)
        
        # ğŸ“Š NUEVO: Registro del Ãºltimo tiempo de llegada por switch
        # Para garantizar separaciÃ³n mÃ­nima entre paquetes
        self.switch_last_arrival = defaultdict(int)

        # â²ï¸  Ãšltimo instante en que **se creÃ³** (primer hop) un paquete
        #     â€“ solo se usa para imponer la separaciÃ³n en el PRIMER enlace
        self.last_packet_start = -Net.PACKET_GAP_EXTRA

        # ğŸš¦ NUEVO: secciÃ³n crÃ­tica global â€“ "una sola cola"
        self.global_queue_busy_until = 0

        # --- Espacios de observaciÃ³n y acciÃ³n --------------------------- #
        self._setup_spaces()

        # --- Logger ------------------------------------------------------ #
        self.logger = logging.getLogger(f"{__name__}.{os.getpid()}")
        self.logger.setLevel(logging.INFO)

        # Cache: Â«Â¿es nodo final?Â»
        self._es_node_cache: Dict[Any, bool] = {}
        
        # Variable para datos de operaciÃ³n
        self.last_operation_info = {}
        self.agent_decisions = {}

        # Orden FIFO inmutable: simplemente el Ã­ndice de creaciÃ³n del flujo
        # (flows ya estÃ¡ en el mismo orden en que se generaron).
        self._fifo_order = list(range(self.num_flows))

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        #  ğŸ“Š  MÃ©tricas de latencia extremo-a-extremo (Âµs) por episodio
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        #   Se irÃ¡n llenando a medida que cada flujo se completa.
        #   Se resumen al final del episodio en environment_impl.step.
        self._latency_samples: list[int] = []

        # â±ï¸  lista para almacenar la latencia de cada flujo completado
        self._flow_latencies: list[int] = []

    # ----------------------------------------------------------------- #
    #  Helper estÃ¡tico (picklable) para muestrear la separaciÃ³n global  #
    # ----------------------------------------------------------------- #
    @staticmethod
    def _next_packet_gap() -> int:
        """
        Devuelve la separaciÃ³n entre paquetes (Âµs) delegando la lÃ³gica
        Ã­ntegramente a ``Net.sample_packet_gap``.  
        Esta versiÃ³n elimina cÃ³digo muerto y evita ramas nunca alcanzadas.
        """
        return Net.sample_packet_gap()

    # --------------------------------------------------------------------- #
    #  ConfiguraciÃ³n de gymnasium                                           #
    # --------------------------------------------------------------------- #    def _default_gcl_info(self):
        return self.GclInfo()

    def _setup_spaces(self):
        # MODIFICAR LA OBSERVACIÃ“N: Incluir informaciÃ³n de mÃºltiples flujos (hasta 5)
        # Para cada flujo: [perÃ­odo_norm, payload_norm, progreso, tiempo_espera_norm, es_seleccionable]
        # AdemÃ¡s de las caracterÃ­sticas globales originales
        FLUJOS_OBSERVABLES = 5  # NÃºmero de flujos que podemos observar a la vez
        FEATURES_POR_FLUJO = 5  # CaracterÃ­sticas por flujo
        FEATURES_GLOBALES = 4   # CaracterÃ­sticas globales (tiempo, ocupaciÃ³n GCL, etc.)
        
        # NUEVO: Almacenar constantes para uso posterior
        self.NUM_FLUJOS_OBSERVABLES = FLUJOS_OBSERVABLES
        self.FEATURES_POR_FLUJO = FEATURES_POR_FLUJO
        self.FEATURES_GLOBALES = FEATURES_GLOBALES
        
        if self.use_graph_observation:
            # Dict observation space for graph-based extractors (HATSExtractor)
            num_nodes = len(self.graph.nodes())
            num_edges = len(self.graph.edges())
            max_hops = max(len(flow.path) for flow in self.flows) if self.flows else 10
            
            self.observation_space = spaces.Dict({
                'features_matrix': spaces.Box(
                    low=-1.0, high=1.0, 
                    shape=(num_nodes, 8), dtype=np.float32
                ),
                'edge_features': spaces.Box(
                    low=0.0, high=1.0, 
                    shape=(num_edges, 4), dtype=np.float32
                ),
                'adjacency_matrix': spaces.Box(
                    low=0, high=1, 
                    shape=(num_nodes, num_nodes), dtype=np.int32
                ),
                'flow_feature': spaces.Box(
                    low=0.0, high=1.0, 
                    shape=(6,), dtype=np.float32
                ),
                'link_feature': spaces.Box(
                    low=0.0, high=1.0, 
                    shape=(4,), dtype=np.float32
                ),
                'remain_hops': spaces.Box(
                    low=0, high=max_hops, 
                    shape=(max_hops,), dtype=np.int32
                )
            })
        else:
            # Box observation space for standard extractors (FeaturesExtractor)
            self.observation_space = spaces.Box(
                low=0.0, 
                high=1.0, 
                shape=(FEATURES_GLOBALES + FLUJOS_OBSERVABLES * FEATURES_POR_FLUJO,), 
                dtype=np.float32
            )

        # â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
        # â•‘  ESPACIO DE ACCIÃ“N (3 DIMENSIONES)                                    â•‘
        # â•‘  0. Guard factor [0-4]                                                â•‘
        # â•‘  1. Gap mÃ­nimo en switch [0-3]                                        â•‘
        # â•‘  2. SelecciÃ³n de flujo candidato [0-4]                                â•‘
        # â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        self.action_space = spaces.MultiDiscrete([
            5,                 # Guard factor
            4,                 # Gap mÃ­nimo switch
            FLUJOS_OBSERVABLES # SelecciÃ³n de flujo
        ])

    # --------------------------------------------------------------------- #
    #  Utilidades de selecciÃ³n de flujo / enlace                            #
    # --------------------------------------------------------------------- #
    def select_next_flow_by_agent(self, flow_selection):
        """
        Permite que el agente RL seleccione el prÃ³ximo flujo a programar.
        
        Args:
            flow_selection: Ãndice del flujo seleccionado por el agente (0-4)
        """
        # Usar el Ã­ndice de flujo seleccionado por el agente si estÃ¡ disponible
        if hasattr(self, 'current_candidate_flows') and self.current_candidate_flows:
            if 0 <= flow_selection < len(self.current_candidate_flows):
                selected_idx = self.current_candidate_flows[flow_selection]
                if not self.flow_completed[selected_idx]:
                    self.current_flow_idx = selected_idx
                    return

        # Si la selecciÃ³n directa falla, usar FIFO como fallback
        now = self.global_time
        chosen = None  # (idx, wait_time)

        for idx, done in enumerate(self.flow_completed):
            if done:
                continue
            prog = self.flow_progress[idx]
            if prog == 0 or prog >= len(self.flows[idx].path):
                continue
            next_link = self.flows[idx].path[prog]
            if not next_link[0].startswith('S'):
                continue

            prev_link_id = self.flows[idx].path[prog-1]
            prev_ops = self.links_operations[self.link_dict[prev_link_id]]
            if not prev_ops:
                continue  # el hop previo aÃºn no fue programado
            prev_op = prev_ops[-1][1]
            wait_time = now - prev_op.reception_time
            if chosen is None or wait_time > chosen[1]:
                chosen = (idx, wait_time)

        if chosen:
            self.current_flow_idx = chosen[0]
            return
                
        # Si no se encontrÃ³ un flujo adecuado, buscar cualquier flujo no completado
        if self.flow_completed[self.current_flow_idx]:
            for idx, completed in enumerate(self.flow_completed):
                if not completed:
                    self.current_flow_idx = idx
                    return

    def current_flow(self):
        return self.flows[self.current_flow_idx]

    def current_link(self):
        idx = self.flow_progress[self.current_flow_idx]
        flow = self.current_flow()
        return self.link_dict[flow.path[idx]]

    # ----------------------------------------------------------------- #
    #  FIFO helper                                                      #
    # ----------------------------------------------------------------- #
    def _next_fifo_idx(self) -> int | None:
        """
        Devuelve el Ã­ndice del **Ãºnico** flujo que debe programarse ahora
        segÃºn FIFO estricto (el que mÃ¡s tiempo lleva esperando).
        Si no hay flujos pendientes, devuelve None.
        
        Prioridad: 
        1. Paquetes que estÃ¡n esperando en un switch (para transmisiÃ³n inmediata)
        2. Paquetes con mayor tiempo de espera
        """
        best: tuple[int, int, bool, int] | None = None   # (wait_time, -idx, is_in_switch, hop_idx)
        best_idx: int | None = None

        now = self.global_time
        for idx, done in enumerate(self.flow_completed):
            if done:
                continue
            hop_idx = self.flow_progress[idx]
            if hop_idx >= len(self.flows[idx].path):
                continue

            # --- tiempo desde que el paquete estÃ¡ "listo" ---
            if hop_idx == 0:
                ready_t = 0                          # nunca se ha transmitido
                is_in_switch = False
            else:
                prev_link_id = self.flows[idx].path[hop_idx - 1]
                prev_ops = self.links_operations.get(self.link_dict[prev_link_id], [])
                if not prev_ops:
                    continue        # hop previo aÃºn no programado â‡’ no listo
                
                prev_op = prev_ops[-1][1]
                ready_t = prev_op.reception_time
                
                # Determinar si el paquete estÃ¡ esperando en un switch
                # Si el destino del enlace anterior es un switch y el origen del siguiente enlace
                # coincide con ese switch, entonces el paquete estÃ¡ esperando en un switch
                dst_node = prev_link_id[1] if isinstance(prev_link_id, tuple) else prev_link_id.split('-')[1]
                is_in_switch = dst_node.startswith('S') and not dst_node.startswith('SRV')

            wait = now - ready_t
            fifo_rank = -idx               # menor Ã­ndice â‡’ mÃ¡s antiguo
            
            # Prioridad: 1) paquetes en switch, 2) tiempo de espera, 3) Ã­ndice mÃ¡s bajo
            cand = (int(is_in_switch), wait, fifo_rank, hop_idx)

            if (best is None) or (cand > best):
                best = cand
                best_idx = idx

        return best_idx

    # --------------------------------------------------------------------- #
    #  Reinicio                                                             #
    # --------------------------------------------------------------------- #
    def reset(self, *, seed=None, options=None):
        super().reset(seed=seed)
        
        # Si es necesario incrementar la complejidad
        if self.curriculum_enabled and self.consecutive_successes >= 3:
            if self.increase_complexity():
                # Calcular nÃºmero de flujos segÃºn complejidad actual
                num_flows = int(self.total_flows * self.current_complexity)
                num_flows = max(5, min(self.total_flows, num_flows))
                
                if self.original_flows:
                    # Usar un subset consistente de los flujos originales
                    active_flows_list = self.original_flows[:num_flows]
                    network = Network(self.base_graph, active_flows_list)
                    
                    # Actualizar las estructuras con la nueva red
                    self.graph = network.graph
                    self.flows = list(network.flows)
                    self.line_graph, self.link_dict = (
                        network.line_graph,
                        network.links_dict,
                    )
                    self.num_flows = len(self.flows)
                    
                    self.logger.info(f"Curriculum: Incrementando a {num_flows}/{self.total_flows} flujos (complejidad: {self.current_complexity:.2f})")
                
            self.consecutive_successes = 0

        self.global_time = 0
        self.flow_progress = [0] * self.num_flows
        self.flow_completed = [False] * self.num_flows
        self.flow_first_tx = [None] * self.num_flows
        self.current_flow_idx = 0

        self.links_operations.clear()
        self.links_gcl = {}   # reiniciar placeholder
        self.temp_operations.clear()
        self._es_node_cache.clear()
        self.link_busy_until.clear()
        self.switch_busy_until.clear()
        self.switch_last_arrival.clear()    # NUEVO: Limpiar tiempos de llegada
        self.global_queue_busy_until = 0
        self.last_packet_start = -Net.PACKET_GAP_EXTRA

        # Limpiar mÃ©tricas de latencia para el nuevo episodio
        self._latency_samples.clear()

        # â±ï¸  reiniciar latencias e2e acumuladas
        self._flow_latencies.clear()

        return self._get_observation(), {}    # --------------------------------------------------------------------- #
    #  ObservaciÃ³n                                                          #
    # --------------------------------------------------------------------- #
    def _get_observation(self):
        if self.use_graph_observation:
            return self._get_graph_observation()
        else:
            return self._get_vector_observation()
    
    def _get_vector_observation(self):
        """Get traditional Box observation for standard extractors"""
        # Creamos una observaciÃ³n que contenga informaciÃ³n sobre mÃºltiples flujos
        # La observaciÃ³n tendrÃ¡: [caracterÃ­sticas_globales, caracterÃ­sticas_flujo1, caracterÃ­sticas_flujo2, ...]
        
        # 1. CaracterÃ­sticas globales de la red
        global_time_norm = self.global_time / 10000  # Normalizar tiempo global
        
        # Ya no hay GCL dinÃ¡mico â†’ utilizaciÃ³n 0 siempre
        gcl_util_norm = 0.0
        
        # Calcular porcentaje de flujos completados
        completion_rate = sum(self.flow_completed) / self.num_flows
        
        # Nivel de curriculum
        curriculum_norm = self.current_complexity
        
        # Vector de caracterÃ­sticas globales
        global_features = [global_time_norm, gcl_util_norm, completion_rate, curriculum_norm]
        
        # 2. Obtener una lista de flujos candidatos para programar
        candidatos = []
        for idx, completed in enumerate(self.flow_completed):
            if not completed:
                flow = self.flows[idx]
                hop_idx = self.flow_progress[idx]
                
                # Verificar que el flujo tiene un hop vÃ¡lido para programar
                if hop_idx >= len(flow.path):
                    continue
                    
                # Calcular cuÃ¡nto tiempo ha estado esperando (si aplica)
                wait_time = 0
                if hop_idx > 0:
                    prev_link_id = flow.path[hop_idx-1]
                    prev_ops = self.links_operations.get(self.link_dict[prev_link_id], [])
                    if prev_ops:
                        prev_op = prev_ops[-1][1]
                        wait_time = max(0, self.global_time - prev_op.reception_time)
                
                # Normalizar valores - convertir a caracterÃ­sticas significativas 
                period_norm = flow.period / 10000  # PerÃ­odos mÃ¡s cortos â†’ valores mÃ¡s pequeÃ±os
                payload_norm = flow.payload / Net.MTU  # Payloads mÃ¡s pequeÃ±os â†’ valores mÃ¡s pequeÃ±os
                hop_progress = self.flow_progress[idx] / len(flow.path)
                wait_time_norm = min(wait_time / flow.period, 1.0)  # Normalizar por periodo
                
                # Calcular urgencia basada en plazo prÃ³ximo
                # Cuanto menor sea el tiempo hasta el deadline, mayor urgencia (1.0 = muy urgente)
                deadline_remaining = (flow.period - (self.global_time % flow.period)) / flow.period
                urgency = 1.0 - deadline_remaining  # 0.0 = acaba de empezar, 1.0 = casi vencido
                
                # Log del flujo candidato con sus caracterÃ­sticas para depuraciÃ³n
                self.logger.debug(f"Candidato: {flow.flow_id}, Period: {period_norm:.2f}, Payload: {payload_norm:.2f}, Wait: {wait_time_norm:.2f}, Urgency: {urgency:.2f}")
                
                # AÃ±adir a candidatos con sus caracterÃ­sticas
                candidatos.append((idx, [period_norm, payload_norm, hop_progress, wait_time_norm, urgency]))
        
        # 3. Si no hay candidatos, devolver una observaciÃ³n con ceros
        if not candidatos:
            if self.use_graph_observation:
                return self._get_empty_graph_observation()
            else:
                return np.zeros(self.observation_space.shape, dtype=np.float32)
        
        # 4. Ordenar candidatos de forma significativa para el agente
        # Considerar mÃºltiples factores: tiempo espera (FIFO), urgencia, tamaÃ±o payload
        candidatos = sorted(candidatos, key=lambda x: (
            x[1][3],  # Tiempo de espera (mayor primero - FIFO)
            x[1][4],  # Urgencia (mayor primero)
            -x[1][1]  # Payload (menor primero - mÃ¡s rÃ¡pido)
        ), reverse=True)
        
        # Asegurar que tenemos exactamente NUM_FLUJOS_OBSERVABLES candidatos
        if len(candidatos) > self.NUM_FLUJOS_OBSERVABLES:
            candidatos = candidatos[:self.NUM_FLUJOS_OBSERVABLES]
        
        while len(candidatos) < self.NUM_FLUJOS_OBSERVABLES:
            candidatos.append((-1, [0.0, 0.0, 0.0, 0.0, 0.0]))
        
        # 5. Actualizar los Ã­ndices de flujos candidatos para recuperarlos despuÃ©s
        self.current_candidate_flows = [idx for idx, _ in candidatos if idx >= 0]
        
        # Registro para depuraciÃ³n
        if self.current_candidate_flows:
            self.logger.debug(f"Candidatos ordenados: {[self.flows[idx].flow_id for idx in self.current_candidate_flows]}")
        
        # 6. Construir la observaciÃ³n completa concatenando caracterÃ­sticas
        obs = np.array(global_features + [feat for _, feats in candidatos for feat in feats], dtype=np.float32)
        
        return obs
    
    def _get_graph_observation(self):
        """Get Dict observation for graph-based extractors (HATSExtractor)"""
        try:
            return self._neighbors_features()
        except Exception as e:
            self.logger.warning(f"Error getting graph observation: {e}. Falling back to empty observation.")
            return self._get_empty_graph_observation()
    
    def _get_empty_graph_observation(self):
        """Return empty Dict observation when no candidates available"""
        num_nodes = len(self.graph.nodes())
        num_edges = len(self.graph.edges())
        max_hops = max(len(flow.path) for flow in self.flows) if self.flows else 10
        
        return {
            'features_matrix': np.zeros((num_nodes, 8), dtype=np.float32),
            'edge_features': np.zeros((num_edges, 4), dtype=np.float32),
            'adjacency_matrix': np.array([[1 if self.graph.has_edge(i, j) else 0 
                                         for j in range(num_nodes)] 
                                        for i in range(num_nodes)], dtype=np.int32),
            'flow_feature': np.zeros((6,), dtype=np.float32),
            'link_feature': np.zeros((4,), dtype=np.float32),
            'remain_hops': np.zeros((max_hops,), dtype=np.int32)
        }
    
    def _neighbors_features(self):
        """
        Build graph-based observation for HATSExtractor.
        Returns Dict with graph structure and features.
        """
        if not self.current_candidate_flows or not self.flows:
            return self._get_empty_graph_observation()
        
        # Get current flow being scheduled
        flow_idx = self.current_candidate_flows[0] if self.current_candidate_flows else 0
        if flow_idx >= len(self.flows):
            return self._get_empty_graph_observation()
            
        flow = self.flows[flow_idx]
        hop_idx = self.flow_progress[flow_idx]
        
        if hop_idx >= len(flow.path):
            return self._get_empty_graph_observation()
        
        # Graph structure
        num_nodes = len(self.graph.nodes())
        num_edges = len(self.graph.edges())
        
        # Node features matrix (num_nodes x 8)
        features_matrix = np.zeros((num_nodes, 8), dtype=np.float32)
        node_list = list(self.graph.nodes())
        
        for i, node in enumerate(node_list):
            # Basic node features
            features_matrix[i, 0] = len(list(self.graph.neighbors(node))) / num_nodes  # Degree normalized
            features_matrix[i, 1] = 1.0 if node in flow.path else 0.0  # In current flow path
            features_matrix[i, 2] = 1.0 if i == hop_idx and node in flow.path else 0.0  # Current hop
            
            # Traffic load features
            node_load = sum(1 for f_idx, f in enumerate(self.flows) 
                           if not self.flow_completed[f_idx] and node in f.path)
            features_matrix[i, 3] = node_load / len(self.flows)  # Normalized load
            
            # Timing features
            features_matrix[i, 4] = self.global_time / 10000.0  # Normalized time
            features_matrix[i, 5] = self.current_complexity
            
            # Buffer/congestion indicators (simplified)
            features_matrix[i, 6] = 0.5  # Placeholder for buffer utilization
            features_matrix[i, 7] = 0.0  # Placeholder for congestion
        
        # Edge features matrix (num_edges x 4)  
        edge_features = np.zeros((num_edges, 4), dtype=np.float32)
        edge_list = list(self.graph.edges())
        
        for i, (src, dst) in enumerate(edge_list):
            # Edge load and utilization
            edge_load = sum(1 for f_idx, f in enumerate(self.flows)
                           if not self.flow_completed[f_idx] and 
                           any(f.path[j] == src and f.path[j+1] == dst 
                               for j in range(len(f.path)-1)))
            edge_features[i, 0] = edge_load / len(self.flows)  # Load
            edge_features[i, 1] = 1.0 if (src, dst) in [(flow.path[j], flow.path[j+1]) 
                                                        for j in range(len(flow.path)-1)] else 0.0  # In current path
            edge_features[i, 2] = 0.5  # Bandwidth utilization (placeholder)
            edge_features[i, 3] = 0.0  # Latency (placeholder)
        
        # Adjacency matrix
        adjacency_matrix = np.array([[1 if self.graph.has_edge(i, j) else 0 
                                     for j in range(num_nodes)] 
                                    for i in range(num_nodes)], dtype=np.int32)
        
        # Flow features (6 features)
        flow_feature = np.array([
            flow.period / 10000.0,  # Normalized period
            flow.payload / Net.MTU,  # Normalized payload
            hop_idx / len(flow.path),  # Progress
            (self.global_time % flow.period) / flow.period,  # Phase in period
            len(flow.path) / num_nodes,  # Path length normalized
            sum(self.flow_completed) / len(self.flows)  # Completion rate
        ], dtype=np.float32)
        
        # Link features (4 features) - current link being scheduled
        if hop_idx < len(flow.path) - 1:
            current_link = (flow.path[hop_idx], flow.path[hop_idx + 1])
            link_feature = np.array([
                1.0,  # Link is active
                edge_features[edge_list.index(current_link), 0] if current_link in edge_list else 0.0,  # Load
                0.5,  # Bandwidth (placeholder)
                self.global_time / 10000.0  # Current time
            ], dtype=np.float32)
        else:
            link_feature = np.zeros((4,), dtype=np.float32)
        
        # Remaining hops
        max_hops = max(len(f.path) for f in self.flows) if self.flows else 10
        remain_hops = np.zeros((max_hops,), dtype=np.int32)
        remaining = len(flow.path) - hop_idx
        if remaining > 0:
            remain_hops[:min(remaining, max_hops)] = 1
        
        return {
            'features_matrix': features_matrix,
            'edge_features': edge_features, 
            'adjacency_matrix': adjacency_matrix,
            'flow_feature': flow_feature,
            'link_feature': link_feature,
            'remain_hops': remain_hops
        }

    # ------------------------------------------------------------------ #
    #  MÃ¡scaras de acciÃ³n                                                #
    # ------------------------------------------------------------------ #
    def action_masks(self):
        """
        Genera mÃ¡scaras para el espacio de acciÃ³n MultiDiscreto.
        """
        # Calcular tamaÃ±o total de la mÃ¡scara sumando todas las dimensiones
        mask_size = sum(self.action_space.nvec)
        
        # Crear una mÃ¡scara plana donde todas las acciones estÃ¡n permitidas (0 = permitida)
        mask = np.zeros(mask_size, dtype=np.int8)
        
        try:
            if self.current_flow_idx < len(self.flows) and not self.flow_completed[self.current_flow_idx]:
                flow = self.current_flow()
                hop_idx = self.flow_progress[self.current_flow_idx]
                
                if hop_idx < len(flow.path):
                    link = self.link_dict[flow.path[hop_idx]]
                    
                    # Si es el primer hop, los factores de guard time altos podrÃ­an desperdiciarse
                    if hop_idx == 0:
                        # Ãndice para guard time alto
                        offset = 0  # No offset needed anymore
                        mask[offset + 3] = 1  # Desactivar valores 3 y 4 (factores altos)
                        mask[offset + 4] = 1
        except Exception as e:
            self.logger.warning(f"Error generando mÃ¡scaras de acciÃ³n: {e}")
            
        return mask

    # --------------------------------------------------------------------- #
    #  Comprobaciones de aislamiento y GCL                                  #
    # --------------------------------------------------------------------- #
    def _is_es_source(self, link_id):
        """
        Devuelve True si el **origen** de link_id es una End-Station (ES).
        1) Primero consulta el atributo ``node_type`` del grafo.
        2) Si no existe, usa la convenciÃ³n de nombres:
           E*, C*  = clientes/ES genÃ©ricas  
           SRV*    = servidores (tambiÃ©n ES)  
           S*      = switches
        """
        if link_id in self._es_node_cache:
            return self._es_node_cache[link_id]

        src = link_id[0] if isinstance(link_id, tuple) else link_id.split('-')[0]

        # a) Metadata del grafo (mÃ¡s robusta)
        if self.graph.nodes.get(src, {}).get("node_type") == "ES":
            self._es_node_cache[link_id] = True
            return True

        # b) ConvenciÃ³n de nombres
        is_es = src.startswith(("E", "C", "SRV"))
        self._es_node_cache[link_id] = is_es
        return is_es

    #  â›”  Retirado.  La agrupaciÃ³n de GCL dejÃ³ de ser necesaria.

    def _check_valid_link(self, link, operation):
        return check_valid_link(link, operation, self.current_flow(), self.links_operations)

    def _check_temp_operations(self):
        return check_temp_operations(self.temp_operations, self.links_operations, self.current_flow())

    def _find_next_event_time(self, current_time):
        """Encuentra el siguiente tiempo de evento programado despuÃ©s de current_time"""
        return find_next_event_time(self.link_busy_until, self.switch_busy_until, current_time)

    # MÃ©todo obsoleto: generaciÃ³n dinÃ¡mica de GCL eliminada.
    # Se mantiene para compatibilidad pero no hace nada y devuelve False.
    def add_gating_with_grouping(self, *args, **kwargs):
        return False

    def increase_complexity(self):
        """Incrementa el nivel de dificultad del entorno"""
        if not self.curriculum_enabled or self.current_complexity >= 1.0:
            return False
            
        # Incrementar complejidad gradualmente
        self.current_complexity = min(1.0, self.current_complexity + self.curriculum_step)
        return True

    # --------------------------------------------------------------------- #
    #  MÃ©todo principal step() - delegado al mÃ³dulo environment_impl         #
    # --------------------------------------------------------------------- #
    from core.learning.environment_impl import step  # (sigue igual, pero sin GCL)

    # --------------------------------------------------------------------- #
    #  gym stubs                                                             #
    # --------------------------------------------------------------------- #
    def render(self):
        pass

    def close(self):
        pass




## ARCHIVO: core/learning/environment_impl.py
## ==================================================

import math
import numpy as np
from core.learning.env_utils import ErrorType, SchedulingError, find_next_event_time
from core.network.net import Net
from core.network.operation import Operation
# import directo (la funciÃ³n sigue existiendo)
from core.learning.env_actions import process_step_action
import math, statistics as _stat
# Add missing import for logging
import logging


def step(self, action):
    """
    Realiza un paso en el entorno segÃºn la acciÃ³n proporcionada.
    
    Este mÃ©todo implementa la lÃ³gica principal para:
    - Procesar la acciÃ³n del agente
    - Calcular tiempos de transmisiÃ³n
    - Manejar conflictos
    - Actualizar el estado del entorno
    - Calcular la recompensa
    
    Args:
        action: AcciÃ³n multidimensional del agente RL
        
    Returns:
        Tuple: (observaciÃ³n, recompensa, terminado, truncado, info)
    """
    try:
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        #  Inicializar mÃ©tricas de latencia a 0 â€“ se actualizarÃ¡n al final
        #  del episodio si todos los flujos se completan con Ã©xito.
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        avg_lat = jitter = max_lat = 0

        # NUEVO: Extraer la selecciÃ³n de flujo de la acciÃ³n y aplicarla ANTES de procesar
        flow_selection = int(action[-1])  # La Ãºltima dimensiÃ³n es la selecciÃ³n de flujo
        
        # IMPORTANTE: Aplicar la selecciÃ³n de flujo inmediatamente
        flow_reward_adj = 0.0
        agent_selected = False                     # Controla si se aplicÃ³ la selecciÃ³n RL
        original_flow_idx = self.current_flow_idx  # Guardar para comprobar si cambiÃ³
        
        if hasattr(self, 'current_candidate_flows') and self.current_candidate_flows:
            if 0 <= flow_selection < len(self.current_candidate_flows):
                selected_idx = self.current_candidate_flows[flow_selection]
                if not self.flow_completed[selected_idx]:
                    # Verificar que el flujo seleccionado tiene un hop vÃ¡lido para programar
                    if self.flow_progress[selected_idx] < len(self.flows[selected_idx].path):
                        self.current_flow_idx = selected_idx
                        agent_selected = True
                        
                        # AÃ±adir informaciÃ³n de debug para seguimiento
                        # â†“  Pasa a DEBUG para no saturar la consola
                        self.logger.debug(
                            "Agente seleccionÃ³ flujo %s (idx %d) de candidatos: %s",
                            self.flows[selected_idx].flow_id,
                            selected_idx,
                            [self.flows[idx].flow_id for idx in self.current_candidate_flows],
                        )
                        
                        # Evaluar si la selecciÃ³n fue buena basÃ¡ndose en caracterÃ­sticas
                        selected_flow = self.flows[selected_idx]
                        
                        # Calcular recompensa por selecciÃ³n inteligente
                        period_factor   = 1.0 - (selected_flow.period / 10000)
                        payload_factor  = 1.0 - (selected_flow.payload / Net.MTU)
                        remaining       = len(selected_flow.path) - self.flow_progress[selected_idx]
                        progress_factor = remaining / len(selected_flow.path)

                        flow_reward_adj = (period_factor * 0.15 +
                                           payload_factor * 0.15 +
                                           progress_factor * 0.15)
                        
                        self.logger.debug(f"Flow selection: {flow_selection} â†’ candidate #{selected_idx} (Flow {selected_flow.flow_id})")
                        self.logger.debug(f"Flow reward adjustment: +{flow_reward_adj:.4f}")
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        #  ENFORCE PRIORITY SCHEDULING - IMMEDIATE FORWARDING FROM SWITCHES
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        fifo_idx = self._next_fifo_idx()
        if fifo_idx is None:
            # No quedan flujos pendientes â€“ deberÃ­a terminar normalmente
            return self._get_observation(), 0, True, False, {"success": True}

        # Respetar la selecciÃ³n del agente salvo que no haya seleccionado
        if not agent_selected and self.current_flow_idx != fifo_idx:
            # Priorizar la transmisiÃ³n desde switches
            prev_flow = self.flows[self.current_flow_idx]
            new_flow = self.flows[fifo_idx]
            prev_hop_idx = self.flow_progress[self.current_flow_idx]
            new_hop_idx = self.flow_progress[fifo_idx]
            
            # Verificar si es un cambio a un flujo que estÃ¡ en un switch esperando
            if new_hop_idx > 0:
                prev_link_id = new_flow.path[new_hop_idx - 1]
                dst_node = prev_link_id[1] if isinstance(prev_link_id, tuple) else prev_link_id.split('-')[1]
                is_at_switch = dst_node.startswith('S') and not dst_node.startswith('SRV')
                
                if is_at_switch:
                    self.logger.debug(f"Priorizando transmisiÃ³n inmediata desde switch: flujo {new_flow.flow_id}")
            
            # Actualizar el flujo actual
            self.current_flow_idx = fifo_idx

        # A partir de aquÃ­ TODO el cÃ³digo sigue igual: ya trabajamos con el
        # flujo correcto; si posteriormente no cabe en el perÃ­odo, se lanzarÃ¡
        # el SchedulingError habitual y el episodio terminarÃ¡ "con error".
        
        # Procesar la acciÃ³n y obtener la informaciÃ³n necesaria
        # â‹  Desestructuramos el nuevo elemento `guard_factor`
        (flow, hop_idx, link, gating, trans_time,
         guard_time, guard_factor,                # â† aquÃ­
         offset_us, switch_gap, sw_src,
         is_egress_from_switch, gcl_strategy) = process_step_action(self, action)

        # ------------------------------------------------------------ #
        # 1. Calcular tiempos                                          #
        # ------------------------------------------------------------ #
        if hop_idx == 0:        # ---------- primer hop ----------
            # Registrar *exactamente* el instante en que se libera el primer bit
            # del paquete en el cliente â†’ op.start_time (no global_time).
            if self.flow_first_tx[self.current_flow_idx] is None:
                # El objeto `op` se crea unas lÃ­neas mÃ¡s abajo; de momento
                # guardamos el valor provisional y lo sobrescribiremos enseguida.
                self.flow_first_tx[self.current_flow_idx] = -1

            # Si no se entrega una red, construir topologÃ­a y flujos sencillos
            if self.flow_first_tx[self.current_flow_idx] is None:
                self.flow_first_tx[self.current_flow_idx] = self.global_time
            # â¶  Primer hop: basta con que el enlace estÃ© libre
            # â¡ï¸  SÃ³lo el *primer* enlace respeta Net.PACKET_GAP_EXTRA
            earliest = max(
                self.link_busy_until[link],
                self.global_queue_busy_until,
                self.last_packet_start + self._next_packet_gap()
            )  # Removed offset_us
            # Obtener el switch de destino para este paquete
            dst_node = link.link_id[1] if isinstance(link.link_id, tuple) else link.link_id.split('-')[1]
            if dst_node.startswith('S'):
                # Asegurar separaciÃ³n mÃ­nima de 1Î¼s entre llegadas al switch
                # Calcula el tiempo de llegada *potencial* al switch
                potential_arrival_time = earliest + trans_time + Net.DELAY_PROP
                min_arrival_time = self.switch_last_arrival[dst_node] + switch_gap
                if potential_arrival_time < min_arrival_time:
                    # Ajustar el tiempo de inicio para garantizar 1Î¼s de separaciÃ³n en destino
                    delay = min_arrival_time - potential_arrival_time
                    earliest += delay
                    # Actualizar el tiempo de llegada real
                    arrival_time = min_arrival_time
                else:
                    arrival_time = potential_arrival_time
                # Actualizar el Ãºltimo tiempo de llegada registrado para este switch
                self.switch_last_arrival[dst_node] = arrival_time

            # Re-calcular la ventana tras cualquier retraso aplicado
            latest = earliest + Net.SYNC_ERROR

            # Para el primer hop desde ES, no hay gating. Dequeue es el inicio mÃ¡s temprano.
            offset   = 0            # sin margen de sincronÃ­a
            dequeue  = earliest     # comienza tan pronto el enlace queda libre
            end_time = dequeue + trans_time
            if end_time > flow.period:
                 # Primer hop (sale de una ES): no hay switch para crear espera
                 raise SchedulingError(ErrorType.PeriodExceed, "ExcediÃ³ perÃ­odo")

            # --- Crear objeto Operation para hop_idx == 0 ---
            op_start_time = earliest
            op_gating_time = None # No hay gating desde ES
            op_latest_time = latest # Usamos latest calculado
            op_end_time = end_time
            # Crear la operaciÃ³n AHORA para que 'op' estÃ© definida
            op = Operation(op_start_time, op_gating_time, op_latest_time, op_end_time)

            # â• GUARDAR datos que el visualizador necesita
            op.guard_factor      = guard_factor      # decisiÃ³n RL
            op.min_gap_value     = switch_gap        # decisiÃ³n RL
            op.guard_time        = guard_time        # longitud real del guard-band

            # â±ï¸  ahora sÃ­: fijamos el instante real de partida
            if self.flow_first_tx[self.current_flow_idx] == -1:
                self.flow_first_tx[self.current_flow_idx] = op_start_time

        else:                   # ---------- hops siguientes ----------
            # â·  Resto de hops:
            prev_link_id = flow.path[hop_idx - 1]
            prev_link = self.link_dict[prev_link_id]
            prev_op = self.links_operations[prev_link][-1][1]

            # Tiempo base: cuando el paquete estÃ¡ listo en el nodo actual
            packet_ready_time = prev_op.reception_time

            # Earliest possible start considerando sÃ³lo llegada y disponibilidad del ENLACE
            # âš ï¸  En hops posteriores **no** aplicamos la separaciÃ³n global:
            earliest_possible_start_on_link = max(packet_ready_time,
                                         self.link_busy_until[link],
                                         self.global_queue_busy_until)  # Removed offset_us

            # Earliest start considerando tambiÃ©n la disponibilidad del PUERTO del SWITCH (si aplica)
            if is_egress_from_switch:
                # FCFS se mantiene, pero no registramos la espera (no la decide el agente)
                final_earliest_start = max(earliest_possible_start_on_link,
                                            self.switch_busy_until[sw_src])
            else:
                final_earliest_start = earliest_possible_start_on_link

            # Calcular la ventana 'latest' basada en el inicio mÃ¡s temprano real
            latest = final_earliest_start + Net.SYNC_ERROR

            # Determinar el tiempo real de DEQUEUE (inicio de transmisiÃ³n)
            offset   = 0            # sin margen de sincronÃ­a
            dequeue  = final_earliest_start
            
            # Calcular tiempo de fin de transmisiÃ³n
            end_time = dequeue + trans_time
            if end_time > flow.period:
                # Si no cabe en su periodo, abortar sin intentos de recolocaciÃ³n
                raise SchedulingError(ErrorType.PeriodExceed, "ExcediÃ³ perÃ­odo")

            # --- Crear objeto Operation ---
            # start_time: CuÃ¡ndo podrÃ­a haber empezado (llegada + disponibilidad enlace)
            # gating_time: CuÃ¡ndo empezÃ³ realmente (dequeue), si aplica gating
            # latest_time: LÃ­mite superior de la ventana para gating
            # end_time: CuÃ¡ndo terminÃ³ la transmisiÃ³n
            op_start_time = earliest_possible_start_on_link
            op_gating_time = dequeue if gating and is_egress_from_switch else None 
            # Importante: Si hay gating, latest_time debe ser igual a gating_time
            if gating and is_egress_from_switch:
                op_latest_time = op_gating_time  # Si hay gating, ambos deben ser iguales
            else:
                op_latest_time = latest  # Sin gating, latest_time mantiene su valor normal

            op_end_time = end_time

            op = Operation(op_start_time, op_gating_time, op_latest_time, op_end_time)

            # â• Actualizar tambiÃ©n en la rama "hops > 0"
            op.guard_factor  = guard_factor
            op.min_gap_value = switch_gap
            op.guard_time    = guard_time
            # No guardamos esperas que no sean decisiÃ³n del agente

            # â”€â”€ Nuevo: garantizar separaciÃ³n mÃ­nima entre llegadas al switch destino â”€â”€
            dst_node = link.link_id[1] if isinstance(link.link_id, tuple) \
                       else link.link_id.split('-')[1]
            if dst_node.startswith('S'):                       # sÃ³lo switches reales
                arrival = op.reception_time - Net.DELAY_PROC_RX
                min_arrival = self.switch_last_arrival[dst_node] + switch_gap
                if arrival < min_arrival:
                    delay = min_arrival - arrival
                    op.add(delay)              # ajusta *todos* los tiempos de la operaciÃ³n
                    op.min_gap_wait += delay   # registrar espera por gap mÃ­nimo
                    dequeue   += delay
                    end_time  += delay
                    op_start_time += delay
                    op_end_time   += delay
                    arrival = min_arrival
                # Registrar llegada para el siguiente paquete
                self.switch_last_arrival[dst_node] = arrival

        # --- Regla *unâ€‘soloâ€‘paqueteâ€‘switch* ---
        # 2. Crear operaciÃ³n temporal                                  #
        # ------------------------------------------------------------ #
        # op ya estÃ¡ creado con los tiempos correctos
        self.temp_operations.append((link, op))

        # Resolver conflictos por desplazamiento
        offset = self._check_temp_operations()
        max_iter = 16  # salvaguarda contra bucles infinitos
        while offset is not None and max_iter:
            # Desplazar la operaciÃ³n segÃºn el offset de conflicto
            op_start_time += offset
            
            # Actualizar TODAS las propiedades temporales
            if op_gating_time is not None:
                # Con gating, todo se desplaza por igual
                op_gating_time += offset
                op_latest_time += offset  # latest siempre alineado con gating
            else:
                # Sin gating, latest avanza con start (son independientes)
                op_latest_time += offset
            
            # Tiempo final siempre se recalcula respecto al inicio real
            op_end_time = (op_gating_time if op_gating_time is not None else op_start_time) + trans_time

            # Recrear la operaciÃ³n con los nuevos tiempos
            op = Operation(op_start_time, op_gating_time, op_latest_time, op_end_time)
            
            # Si hay gating, validar que aÃºn estÃ¡ dentro del perÃ­odo del flujo
            if op_gating_time is not None and op_end_time > flow.period:
                # El inicio real ocurrirÃ­a despuÃ©s del final del perÃ­odo
                raise SchedulingError(
                    ErrorType.PeriodExceed, 
                    "Flow cycles into next period"
                )
            
            # Recrear el array de operaciones temporales con la actualizada
            self.temp_operations = [(link, op)]
            
            # Volver a verificar conflictos
            offset = self._check_temp_operations()
            max_iter -= 1
            
            # Use default fixed conflict resolution strategy
            # (previous conflict_strategy action dimension was removed)
            # Apply a small minimum offset to ensure progress
            if offset is not None and offset == 0:
                offset = max(1, int(switch_gap * Net.SWITCH_GAP_MIN))

        if max_iter == 0 and offset is not None:
            raise SchedulingError(
                ErrorType.PeriodExceed,
                "Failed to resolve conflict after 16 iterations"
            )

        #  â›”  Ya no se generan ni reservan reglas GCL durante el scheduling.

        # ---------- REWARD SHAPING ----------
        GB_PEN      = 0.05   # guard-band (sÃ­ la decide RL)

        reward = 1.0
        reward -= GB_PEN * (guard_time / flow.period)

        # NUEVO: AÃ±adir el ajuste de recompensa por selecciÃ³n inteligente de flujo
        reward += flow_reward_adj

    except SchedulingError as e:
        # Un flujo no cabe en su perÃ­odo â”€ cerramos el episodio,
        #  pero entregando TODO lo que sÃ­ se ha programado hasta ahora.
        self.logger.debug(f"Fallo: {e.msg} (flujo {self.current_flow_idx})")

        partial_res = {lnk: ops.copy()          # copia superficial es suficiente
                       for lnk, ops in self.links_operations.items()}

        return self._get_observation(), -1, True, False, {
            "success": False,                  # episodio fallido
            "ScheduleRes": partial_res,        # â† planificaciÃ³n parcial
        }

    # ------------------------------------------------------------ #
    # 3. Avanzar progreso del flujo                                #
    # ------------------------------------------------------------ #
    self.links_operations[link].append((flow, op))
    self.temp_operations.clear()

    # ğŸŒ Registrar sÃ³lo si es el *primer* hop del flujo
    if hop_idx == 0:
        self.last_packet_start = op_start_time

    # â·  Marcar el enlace como ocupado hasta que el paquete estÃ© completamente recibido Y PROCESADO
    # Usar reception_time que ya incluye DELAY_PROP + DELAY_PROC_RX
    self.link_busy_until[link] = op.reception_time  # En lugar de op.end_time + Net.DELAY_PROP
    # ğŸ”’ Mantener la secciÃ³n crÃ­tica ocupada hasta que el frame se recibe
    self.global_queue_busy_until = op.reception_time

    if is_egress_from_switch:                 # â· liberar switch al terminar
        # Mantener el puerto bloqueado tambiÃ©n durante la guard-band escogida
        # para reflejar exactamente la reserva temporal del modelo matemÃ¡tico
        self.switch_busy_until[sw_src] = op.end_time + guard_time

    self.flow_progress[self.current_flow_idx] += 1


    # â¸  El "reloj" global se redefine como el evento mÃ¡s temprano pendiente
    next_events = [*self.link_busy_until.values(),
                  *self.switch_busy_until.values()]
    # Si no quedan eventos pendientes, mantenemos el reloj en lugar de "rebobinar" a 0
    self.global_time = min(next_events, default=self.global_time)

    # Â¿TerminÃ³ este flujo?
    if self.flow_progress[self.current_flow_idx] == len(flow.path):
        self.flow_completed[self.current_flow_idx] = True
        # ---------- verificaciÃ³n latencia extremo-a-extremo ----------
        fst = self.flow_first_tx[self.current_flow_idx]
        e2e_latency = op.reception_time - fst if fst is not None else 0
        
        # NUEVO: Guardar latencia e2e para estadÃ­sticas globales
        self._flow_latencies.append(e2e_latency)

        # â• Registrar la muestra en el acumulador global
        self._latency_samples.append(e2e_latency)
        
        # *Incluir* la propagaciÃ³n y el procesamiento de RX en el presupuesto
        # ğŸ’¡ Tomar el peorâ€‘caso acumulativo sobre la ruta completa
        hops = len(flow.path)
        e2e_budget = (flow.e2e_delay +                      # presupuesto nominal
                      Net.DELAY_PROP   * hops +            # propagaciÃ³n
                      Net.DELAY_PROC_RX * hops +           # procesado RX
                      guard_time        * (hops - 1))      # guardâ€‘band por hop
        if e2e_latency > e2e_budget:
            raise SchedulingError(ErrorType.PeriodExceed,
                                  f"E2E delay {e2e_latency} > {e2e_budget}")
        reward += 2

    # Â¿TerminÃ³ episodio?
    done = all(self.flow_completed)
    
    # DespuÃ©s de procesar un hop de un flujo, si el destino es un switch,
    # inmediatamente preparar el siguiente hop para transmisiÃ³n
    if not done and hop_idx < len(flow.path) - 1:
        dst_node = link.link_id[1] if isinstance(link.link_id, tuple) else link.link_id.split('-')[1]
        if dst_node.startswith('S') and not dst_node.startswith('SRV'):
            # Este paquete llegÃ³ a un switch, marcar como alta prioridad
            # para ser procesado en el siguiente paso
            arrival_time = op.reception_time
            self.switch_last_arrival[dst_node] = min(arrival_time, self.switch_last_arrival[dst_node])
            
            # Actualizar el reloj global para favorecer el procesamiento inmediato
            # de este paquete que acaba de llegar al switch
            if arrival_time < self.global_time:
                next_events = [*self.link_busy_until.values(), *self.switch_busy_until.values(), arrival_time]
                self.global_time = min(next_events)

    # Gestionar el curriculum learning
    if done and all(self.flow_completed):
        # Episodio exitoso: incrementar contador de Ã©xitos consecutivos
        self.consecutive_successes += 1
        # AÃ±adir bonificaciÃ³n de recompensa proporcional al nivel de complejidad
        reward += 5.0 * self.current_complexity

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        #  ã€½ï¸  Calculamos las estadÃ­sticas de latencia del episodio
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if self._latency_samples:
            avg_lat = sum(self._latency_samples) / len(self._latency_samples)
            max_lat = max(self._latency_samples)
            jitter  = _stat.pstdev(self._latency_samples) if len(self._latency_samples) > 1 else 0
            self.logger.info(
                f"â±ï¸  Latencia promedio={avg_lat:.1f} Âµs Â· "
                f"jitter={jitter:.1f} Âµs Â· "
                f"mÃ¡xima={max_lat} Âµs"
            )
        else:
            avg_lat = max_lat = jitter = 0
        
        # Mostrar informaciÃ³n del progreso del curriculum
        if self.curriculum_enabled:
            self.logger.info(f"Ã‰xito con {len(self.flows)}/{self.total_flows} flujos (complejidad: {self.current_complexity:.2f}, Ã©xitos: {self.consecutive_successes}/3)")
    
    info = {
        "success": done,
        "ScheduleRes": self.links_operations.copy() if done else None,
        "curriculum_level": self.current_complexity,
        "num_flows": len(self.flows),

        # â”€â”€â”€ mÃ©tricas de latencia E2E â”€â”€â”€
        "latency_us": {
            "average": avg_lat,
            "jitter" : jitter,
            "maximum": max_lat,
            "samples": self._latency_samples.copy(),
        },
        # NUEVO: AÃ±adir informaciÃ³n sobre selecciÃ³n de flujos
        "flow_selection": {
            "current_flow_idx": self.current_flow_idx,
            "available_candidates": getattr(self, 'current_candidate_flows', []),
            "selected_option": flow_selection,
            "reward_adj": flow_reward_adj
        }
    }

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    #  Al terminar el episodio (todos los flujos entregados) â†’ mÃ©tricas
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if done and self._flow_latencies:
        avg_lat = sum(self._flow_latencies) / len(self._flow_latencies)
        max_lat = max(self._flow_latencies)
        import statistics as _st
        jitter = _st.pstdev(self._flow_latencies) if len(self._flow_latencies) > 1 else 0

        # Log amigable
        self.logger.info(
            f"â±ï¸  Latencia promedio={avg_lat:.0f} Âµs Â· "
            f"jitter={jitter:.0f} Âµs Â· mÃ¡xima={max_lat:.0f} Âµs"
        )

        # AÃ±adir al diccionario `info`
        info["latency_us"] = {
            "average": avg_lat,
            "jitter":  jitter,
            "max":     max_lat,
        }

    return self._get_observation(), reward, done, False, info



## ARCHIVO: core/learning/hats_extractor.py
## ==================================================

"""
HATSExtractor
=============

Encoder jerÃ¡rquico para grafos TSN:
  1) **GATv2ConvÃ—3** â†’ lectura global con atenciÃ³n
  2) **TransformerEncoder** para la secuencia de enlaces restantes
  3) **MLP** para contexto escalar
Devuelve un embedding unificado (dim 512).
"""

from typing import Dict
import gymnasium as gym
import torch
import torch.nn as nn
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor

try:
    from torch_geometric.data import Data, Batch
    from torch_geometric.nn import GATv2Conv, GlobalAttention
    TORCH_GEOMETRIC_AVAILABLE = True
except ImportError:
    TORCH_GEOMETRIC_AVAILABLE = False
    print("Warning: torch-geometric not available. HATSExtractor will use fallback implementation.")

# ---------- helpers ----------

def dict_to_batch_with_edges(obs: Dict) -> Batch:
    """Convierte el dict de SB3 en Batch de PyG (incluyendo edge_attr)."""
    if not TORCH_GEOMETRIC_AVAILABLE:
        raise ImportError("torch-geometric is required for HATSExtractor")
    
    data_list = []
    for adj, feat, eattr in zip(obs["adjacency_matrix"],
                                obs["features_matrix"],
                                obs["edge_features"]):
        data_list.append(
            Data(x=feat,
                 edge_index=adj.to(torch.int64),
                 edge_attr=eattr)
        )
    return Batch.from_data_list(data_list)

# ---------- extractor ----------

class HATSExtractor(BaseFeaturesExtractor):
    def __init__(self, observation_space: gym.spaces.Dict):
        # Calcular dimensiÃ³n de features basado en el observation_space
        sample_obs = {key: torch.zeros(space.shape) for key, space in observation_space.spaces.items()}
        super().__init__(observation_space, features_dim=1)  # Temporal, se actualizarÃ¡
        
        if not TORCH_GEOMETRIC_AVAILABLE:
            # Fallback a implementaciÃ³n simple sin PyG
            self._init_fallback(observation_space)
            return
            
        self._init_gat(observation_space)

    def _init_fallback(self, observation_space):
        """ImplementaciÃ³n de respaldo sin torch-geometric"""
        # Obtener dimensiones de caracterÃ­sticas
        if "features_matrix" in observation_space.spaces:
            n_node_feat = observation_space.spaces["features_matrix"].shape[-1]
        else:
            n_node_feat = 64  # valor por defecto
            
        if "flow_feature" in observation_space.spaces and "link_feature" in observation_space.spaces:
            ctx_dim = (observation_space.spaces["flow_feature"].shape[0] + 
                      observation_space.spaces["link_feature"].shape[0])
        else:
            ctx_dim = 128  # valor por defecto
        
        # MLP simple como fallback
        self.fallback_net = nn.Sequential(
            nn.Linear(n_node_feat + ctx_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 256),
            nn.ReLU(),
            nn.Linear(256, 256)
        )
        self._features_dim = 256

    def _init_gat(self, observation_space):
        """InicializaciÃ³n con GAT cuando torch-geometric estÃ¡ disponible"""
        n_node_feat = observation_space.spaces["features_matrix"].shape[-1]
        n_edge_feat = observation_space.spaces["edge_features"].shape[-1] if "edge_features" in observation_space.spaces else 3
        ctx_dim = (observation_space.spaces["flow_feature"].shape[0] + 
                  observation_space.spaces["link_feature"].shape[0])
        seq_len = observation_space.spaces["remain_hops"].shape[0] if "remain_hops" in observation_space.spaces else 64

        # 1) grÃ¡ficas estÃ¡ticas --------------------------------------------
        self.gat_layers = nn.ModuleList([
            GATv2Conv(
                in_channels=n_node_feat if i == 0 else 64,
                out_channels=64,
                heads=4,
                concat=True,
                edge_dim=n_edge_feat,
                add_self_loops=False
            )
            for i in range(3)
        ])
        self.readout = GlobalAttention(nn.Linear(64*4, 1))

        # 2) camino temporal -----------------------------------------------
        tf_layer = nn.TransformerEncoderLayer(
            d_model=64, nhead=4, batch_first=True, dropout=0.1
        )
        self.path_enc = nn.TransformerEncoder(tf_layer, num_layers=4)
        self.path_proj = nn.Linear(64, 128)

        # 3) contexto escalar ----------------------------------------------
        self.ctx_mlp = nn.Sequential(
            nn.Linear(ctx_dim, 128), nn.ReLU(),
            nn.Linear(128, 64)
        )

        self._features_dim = 64*4 + 128 + 64

    @property
    def features_dim(self):
        return self._features_dim

    def forward(self, obs: Dict):
        if not TORCH_GEOMETRIC_AVAILABLE:
            return self._forward_fallback(obs)
        return self._forward_gat(obs)

    def _forward_fallback(self, obs: Dict):
        """Forward pass sin torch-geometric"""
        # Concatenar caracterÃ­sticas disponibles
        features = []
        
        if "features_matrix" in obs:
            # Aplanar la matriz de caracterÃ­sticas
            feat_flat = obs["features_matrix"].flatten(start_dim=1)
            features.append(feat_flat)
            
        if "flow_feature" in obs and "link_feature" in obs:
            ctx = torch.cat([obs["flow_feature"], obs["link_feature"]], dim=1)
            features.append(ctx)
        
        if features:
            x = torch.cat(features, dim=1)
        else:
            # Si no hay caracterÃ­sticas, crear tensor dummy
            x = torch.zeros(obs[list(obs.keys())[0]].shape[0], 64, device=obs[list(obs.keys())[0]].device)
            
        return self.fallback_net(x)

    def _forward_gat(self, obs: Dict):
        """Forward pass con GAT"""
        # ---- GAT ----
        batch = dict_to_batch_with_edges(obs)
        x = batch.x
        for gat in self.gat_layers:
            x = torch.relu(gat(x, batch.edge_index, batch.edge_attr))
        batch.x = x
        g_emb = self.readout(batch.x, batch.batch)                # (B, 256)

        # ---- Transformer (camino) ----
        B = obs["remain_hops"].shape[0]
        seq = obs["remain_hops"].view(B, -1, x.size(-1))          # (B,L,64)
        p_emb = self.path_enc(seq)[:, 0]                          # token 0
        p_emb = torch.relu(self.path_proj(p_emb))                 # (B,128)

        # ---- contexto ----
        ctx = torch.cat([obs["flow_feature"], obs["link_feature"]], dim=1)
        c_emb = self.ctx_mlp(ctx)                                 # (B,64)

        return torch.cat([g_emb, p_emb, c_emb], dim=1)



## ARCHIVO: core/learning/maskable_sac.py
## ==================================================

"""
Maskable SAC
============
Envoltura mÃ­nima sobre `stable_baselines3.SAC` que soporta enmascarado
de acciones (discretas) tal y como hace sb3_contrib para PPO.
"""
import torch
from stable_baselines3 import SAC
from stable_baselines3.common.utils import get_device


class MaskableSAC(SAC):
    def __init__(self, *args, action_mask_fn=None, **kwargs):
        super().__init__(*args, **kwargs)
        # funciÃ³n que devuelve un tensor bool con las acciones vÃ¡lidas
        self._mask_fn = action_mask_fn

    # ---------------------------------------------------------
    def _get_action_masks(self):
        if self._mask_fn is not None:
            return torch.tensor(self._mask_fn(), device=get_device(self.actor))
        # fallback: llamar al mÃ©todo estÃ¡ndar de VecEnv
        if hasattr(self.env, "env_method"):
            return torch.tensor(self.env.env_method("action_masks"),
                                device=get_device(self.actor))
        raise RuntimeError("No se proporcionÃ³ 'action_mask_fn' y el env no "
                           "expone 'action_masks'.")

    def predict(self, observation, state=None,
                episode_start=None, deterministic=False, action_masks=None):
        if action_masks is None:
            action_masks = self._get_action_masks()
        return super().predict(
            observation, state, episode_start,
            deterministic, action_masks=action_masks
        )



## ARCHIVO: core/network/__init__.py
## ==================================================

"""
MÃ³dulos de modelado de red y topologÃ­as
"""

from .net import Flow, Link, Net, Network, generate_graph, generate_flows, FlowGenerator
from .operation import Operation, check_operation_isolation







## ARCHIVO: core/network/net.py
## ==================================================

import logging
import math
import networkx as nx
import numpy as np
import random
import typing


# Conjunto de perÃ­odos disponibles
PERIOD_SET = [1000]

class Net:
    # ConfiguraciÃ³n de red y temporizaciones
    MTU = 1518
    # â¡ï¸ Nueva: separaciÃ³n mÃ­nima entre llegadas sucesivas al mismo switch (Âµs)
    SWITCH_GAP_MIN = 1

    # â¡ï¸ **NUEVO** â€“ parÃ¡metros para la separaciÃ³n probabilÃ­stica entre creaciones
    #     de paquetes (sÃ³lo se evalÃºa en el PRIMER hop de cada flujo)
    #
    #     â€¢ PACKET_GAP_MODE:
    #         â”€ fixed        â†’ separaciÃ³n constante = PACKET_GAP_EXTRA
    #         â”€ uniform      â†’ U[min,max]  definido en PACKET_GAP_UNIFORM
    #         â”€ exponential  â†’ Exp(Î»)  con media = PACKET_GAP_EXTRA
    #         â”€ gaussian     â†’ N(Î¼,ÏƒÂ²)  definido en PACKET_GAP_GAUSS
    #         â”€ pareto       â†’ Pareto(Î±,xm) definido en PACKET_GAP_PARETO
    #     â€¢ Los valores se configuran desde CLI en ui/train.py / ui/test.py.
    #
    PACKET_GAP_MODE    = "fixed"
    PACKET_GAP_EXTRA   = 0
    PACKET_GAP_UNIFORM = (0, 0)       # sÃ³lo usado si mode == uniform
    # Nuevos contenedores para gaussian y pareto
    PACKET_GAP_GAUSS   = (0.0, 1.0)   # Î¼, Ïƒ
    PACKET_GAP_PARETO  = (2.0, 1.0)   # Î±, x_m

    # ------------------------------------------------------------------
    #  ConfiguraciÃ³n centralizada desde CLI
    # ------------------------------------------------------------------
    @staticmethod
    def set_gap_distribution(dist: str, params: list[float] | None = None):
        """
        Ajusta la distribuciÃ³n global del gap entre creaciones de paquetes.

        â”€ dist = fixed        â†’ params = [gap]
        â”€ dist = uniform      â†’ params = [min, max]
        â”€ dist = exponential  â†’ params = [mean]
        â”€ dist = gaussian     â†’ params = [mean, sigma]
        â”€ dist = pareto       â†’ params = [alpha, x_m]
        """
        params = params or []
        Net.PACKET_GAP_MODE = dist

        if dist == "fixed":
            Net.PACKET_GAP_EXTRA = int(params[0]) if params else 0

        elif dist == "uniform":
            assert len(params) == 2, "--dist-params necesita MIN MAX"
            Net.PACKET_GAP_UNIFORM = (int(params[0]), int(params[1]))

        elif dist == "exponential":
            Net.PACKET_GAP_EXTRA = int(params[0]) if params else 1

        elif dist == "gaussian":
            assert len(params) >= 2, "--dist-params necesita MU SIGMA"
            Net.PACKET_GAP_GAUSS = (float(params[0]), float(params[1]))

        elif dist == "pareto":
            assert len(params) == 2, "--dist-params necesita ALPHA XM"
            Net.PACKET_GAP_PARETO = (float(params[0]), float(params[1]))

        else:
            raise ValueError(f"DistribuciÃ³n desconocida: {dist}")

    @staticmethod
    def sample_packet_gap() -> int:
        """Devuelve la separaciÃ³n (Âµs) segÃºn la configuraciÃ³n global."""
        if Net.PACKET_GAP_MODE == "fixed":
            return Net.PACKET_GAP_EXTRA
        elif Net.PACKET_GAP_MODE == "uniform":
            lo, hi = Net.PACKET_GAP_UNIFORM
            return random.randint(lo, hi) if hi > lo else lo
        elif Net.PACKET_GAP_MODE == "exponential":
            Î¼ = max(Net.PACKET_GAP_EXTRA, 1)      # evita Î»=0
            return int(random.expovariate(1/Î¼))
        elif Net.PACKET_GAP_MODE == "gaussian":
            Î¼, Ïƒ = Net.PACKET_GAP_GAUSS
            # recortamos a â‰¥0 y redondeamos
            return max(0, int(random.gauss(Î¼, Ïƒ)))
        elif Net.PACKET_GAP_MODE == "pareto":
            Î±, xm = Net.PACKET_GAP_PARETO
            # random.paretovariate devuelve xm*(1-U)^(-1/Î±) con xm=1
            return int(random.paretovariate(Î±)*xm)
        else:
            raise ValueError(f"Modo de gap desconocido: {Net.PACKET_GAP_MODE}")
  
    DELAY_PROC_RX = 1  # Nuevo: tiempo de procesamiento de recepciÃ³n
    SYNC_ERROR = 0  # Sin incertidumbre: relojes perfectamente alineados
    DELAY_PROP = 1
    GCL_CYCLE_MAX = 128000
    # Longitud mÃ¡xima tÃ­pica de un Gate Control List en hardware TSN (open/close)
    GCL_LENGTH_MAX = 256


class Link:
    embedding_length = 3

    def __init__(self, link_id, link_rate):
        self.link_id = link_id
        self.gcl_capacity = Net.GCL_LENGTH_MAX
        self.link_rate = link_rate

    def __hash__(self):
        return hash(self.link_id)

    def __eq__(self, other):
        return isinstance(other, Link) and self.link_id == other.link_id

    def __repr__(self):
        return f"Link{self.link_id}"

    def interference_time(self):
        return self.transmission_time(Net.MTU)

    def transmission_time(self, payload):
        # 12B para IFG, 8B para preÃ¡mbulo
        return math.ceil((payload + 12 + 8) * 8 / self.link_rate)

class Flow:
    def __init__(self, flow_id, src_id, dst_id, path, period=2000, payload=100, e2e_delay=None):
        self.flow_id = flow_id
        self.src_id = src_id
        self.dst_id = dst_id
        self.path = path
        # Validar perÃ­odo
        assert period in PERIOD_SET, f"PerÃ­odo invÃ¡lido {period}"
        self.period = period
        self.payload = payload
        self.e2e_delay = period if e2e_delay is None else e2e_delay
        # Jitter completamente eliminado

    def __hash__(self):
        return hash(self.flow_id)

    def __eq__(self, other):
        return isinstance(other, Flow) and self.flow_id == other.flow_id

    def __repr__(self):
        return f"Flow(id='{self.flow_id}', src='{self.src_id}', dst='{self.dst_id}', period={self.period})"

# Funciones para generar topologÃ­as simplificadas
def generate_graph(topo, link_rate=100):
    """
    Devuelve el grafo dirigido para la topologÃ­a *topo*.

    Â· SIMPLE              â€“ mÃ­nimo S-E-S bidireccional  
    Â· UNIDIR, UNIDIR2-8   â€“ variantes unidireccionales (solo ES â†’ SRV1)
    """
    # Bidireccional de referencia
    if topo == "SIMPLE":
        return generate_simple_topology(link_rate)

    # Familia unidireccional  (cualquier nombre que empiece por UNIDIRâ€¦)
    topo = topo.upper()
    if topo.startswith("UNIDIR"):
        mapping = {
            "UNIDIR":  generate_unidirectional_topology,
            "UNIDIR2": generate_unidirectional_chain_topology2,
            "UNIDIR3": generate_unidirectional_chain_topology3,
            "UNIDIR4": generate_unidirectional_topology4,
            "UNIDIR5": generate_unidirectional_topology5,
            "UNIDIR6": generate_unidirectional_topology6,
            "UNIDIR7": generate_unidirectional_topology7,
            "UNIDIR8": generate_unidirectional_topology8,
            "UNIDIR9": generate_unidirectional_topology9,
            "UNIDIR10": generate_unidirectional_topology10,
            "UNIDIR11": generate_unidirectional_topology11,
            "UNIDIR12": generate_unidirectional_topology12,
            "UNIDIR13": generate_unidirectional_topology13,
            "UNIDIR14": generate_unidirectional_topology14,
            "UNIDIR15": generate_unidirectional_topology15,
        }
        try:
            return mapping[topo](link_rate)
        except KeyError:
            raise ValueError(
                f"TopologÃ­a desconocida: {topo}. "
                f"Opciones vÃ¡lidas: {', '.join(mapping.keys())}"
            )

    raise ValueError(f"TopologÃ­a desconocida: {topo}")

def generate_simple_topology(link_rate: int = 100):
    """Genera una topologÃ­a S-E-S mÃ­nima con identificadores **tuple**.

    Mantener el mismo tipo de ``link_id`` en toda la base de cÃ³digo
    evita las mÃºltiples llamadas a ``split('-')`` y simplifica las
    comprobaciones de si un nodo es *switch* o *end-station*.
    """
    graph = nx.DiGraph()
    graph.add_node("S1", node_type="SW")

    for node in ("C1", "C2", "SRV1"):
        graph.add_node(node, node_type="ES")
        # Enlaces bidireccionales (src, dst)
        graph.add_edge(node, "S1", link_id=(node, "S1"), link_rate=link_rate)
        graph.add_edge("S1", node, link_id=("S1", node), link_rate=link_rate)

    return graph

def generate_unidirectional_topology(link_rate=100):
    """Genera una topologÃ­a unidireccional donde los datos fluyen solo de clientes a servidor"""
    graph = nx.DiGraph()
    # Crear nodos
    graph.add_node("S1", node_type="SW")
    for node in ["C1", "C2", "SRV1"]:
        graph.add_node(node, node_type="ES")
    # Crear enlaces unidireccionales
    for client in ["C1", "C2"]:
        graph.add_edge(client, "S1", link_id=(client, "S1"), link_rate=link_rate)
    graph.add_edge("S1", "SRV1", link_id=("S1", "SRV1"), link_rate=link_rate)
    return graph

def generate_unidirectional_chain_topology(num_switches: int = 2,
                                           link_rate: int = 100):
    """
    Genera una topologÃ­a en cadena unidireccional con el nÃºmero especificado
    de switches (Edge â†’ Aggregation) y un servidor final.

    Los nodos cliente se asignan automÃ¡ticamente segÃºn la convenciÃ³n
    C1, C2, ..., Cn.
    """
    g = nx.DiGraph()

    # Agregar nodos de switch
    for i in range(1, num_switches + 1):
        g.add_node(f"S{i}", node_type="SW")

    # Nodo servidor
    g.add_node("SRV1", node_type="ES")

    # Clientes y enlaces cliente â†’ switch
    for i in range(1, num_switches + 1):
        cli = f"C{i}"
        g.add_node(cli, node_type="ES")                  # â† Â¡aquÃ­ el fix!
        g.add_edge(cli, f"S{i}", link_id=(cli, f"S{i}"), link_rate=link_rate)

    # Enlaces entre switches
    for i in range(1, num_switches):
        g.add_edge(f"S{i}", f"S{i+1}", link_id=(f"S{i}", f"S{i+1}"), link_rate=link_rate)

    # Enlace del Ãºltimo switch al servidor
    g.add_edge(f"S{num_switches}", "SRV1", link_id=(f"S{num_switches}", "SRV1"), link_rate=link_rate)

    return g

# Helper functions to maintain backward compatibility
def generate_unidirectional_chain_topology2(link_rate=100):
    return generate_unidirectional_chain_topology(2, link_rate)

def generate_unidirectional_chain_topology3(link_rate=100):
    return generate_unidirectional_chain_topology(3, link_rate)

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  NUEVA  TOPOLOGÃA  â€“  UNIDIR4                                     â•‘
# â•‘  "agregaciÃ³n 1-nivel":                                            â•‘
# â•‘        C1,C2,C3,C4 â†’ S1 â†’ S2 â†’ SRV1                               â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def generate_unidirectional_topology4(link_rate=100):
    """
    TopologÃ­a de dos switches en cascada (Edge â†’ Aggregation) con
    cuatro clientes y un servidor.  Todo el trÃ¡fico fluye **hacia SRV1**.
    """
    g = nx.DiGraph()

    # switches
    g.add_node("S1", node_type="SW")
    g.add_node("S2", node_type="SW")

    # end-stations
    for c in ["C1", "C2", "C3", "C4", "SRV1"]:
        g.add_node(c, node_type="ES")

    # enlaces cliente â†’ edge-switch
    for c in ["C1", "C2", "C3", "C4"]:
        g.add_edge(c, "S1", link_id=(c, "S1"), link_rate=link_rate)

    # backbone unidireccional
    g.add_edge("S1", "S2",     link_id=("S1", "S2"),     link_rate=link_rate)
    g.add_edge("S2", "SRV1",   link_id=("S2", "SRV1"),   link_rate=link_rate)

    return g

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  NUEVA  TOPOLOGÃA  â€“  UNIDIR5                                     â•‘
# â•‘  "anillo parcial / tres switches":                                â•‘
# â•‘   (C1,C2)â†’S1 â†’ S2 â†’ S3 â†’ SRV1                                     â•‘
# â•‘   (C3,C4)â†’S2                                                     â•‘
# â•‘   (C5,C6)â†’S3                                                     â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def generate_unidirectional_topology5(link_rate=100):
    """
    Tres switches en lÃ­nea (S1â†’S2â†’S3) simulando el *spine* de un pequeÃ±o
    anillo industrial.  Seis clientes repartidos, todos dirigidos a SRV1.
    """
    g = nx.DiGraph()

    # switches
    for sw in ["S1", "S2", "S3"]:
        g.add_node(sw, node_type="SW")

    # end-stations
    for es in ["C1", "C2", "C3", "C4", "C5", "C6", "SRV1"]:
        g.add_node(es, node_type="ES")

    # clientes â†’ switches locales
    for c in ["C1", "C2"]:
        g.add_edge(c, "S1", link_id=(c, "S1"), link_rate=link_rate)
    for c in ["C3", "C4"]:
        g.add_edge(c, "S2", link_id=(c, "S2"), link_rate=link_rate)
    for c in ["C5", "C6"]:
        g.add_edge(c, "S3", link_id=(c, "S3"), link_rate=link_rate)

    # backbone lineal
    g.add_edge("S1", "S2",   link_id=("S1", "S2"),   link_rate=link_rate)
    g.add_edge("S2", "S3",   link_id=("S2", "S3"),   link_rate=link_rate)
    g.add_edge("S3", "SRV1", link_id=("S3", "SRV1"), link_rate=link_rate)

    return g

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  NUEVAS TOPOLOGÃAS UNIDIR 6-8
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _add_es(graph, node_list, sw_name, link_rate):
    """Helper: aÃ±ade ESâ†’switch unidireccional."""
    for n in node_list:
        graph.add_node(n, node_type="ES")
        graph.add_edge(n, sw_name, link_id=(n, sw_name), link_rate=link_rate)


def generate_unidirectional_topology6(link_rate=100):
    """
    UNIDIR6 â€“ 'ring line' de 3 switches  
    C1,C2â†’S1 â†’ S2 â†’ SRV1  
    C3,C4 conectan a S2; C5,C6 a S3.
    """
    g = nx.DiGraph()
    # Switches
    for s in ("S1", "S2", "S3"):
        g.add_node(s, node_type="SW")
    # Server
    g.add_node("SRV1", node_type="ES")

    # End-stations
    _add_es(g, ("C1", "C2"), "S1", link_rate)
    _add_es(g, ("C3", "C4"), "S2", link_rate)
    _add_es(g, ("C5", "C6"), "S3", link_rate)

    # Backbone line
    g.add_edge("S1", "S2", link_id=("S1", "S2"), link_rate=link_rate)
    g.add_edge("S2", "S3", link_id=("S2", "S3"), link_rate=link_rate)
    g.add_edge("S3", "SRV1", link_id=("S3", "SRV1"), link_rate=link_rate)
    return g


def generate_unidirectional_topology7(link_rate=100):
    """
    UNIDIR7 â€“ 'star-of-switches'  
    Tres leaf-switches (S1-S3) con 2 clientes cada uno â†’ core S0 â†’ SRV1.
    """
    g = nx.DiGraph()
    g.add_node("S0", node_type="SW")   # Core
    for s in ("S1", "S2", "S3"):
        g.add_node(s, node_type="SW")
    g.add_node("SRV1", node_type="ES")

    _add_es(g, ("C1", "C2"), "S1", link_rate)
    _add_es(g, ("C3", "C4"), "S2", link_rate)
    _add_es(g, ("C5", "C6"), "S3", link_rate)

    for leaf in ("S1", "S2", "S3"):
        g.add_edge(leaf, "S0", link_id=(leaf, "S0"), link_rate=link_rate)
    g.add_edge("S0", "SRV1", link_id=("S0", "SRV1"), link_rate=link_rate)
    return g


def generate_unidirectional_topology8(link_rate=100):
    """
    UNIDIR8 â€“ spine/leaf simplificado (2 niveles)  
    4 leafs con 2 clientes cada uno â†’ agg S5 â†’ core S6 â†’ SRV1.
    """
    g = nx.DiGraph()
    # Switches
    for s in ("S1", "S2", "S3", "S4", "S5", "S6"):
        g.add_node(s, node_type="SW")
    g.add_node("SRV1", node_type="ES")

    # Leafs
    _add_es(g, ("C1", "C2"), "S1", link_rate)
    _add_es(g, ("C3", "C4"), "S2", link_rate)
    _add_es(g, ("C5", "C6"), "S3", link_rate)
    _add_es(g, ("C7", "C8"), "S4", link_rate)

    for leaf in ("S1", "S2", "S3", "S4"):
        g.add_edge(leaf, "S5", link_id=(leaf, "S5"), link_rate=link_rate)
    g.add_edge("S5", "S6", link_id=("S5", "S6"), link_rate=link_rate)
    g.add_edge("S6", "SRV1", link_id=("S6", "SRV1"), link_rate=link_rate)
    return g

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  NUEVAS TOPOLOGÃAS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def generate_unidirectional_topology9(link_rate=100):
    """Estrella clÃ¡sica: S1 como hub central."""
    g = nx.DiGraph()
    g.add_node("S1", node_type="SW")
    _add_es(g, [f"C{i}" for i in range(1, 7)], "S1", link_rate)
    g.add_node("SRV1", node_type="ES")
    g.add_edge("S1", "SRV1", link_id=("S1", "SRV1"), link_rate=link_rate)
    return g

def generate_unidirectional_topology10(link_rate=100):
    """Anillo de 3 switches â€“ todos los ES envÃ­an a SRV1."""
    g = nx.DiGraph()
    for sw in ("S1", "S2", "S3"):
        g.add_node(sw, node_type="SW")
    # anillo unidireccional (S1â†’S2â†’S3â†’S1)
    g.add_edge("S1", "S2", link_id=("S1", "S2"), link_rate=link_rate)
    g.add_edge("S2", "S3", link_id=("S2", "S3"), link_rate=link_rate)
    g.add_edge("S3", "S1", link_id=("S3", "S1"), link_rate=link_rate)
    # clientes
    _add_es(g, ["C1"], "S1", link_rate)
    _add_es(g, ["C2"], "S2", link_rate)
    _add_es(g, ["C3"], "S3", link_rate)
    # servidor accesible desde cualquier switch
    g.add_node("SRV1", node_type="ES")
    for sw in ("S1", "S2", "S3"):
        g.add_edge(sw, "SRV1", link_id=(sw, "SRV1"), link_rate=link_rate)
    return g

def generate_unidirectional_topology11(link_rate=100):
    """Core-Aggregation-Edge con 8 ES â€“ prueba multi-nivel."""
    g = nx.DiGraph()
    # core
    g.add_node("S0", node_type="SW")
    # aggregation
    for aggr in ("S1", "S2"):
        g.add_node(aggr, node_type="SW")
        g.add_edge(aggr, "S0", link_id=(aggr, "S0"), link_rate=link_rate)
    # edge
    edge_map = {"S1": ("S3", "S4"), "S2": ("S5", "S6")}
    for aggr, edges in edge_map.items():
        for esw in edges:
            g.add_node(esw, node_type="SW")
            g.add_edge(esw, aggr, link_id=(esw, aggr), link_rate=link_rate)
    # end-stations under edge switches
    client_map = {
        "S3": ("C1", "C2"), "S4": ("C3", "C4"),
        "S5": ("C5", "C6"), "S6": ("C7", "C8"),
    }
    for esw, cls in client_map.items():
        _add_es(g, cls, esw, link_rate)
    # server
    g.add_node("SRV1", node_type="ES")
    g.add_edge("S0", "SRV1", link_id=("S0", "SRV1"), link_rate=link_rate)
    return g

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  UNIDIR-12  â†’  grafo completo (7 nodos)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def generate_unidirectional_topology12(link_rate=100):
    """
    Grafo *todos-con-todos*: dos switches + 4 clientes + 1 servidor.
    Se generan enlaces **dirigidos** entre *cada* par de nodos distinto.
    """
    g = nx.DiGraph()

    switches = ["S1", "S2"]
    clients  = [f"C{i}" for i in range(1, 5)]
    server   = ["SRV1"]
    all_nodes = switches + clients + server

    # anotar tipo de nodo
    for n in switches:
        g.add_node(n, node_type="SW")
    for n in clients + server:
        g.add_node(n, node_type="ES")

    # enlaces dirigidos entre cualquier par u â‰  v
    for u in all_nodes:
        for v in all_nodes:
            if u == v:
                continue
            g.add_edge(u, v, link_id=(u, v), link_rate=link_rate)

    return g

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  UNIDIR-13  Â·  Cliente + anillo SW + 2 servidores
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def generate_unidirectional_topology13(link_rate: int = 100):
    """
    TopologÃ­a:

        C1 â†’ S1 â†’ S2 â†’ S3 â†’ S4 â†’ S1   (anillo CW)
        S1 â†’ SRV2
        S3 â†’ SRV1

    Todos los enlaces son **dirigidos** y de la misma velocidad.
    """
    g = nx.DiGraph()

    # â”€â”€ nodos â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    sw_nodes  = ["S1", "S2", "S3", "S4"]
    cli_nodes = ["C1"]
    srv_nodes = ["SRV1", "SRV2"]

    for n in sw_nodes:
        g.add_node(n, node_type="SW")
    for n in cli_nodes + srv_nodes:
        g.add_node(n, node_type="ES")

    # â”€â”€ enlaces clienteâ†’anillo â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    g.add_edge("C1", "S1", link_id=("C1", "S1"), link_rate=link_rate)

    # â”€â”€ anillo unidireccional S1â†’S2â†’S3â†’S4â†’S1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ring = ["S1", "S2", "S3", "S4", "S1"]
    for u, v in zip(ring, ring[1:]):
        g.add_edge(u, v, link_id=(u, v), link_rate=link_rate)

    # â”€â”€ salidas a servidores â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    g.add_edge("S3", "SRV1", link_id=("S3", "SRV1"), link_rate=link_rate)
    g.add_edge("S1", "SRV2", link_id=("S1", "SRV2"), link_rate=link_rate)

    return g

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  UNIDIR-14 :  Â«ruedaÂ» con hub central + anillo de 4 switches
#               â”€ 3 clientes en S1-S3, servidor en S4
#               â”€ trÃ¡fico *unidireccional* desde los clientes â†’ servidor
#                 (paths:  Cxâ†’Siâ†’S0â†’S4â†’SRV1)
# ---------------------------------------------------------------------
def generate_unidirectional_topology14(link_rate=100):
    """
    TopologÃ­a 'rueda':
        â€¢ S0  â”€ hub central
        â€¢ S1,S2,S3,S4 â”€ switches de borde formando un *anillo* unidireccional
        â€¢ C1,C2,C3   â”€ clientes (uno por S1-S3)
        â€¢ SRV1       â”€ servidor conectado a S4

    DirecciÃ³n de los enlaces (â†’):
        C* â†’ S*    (clientes al borde)
        Si â†’ S0    (borde â‡’ hub)
        S0 â†’ S4    (hub â‡’ borde con servidor)
        S1 â†’ S2 â†’ S3 â†’ S4 â†’ S1   (anillo horario)
        S4 â†’ SRV1  (Ãºltimo salto al servidor)
    """
    g = nx.DiGraph()

    # Hub central
    g.add_node("S0", node_type="SW")

    # Switches de borde en el anillo
    edge_sw = ["S1", "S2", "S3", "S4"]
    for sw in edge_sw:
        g.add_node(sw, node_type="SW")

    # Enlaces del anillo (S1â†’S2â†’S3â†’S4â†’S1)
    for i, src in enumerate(edge_sw):
        dst = edge_sw[(i + 1) % len(edge_sw)]
        g.add_edge(src, dst, link_id=(src, dst), link_rate=link_rate)

    # Spokes (borde â†’ hub) y regreso hub â†’ S4 para llegar al servidor
    for sw in edge_sw:
        g.add_edge(sw, "S0", link_id=(sw, "S0"), link_rate=link_rate)
    g.add_edge("S0", "S4", link_id=("S0", "S4"), link_rate=link_rate)

    # Clientes
    clients = [("C1", "S1"), ("C2", "S2"), ("C3", "S3")]
    for c, sw in clients:
        g.add_node(c, node_type="ES")
        g.add_edge(c, sw, link_id=(c, sw), link_rate=link_rate)

    # Servidor
    g.add_node("SRV1", node_type="ES")
    g.add_edge("S4", "SRV1", link_id=("S4", "SRV1"), link_rate=link_rate)

    return g

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  UNIDIR-15 :  pentÃ¡gono de 5 switches (S0-S4) con 3 clientes y 1 servidor
#               â€¢ ciclo dirigido: S0â†’S1â†’S2â†’S3â†’S4â†’S0
#               â€¢ C1â†’S0 ; C2â†’S1 ; C3â†’S2
#               â€¢ S3â†’SRV1   (servidor colgado en S3)
# ---------------------------------------------------------------------
def generate_unidirectional_topology15(link_rate=100):
    """
    TopologÃ­a pentagonal unidireccional:

        C1â†’S0 â†’ S1 â†’ S2 â†’ S3 â†’ S4 â†’ S0
                  â†‘     â”‚
        C2â†’S1     â”‚     â”‚
                  â”‚     â†“
        C3â†’S2     SRV1â†S3

    Todo el trÃ¡fico fluye **hacia delante** siguiendo la orientaciÃ³n de las flechas,
    por lo que cualquier paquete de un cliente alcanzarÃ¡ el servidor en S3
    despuÃ©s de recorrer parte del pentÃ¡gono.
    """
    g = nx.DiGraph()

    # ----- switches S0-S4 -----
    sw = [f"S{i}" for i in range(5)]
    for s in sw:
        g.add_node(s, node_type="SW")

    # pentÃ¡gono dirigido (horario)
    for i in range(5):
        src = sw[i]
        dst = sw[(i + 1) % 5]
        g.add_edge(src, dst, link_id=(src, dst), link_rate=link_rate)

    # ----- clientes -----
    clients = [("C1", "S0"), ("C2", "S1"), ("C3", "S2")]
    for c, s in clients:
        g.add_node(c, node_type="ES")
        g.add_edge(c, s, link_id=(c, s), link_rate=link_rate)

    # ----- servidor -----
    g.add_node("SRV1", node_type="ES")
    g.add_edge("S3", "SRV1", link_id=("S3", "SRV1"), link_rate=link_rate)

    return g

def _transform_line_graph(graph):
    """Transforma un grafo en su lÃ­nea de grafo y crea diccionario de enlaces"""
    line_graph = nx.line_graph(graph)
    links_dict = {node: Link(node, graph.edges[node]['link_rate']) for node in line_graph.nodes}
    return line_graph, links_dict

class Network:
    def __init__(self, graph, flows):
        self.graph = graph
        self.flows = flows
        # Construir lÃ­nea de grafo y diccionario de enlaces
        self.line_graph, self.links_dict = _transform_line_graph(graph)

    def disable_gcl(self, num_nodes):
        """Deshabilita la capacidad GCL para un nÃºmero especÃ­fico de nodos"""
        list_nodes = random.sample(list(self.graph.nodes), num_nodes)
        list_links = []
        for node in list_nodes:
            list_links.extend([link for link in self.links_dict.values() if node == link.link_id[0]])
        for link in list_links:
            link.gcl_capacity = 0

    def set_gcl(self, num_gcl):
        """Establece la capacidad GCL para todos los enlaces"""
        for link in self.links_dict.values():
            link.gcl_capacity = num_gcl

class FlowGenerator:
    def __init__(self, graph, seed=None, period_set=None, min_payload=1500, max_payload=1518): # AÃ±adir min/max payload
        self.graph = graph
        # Inicializar semilla para reproducibilidad
        if seed is not None:
            random.seed(seed)
        # Inicializar conjunto de perÃ­odos usando el global PERIOD_SET como default
        self.period_set = period_set if period_set is not None else PERIOD_SET
        for period in self.period_set:
            assert isinstance(period, int) and period > 0
        # Jitters eliminado
        # Identificar nodos finales
        self.es_nodes = [n for n, d in graph.nodes(data=True) if d['node_type'] == 'ES']
        self.num_generated_flows = 0
        # Guardar rango de payload
        self.min_payload = min_payload
        self.max_payload = max_payload
        assert self.min_payload <= self.max_payload, "min_payload debe ser <= max_payload"

    def _generate_flow(self):
        """Genera un Ãºnico flujo aleatorio"""
        # Seleccionar dos nodos aleatorios
        src_id, dst_id = random.sample(self.es_nodes, 2)
        # Calcular ruta mÃ¡s corta
        path = nx.shortest_path(self.graph, src_id, dst_id)
        path = [(path[i], path[i+1]) for i in range(len(path)-1)]
        # Seleccionar perÃ­odo aleatorio
        period = random.choice(self.period_set)

        # Crear flujo con payload aleatorio dentro del rango especificado
        flow = Flow(
            f"F{self.num_generated_flows}", src_id, dst_id, path,
            payload=random.randint(self.min_payload, self.max_payload), # Usar el rango
            period=period
        )
        self.num_generated_flows += 1
        return flow

    def __call__(self, num_flows=1):
        """Genera un conjunto de flujos aleatorios"""
        return [self._generate_flow() for _ in range(num_flows)]

class UniDirectionalFlowGenerator(FlowGenerator):
    def _generate_flow(self):
        """Genera un flujo unidireccional desde clientes a servidores"""
        # Identificar nodos cliente y servidor
        client_nodes = [n for n in self.es_nodes if n.startswith('C')]
        server_nodes = [n for n in self.es_nodes if n.startswith('SRV')]
        
        # Si no hay clientes o servidores, usar generaciÃ³n normal
        if not client_nodes or not server_nodes:
            return super()._generate_flow()
            
        # Seleccionar origen y destino
        src_id = random.choice(client_nodes)
        dst_id = random.choice(server_nodes)
        
        # Calcular ruta
        path = nx.shortest_path(self.graph, src_id, dst_id)
        path = [(path[i], path[i+1]) for i in range(len(path)-1)]
        
        # Seleccionar parÃ¡metros
        period = random.choice(self.period_set)
        
        # Crear flujo con payload aleatorio dentro del rango especificado
        flow = Flow(
            f"F{self.num_generated_flows}", src_id, dst_id, path,
            payload=random.randint(self.min_payload, self.max_payload), # Usar el rango
            period=period
        )
        self.num_generated_flows += 1
        return flow

def generate_flows(graph, num_flows=50, seed=None, period_set=PERIOD_SET, unidirectional=False, min_payload=1500, max_payload=1518): # AÃ±adir min/max payload
    """FunciÃ³n para generar flujos con configuraciÃ³n especÃ­fica"""
    generator_class = UniDirectionalFlowGenerator if unidirectional else FlowGenerator
    # Pasar el rango de payload al constructor del generador
    generator = generator_class(graph, seed, period_set, min_payload, max_payload)
    return generator(num_flows)



## ARCHIVO: core/network/operation.py
## ==================================================

import copy
from dataclasses import dataclass
import math
from typing import Optional
import numpy as np

from core.network.net import Net  # AÃ±adido para acceder a Net.DELAY_PROC_RX



@dataclass
class Operation:
    start_time: int
    gating_time: Optional[int]
    latest_time: int  # must equal to gating time if enable gating
    end_time: int
    # Nuevos campos para anÃ¡lisis y visualizaciÃ³n
    reception_time: Optional[int] = None
    
    # Solo se conserva la espera por *min-gap* (controlada por RL)
    min_gap_wait: int = 0
    
    # Campos para decisiones de RL (eliminado gcl_cycle_opt)
    guard_factor: float = 1.0
    min_gap_value: float = 1.0
    conflict_strategy: int = 0

    def __post_init__(self):
        if self.gating_time is not None:
            assert self.start_time <= self.gating_time == self.latest_time < self.end_time, \
                   "Invalid Operation: desajuste temporal"
        else:
            assert self.start_time <= self.latest_time < self.end_time, "Invalid Operation"
        
        # Calcular automÃ¡ticamente el tiempo de recepciÃ³n completa si no se proporciona
        if self.reception_time is None:
            # Tomar en cuenta retardo de propagaciÃ³n y procesamiento de recepciÃ³n
            self.reception_time = self.end_time + Net.DELAY_PROP + Net.DELAY_PROC_RX

    def add(self, other: int | np.integer):
        """
        Desplaza la operaciÃ³n en ``other`` Âµs.
        Acepta enteros python normales **o** cualquier subtipo de ``np.integer``.
        """
        # Convertir numpy.int* a int nativo para evitar fallos de tipo
        if isinstance(other, np.integer):
            other = int(other)
        assert isinstance(other, int), "Operation.add() espera un entero"

        self.start_time += other
        if self.gating_time is not None:
            self.gating_time += other
        self.latest_time += other
        self.end_time += other
        # earliest_time ya no existe o se alinea con start_time
        # Actualizar reception_time tambiÃ©n
        if self.reception_time is not None:
             self.reception_time += other
        return self

    def __repr__(self):
        if self.gating_time is not None:
            return (f"Operation(start={self.start_time}, gate={self.gating_time}, "
                    f"end={self.end_time}, rcv={self.reception_time})")
        return (f"Operation(start={self.start_time}, latest={self.latest_time}, "
                f"end={self.end_time}, rcv={self.reception_time})")

    # â• Utilidad
    @property
    def duration(self):
        """DuraciÃ³n efectiva de la transmisiÃ³n (end âˆ’ start)"""
        return self.end_time - self.start_time

    # Nueva propiedad para obtener el desglose de esperas
    @property
    def wait_breakdown(self):
        """Retorna un diccionario con el desglose de las causas de espera"""
        if self.gating_time is None or self.gating_time <= self.start_time:
            return {}
        
        total_wait = self.gating_time - self.start_time
        result = {
            'total': total_wait,
            'min_gap': self.min_gap_wait,
            'other': total_wait - self.min_gap_wait
        }
        return result

    # Eliminar la propiedad jitter_percent
    
    # MÃ©todo para registrar el perÃ­odo asociado
    def set_period(self, period: int):
        """Almacena el perÃ­odo asociado a esta operaciÃ³n para cÃ¡lculos posteriores"""
        self._period = period
        return self

#
def check_operation_isolation(operation1: tuple[Operation, int],
                              operation2: tuple[Operation, int]) -> Optional[int]:
    """

    :param operation1:
    :param operation2:
    :return: None if isolation constraint is satisfied,
             otherwise, it returns the offset that `operation1` should add.
             Notice that the adding the returned offset might make `operation` out of period.
    """
    operation1, period1 = operation1
    operation2, period2 = operation2

    assert (operation1.start_time >= 0) and (operation1.end_time <= period1)
    assert (operation2.start_time >= 0) and (operation2.end_time <= period2)

    hyper_period = math.lcm(period1, period2)
    alpha = hyper_period // period1
    beta = hyper_period // period2

    operation_lhs = copy.deepcopy(operation1)

    for _ in range(alpha):

        operation_rhs = copy.deepcopy(operation2)
        for _ in range(beta):
            if (operation_lhs.start_time <= operation_rhs.start_time < operation_lhs.end_time) or \
                    (operation_rhs.start_time <= operation_lhs.start_time < operation_rhs.end_time):
                # Desplazamiento bruto necesario para eliminar la colisiÃ³n
                raw_offset = operation_rhs.end_time - operation_lhs.start_time

                # Reducirlo al mÃ­nimo compatible con ambos perÃ­odos
                gcd_period = math.gcd(period1, period2)
                offset = raw_offset % gcd_period

                # Si el mÃ³dulo es 0 (raw_offset mÃºltiplo del mcd) desplazamos un ciclo completo
                return offset if offset != 0 else gcd_period
            operation_rhs.add(period2)

        operation_lhs.add(period1)
    return None



## ARCHIVO: core/omnet_export.py
## ==================================================

"""core.omnet_export
---------------------------------
Utilities to export a `Network` plus a scheduling result (`ScheduleRes`)
obtained with `ui/test.py` into two artefacts ready to be consumed by
OMNeT++ NETâ€‘TSN:

* **<name>.ned** â€“  the physical topology as a `network` definition.
* **<name>.ini** â€“  simulation configuration that reproduces the exact
  cyclic traffic (period, payload, initial offset) *and* programmes the
  timeâ€‘aware shaping gate according to the static GCL derived during the
  DRL scheduling step.

Files are *always* overwritten if they already exist.

This module is completely decoupled from the rest of the codeâ€‘base â€“ it
only relies on public attributes of `core.network` objects, on the final
`ScheduleRes`, and on the Â«GCL tablesÂ» returned by `ResAnalyzer`.
"""


from __future__ import annotations
import logging

import os
import math
from pathlib import Path
from typing import Dict, List, Tuple
from collections import defaultdict

from core.network.net import Network, Flow, Link
from core.scheduler.scheduler import ScheduleRes

# ---------------------------------------------------------------------------
#  UTILIDADES DE FORMATO  (â†“ se usan en todo el mÃ³dulo, por eso van primero)
# ---------------------------------------------------------------------------
Î¼s = 1e-6

def _us_to_ms_str(us: int) -> str:
    """Âµs â†’ "x.xxms" sin ceros sobrantes."""
    return f"{us * Î¼s * 1_000:.3f}ms".rstrip("0").rstrip(".")

# ---------- helpers para puertos y PCP ----------
def _build_period_order(flows: list["Flow"]) -> dict[int, int]:
    """Mapa perÃ­odoâ†’Ã­ndice incremental (2000 Âµsâ‡’0 â†’ UDP 2000, 4000 Âµsâ‡’1 â†’ 2001â€¦)."""
    unique = sorted({f.period for f in flows})
    return {p: i for i, p in enumerate(unique)}

def _flow_port(period: int, period_order: dict[int, int]) -> int:
    return 2000 + period_order[period]

# PCP: 4 para primer flujo por cliente, 7 el resto
def _pcp_for_flow(node_name: str, seen: dict[str, int]) -> int:
    pcp = 4 if seen[node_name] == 0 else 7
    seen[node_name] += 1
    return pcp

def _first_tx_offset_us(schedule_res: "ScheduleRes", fl: "Flow") -> int:
    """
    Devuelve el **start_time** del primer hop del flujo `fl`.

    ğŸ”¸ La comparaciÃ³n se hace por `flow_id` (==) en lugar de identidad
       de objetos, porque los `Flow` en `schedule_res` pueden ser copias.
    """
    first_link_id = fl.path[0]            # (src, dst) del primer salto
    for link, ops in schedule_res.items():
        if link.link_id != first_link_id:
            continue
        for f, op in ops:
            if f.flow_id == fl.flow_id:   # â† comparaciÃ³n robusta
                return op.start_time
    # fallback â€“ no deberÃ­a ocurrir
    return 0

# ---------------------------------------------------------------------------
#  Small helpers                                                               
# ---------------------------------------------------------------------------

# (las tres funciones ya se han adelantado)


def _indent(n: int) -> str:
    return " " * n


# ---------------------------------------------------------------------------
#  NED generation                                                             
# ---------------------------------------------------------------------------


def _ned_node_line(node_name: str, node_type: str, xpos: int, ypos: int) -> str:
    """Return a *submodule* line for a node with display coordinates."""
    return (
        f"        {node_name}: <default(\"{node_type}\")> like IEthernetNetworkNode {{\n"
        f"            @display(\"p={xpos},{ypos}\");\n"
        f"        }}\n"
    )


#  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  INI generation
#  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _ini_general(net_name: str, gate_port: int) -> str:
    return (
        "#########################################################\n"
        "#  AUTOGENERATED BY gatingdrl omnet_export.py            #\n"
        "#########################################################\n"
        "[General]\n"
        f"network = {net_name}\n"
        "sim-time-limit   = 24ms\n"
        'description      = "Auto-imported schedule"\n\n'
        "**.displayGateSchedules = true\n"
        # Mostrar por defecto todos los puertos (el filtro se puede afinar despuÃ©s)
        f'**.gateFilter = "**.eth[{gate_port}].**"\n'
        "**.gateScheduleVisualizer.height = 16\n"
        '**.gateScheduleVisualizer.placementHint = "top"\n\n'
    )

def _ini_flows(
        flows: List["Flow"],
        schedule_res: ScheduleRes,
        network: "Network"           # â† necesitamos acceso al grafo
) -> str:
    """
    Construye la secciÃ³n SOURCES + SINKS a partir de la lista completa de flujos.
    No debe lanzar excepciÃ³n â€“ si un flujo carece de algÃºn dato, lo salta y sigue.
    """
    by_src: Dict[str, List["Flow"]] = {}
    by_dst: Dict[str, List["Flow"]] = {}
    for f in flows:
        by_src.setdefault(f.src_id.lower(), []).append(f)
        by_dst.setdefault(f.dst_id.lower(), []).append(f)

    txt = "#########################################################\n"
    txt += "#  SOURCES                                               #\n"
    txt += "#########################################################\n"
    # â–¸ Puerto UDP Ãºnico por cliente (2000 + Ã­ndice del cliente)
    client_port_map: Dict[str, int] = {
        n: 2000 + i for i, n in enumerate(sorted(by_src.keys()))
    }
    seen_per_client: Dict[str, int] = defaultdict(int)
    for node, list_flows in by_src.items():
        txt += f"*.{node}.numApps           = {len(list_flows)}\n"
        txt += f"*.{node}.app[*].typename   = \"UdpSourceApp\"\n"
        txt += f"*.{node}.app[*].io.destAddress = \"srv1\"\n\n"

        for idx, fl in enumerate(list_flows):
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ NUEVO: etiqueta legible para el flujo â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # Usa el flow_id ('F0', 'F1'â€¦) en minÃºsculas  â†’  "f0", "f1", â€¦
            # Si se prefiere "video-1" basta con reemplazar la siguiente lÃ­nea
            display_name = fl.flow_id.lower()
            txt += f"*.{node}.app[{idx}].display-name   = \"{display_name}\"\n"

            # â‹ Offset = Â«Disponible (start time)Â» â‡’ op.start_time
            off_us = _first_tx_offset_us(schedule_res, fl)
            off_ms = _us_to_ms_str(off_us)
            
            # âŠ TamaÃ±o de trama = payload mostrado en Â«INFORMACIÃ“N DE FLUJOSÂ»
            #    (no restamos 54 B de cabeceras; es exactamente el valor listado).
            length = max(64, fl.payload)

            txt += f"*.{node}.app[{idx}].source.productionInterval = {fl.period*Î¼s*1_000:.0f}ms\n"
            txt += f"*.{node}.app[{idx}].source.initialProductionOffset = {off_ms}\n"
            txt += f"*.{node}.app[{idx}].source.packetLength = {length}B\n"

            dport = client_port_map[node]           # puerto fijo por cliente
            txt  += f"*.{node}.app[{idx}].io.destPort = {dport}\n"

            pcp = _pcp_for_flow(node, seen_per_client)
            txt += (f"*.{node}.app[{idx}].bridging.streamIdentifier.identifier.mapping = "
                    f"[{{stream: \"{fl.flow_id}\", packetFilter: expr(udp.destPort == {dport})}}]\n")
            txt += (f"*.{node}.app[{idx}].bridging.streamCoder.encoder.mapping = "
                    f"[{{stream: \"{fl.flow_id}\", pcp: {pcp}}}]\n\n")

    # ------- habilitar outgoing streams por cliente -----------
    for node in by_src.keys():
        txt += f"*.{node}.hasOutgoingStreams = true\n"

    # ------- habilitar egress shaping en cada switch ----------
    sw_nodes = [n.lower() for n, d in network.graph.nodes(data=True)
                if d.get("node_type") == "SW"]
    for sw in sw_nodes:
        txt += f"*.{sw}.hasEgressTrafficShaping = true\n"
        txt += (f"*.{sw}.bridging.directionReverser.reverser."
                f"excludeEncapsulationProtocols = [\"ieee8021qctag\"]\n")

    txt += "\n#########################################################\n"
    txt += "#  SINKS                                                 #\n"
    txt += "#########################################################\n"
    for node, list_flows in by_dst.items():
        txt += f"*.{node}.numApps         = {len(list_flows)}\n"
        txt += f"*.{node}.app[*].typename = \"UdpSinkApp\"\n"
        for idx, _ in enumerate(list_flows):
            txt += f"*.{node}.app[{idx}].io.localPort = {2000+idx}\n"
        txt += "\n"

    return txt

def _ini_gcl(
    gcl: Dict["Link", List[Tuple[int, int]]],
    network: "Network",
    gate_port: int = 0,           # â† puerto declarado en **.gateFilter
) -> str:
    """
    Genera la secciÃ³n de programaciÃ³n Time-Aware para cada puerto de switch.
    Si `gcl` estÃ¡ vacÃ­o simplemente devuelve cadena vacÃ­a.
    """
    if not gcl:
        return ""

    txt = "#########################################################\n"
    txt += "#  TIME-AWARE TRAFFIC SHAPING                           #\n"
    txt += "#########################################################\n"

    for link, table in gcl.items():
        src = link.link_id[0] if isinstance(link.link_id, tuple) else link.link_id.split('-')[0]
        dst = link.link_id[1] if isinstance(link.link_id, tuple) else link.link_id.split('-')[1]
        # Usa el mismo Ã­ndice que figura en **.gateFilter
        port = gate_port

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  UNA sola cola "video" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        txt += f"*.{src.lower()}.eth[{port}].macLayer.queue.numTrafficClasses = 1\n"
        txt += f"*.{src.lower()}.eth[{port}].macLayer.queue.*[0].display-name = \"video\"\n"

        # Gate Ãºnico TC0 â€“ usar **la tabla que ya imprimiÃ³ ResAnalyzer**.
        #     â®‘  No se vuelven a ordenar ni a recalcular tiempos.
        # âŠ  HiperperÃ­odo = m.c.m. de los perÃ­odos de *toda la red*
        import math as _m
        hyperperiod = 1
        for _f in network.flows:
            hyperperiod = _m.lcm(hyperperiod, _f.period)

        # â–º LA TABLA LLEGA TAL CUAL SE IMPRIMIO EN
        #   "TABLA GCL GENERADA (t, estado)"  â—„
        #   â”€ no se toca, sÃ³lo se ordena por tiempo.
        # â”€â”€ tabla "corta" que llega desde ResAnalyzer â”€â”€
        events = sorted(table, key=lambda x: x[0])  # ya viene filtrada

        # Ordenar la tabla generada y *reemplazar* su PRIMER registro
        #     por "abierto en 0 Âµs" (no se aÃ±ade, se sustituye).
        if events:
            events[0] = (0, 1)      # primer registro forzado
        else:
            events = [(0, 1)]       # salvaguarda: lista vacÃ­a

        # Elimina duplicados consecutivos (p.ej. â€¦, (0,1), (0,1), â€¦)
        compact: list[tuple[int, int]] = []
        for t, s in events:
            if not compact or compact[-1][1] != s:
                compact.append((t, s))

        durations = [(compact[(i+1)%len(compact)][0] - t) % hyperperiod
                     for i, (t, _) in enumerate(compact)]

        txt += f"*.{src.lower()}.eth[{port}].macLayer.queue.transmissionGate[0].offset = 0ms\n"
        txt += f"*.{src.lower()}.eth[{port}].macLayer.queue.transmissionGate[0].durations = "
        txt += f"[{', '.join(_us_to_ms_str(d) for d in durations)}]\n"

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        #  DEBUG   Comparar la tabla original (events) con la reconstruida
        #          a partir de las durations que vamos a escribir.
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        #
        # 1.  Tabla "original" (la que viene de ResAnalyzer) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        orig_table = sorted(table, key=lambda x: x[0])    # [(t,state), ...]

        # 2.  Reconstruir tabla a partir de durations  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        recon_table: list[tuple[int,int]] = []
        t_acc = 0
        state = 1                      # siempre arrancamos "abierto"
        for d in durations:
            recon_table.append((t_acc, state))
            t_acc = (t_acc + d) % hyperperiod
            state = 1 - state          # alternar 1â†”0

        # 3.  Normalizar y ordenar para comparar
        recon_norm = sorted(recon_table, key=lambda x: x[0])
        orig_norm  = orig_table

        # Si las tablas no coinciden, registra un mensaje breve solo en nivel DEBUG.
        # AsÃ­ evitamos saturar la salida estÃ¡ndar durante `ui/test.py`.
        if recon_norm != orig_norm:
            import logging
            logging.getLogger(__name__).debug(
                f"[GCL-CHECK] Diferencia detectada en {src}.eth[{port}] "
                "(detalles omitidos; habilita DEBUG para ver el diff)."
            )

    return txt


def write_ned(network: Network, outfile: os.PathLike, pkg: str, net_name: str) -> None:
    """Generate a .ned file representing *exactly* the passed network."""

    # â”€â”€ Simple â€“ we only need node names and edges.  Use a naÃ¯ve layout: â”€â”€
    # â€¢ Endâ€‘stations on the left (x â‰ˆ 300)  â€“ spread vertically in steps
    # â€¢ Switches   in the middle (x â‰ˆ 500)
    # â€¢ Servers    on the right (x â‰ˆ 700)

    es_nodes = [n for n, d in network.graph.nodes(data=True) if d.get("node_type") == "ES"]
    sw_nodes = [n for n, d in network.graph.nodes(data=True) if d.get("node_type") == "SW"]

    server_nodes = [n for n in es_nodes if n.startswith("SRV")]
    client_nodes = [n for n in es_nodes if n not in server_nodes]

    # Coordinates
    step = 80
    submod_lines: List[str] = []

    for i, name in enumerate(client_nodes):
        submod_lines.append(_ned_node_line(name.lower(), "TsnDevice", 300, 200 + i * step))

    for i, name in enumerate(sw_nodes):
        submod_lines.append(_ned_node_line(name.lower(), "TsnSwitch", 500, 250 + i * step))

    for i, name in enumerate(server_nodes):
        submod_lines.append(_ned_node_line(name.lower(), "TsnDevice",
                                         700, 250 + i * step))

    # Connections â€“ iterate original directed edges and create <--> pairs.
    conn_lines: List[str] = []
    added = set()
    for src, dst, edata in network.graph.edges(data=True):
        # Create deterministic id for unordered pair to avoid duplicates
        key = tuple(sorted((src, dst)))
        if key in added:
            continue
        added.add(key)

        # Mantener el mismo alias en las conexiones
        def _alias(n: str) -> str:
            return n.lower()
        conn_lines.append(
            f"        {_alias(src)}.ethg++ <--> EthernetLink <--> {_alias(dst)}.ethg++;\n"
        )
        
    net_name = Path(outfile).stem          # p.ej.  Â«unidirÂ»
    # Detectar bitrate (asumimos que todos los enlaces comparten valor)
    try:
        first_edge = next(iter(network.graph.edges(data=True)))
        link_rate = first_edge[2].get("link_rate", 100)
    except StopIteration:
        link_rate = 100

    ned = (
        f"package {pkg};\n\n"
        "import inet.node.ethernet.EthernetLink;\n"
        "import inet.node.contract.IEthernetNetworkNode;\n"
        "import inet.node.tsn.TsnDevice;\n"
        "import inet.node.tsn.TsnSwitch;\n"
        "import inet.networks.base.TsnNetworkBase;\n\n"
        f"network {net_name.lower()} extends TsnNetworkBase\n"  # Force lowercase here
        "{\n"
        "    parameters:\n"
        f"        *.eth[*].bitrate = default({link_rate}Mbps);\n"
        "    submodules:\n"
        + "".join(submod_lines)
        + "\n    connections:\n"
        + "".join(conn_lines)
        + "}\n"
    )

    Path(outfile).write_text(ned, encoding="utf-8")

def write_ini(
    network: Network,
    schedule_res: ScheduleRes,
    gcl_tables: Dict[Link, List[Tuple[int, int]]],
    outfile: os.PathLike,
    net_name: str,
    gate_port: int,
) -> None:
    """Generate an .ini file reproducing traffic & GCL schedule.
    
    This function never fails completely - if problems occur during generation,
    it will still produce at least a basic INI file with the [General] section.
    """
    try:
        flows = list(network.flows)
        num_flows = len(flows)
        
        parts: List[str] = [_ini_general(net_name, gate_port)]
        try:
            parts.append(_ini_flows(flows, schedule_res, network))
        except Exception as exc:
            import logging
            logging.warning(f"[omnet_export] Error generating FLOWS section: {exc}")
            parts.append(f"#  (ERROR generando secciÃ³n SOURCES / SINKS: {exc})\n")

        try:
            parts.append(_ini_gcl(gcl_tables, network, gate_port))
        except Exception as exc:
            import logging
            logging.warning(f"[omnet_export] Error generating GCL section: {exc}")
            parts.append(f"#  (ERROR generando secciÃ³n GCL: {exc})\n")

        Path(outfile).write_text("".join(parts), encoding="utf-8")
            
    except Exception as e:
        import logging
        logging.error(f"[omnet_export] Critical error generating INI file: {e}")
        # Ensure at least a minimal INI file is written
        minimal_ini = "[General]\nnetwork = inet.showcases.tsn.generated.GeneratedTsnNetwork\n"
        Path(outfile).write_text(minimal_ini, encoding="utf-8")



def export_omnet_files(network: Network, schedule_res: ScheduleRes, gcl_tables: Dict[Link, List[Tuple[int, int]]], label: str, out_dir: os.PathLike, gate_port:int=2) -> Tuple[Path, Path]:
    """Create *.ned & *.ini in *out_dir* (overwriting any previous version)."""
    os.makedirs(out_dir, exist_ok=True)

    # Create a subdirectory for this topology if it doesn't exist
    topo_dir = Path(out_dir) / label
    topo_dir.mkdir(exist_ok=True, parents=True)

    ned_path = topo_dir / f"{label}.ned"
    ini_path = topo_dir / f"{label}.ini"

    pkg = "inet.showcases.tsn.trafficshaping.Pruebas_tesis.Red_" + label
    net_name = label.lower()

    write_ned(network, ned_path, pkg, net_name)
    write_ini(network, schedule_res, gcl_tables, ini_path, net_name, gate_port)

    print(f"[omnet_export]  generated â†’ {ned_path}, {ini_path}")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    #  Resumen global de planificaciÃ³n (ahora SÃ con variables)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    total_flows = len(network.flows)
    scheduled_ids = {
        f.flow_id
        for ops in schedule_res.values()
        for f, _ in ops
    }                                        # â† Ãºnicos
    ok_flows = len(scheduled_ids)

    msg = f"Programados con Ã©xito: {ok_flows}/{total_flows} flujos"
    print(msg)           # â† visible siempre
    logging.info(msg)    # â† para quien tenga el logger a nivel INFO
    return ned_path, ini_path



## ARCHIVO: core/scheduler/__init__.py
## ==================================================

"""
Algoritmos de scheduling para Time Sensitive Networks
"""

from .scheduler import DrlScheduler, ResAnalyzer, ScheduleRes







## ARCHIVO: core/scheduler/scheduler.py
## ==================================================

import logging
import os
from typing import Dict, List, Tuple
import math
from collections import defaultdict

import numpy as np
from sb3_contrib import MaskablePPO
from stable_baselines3 import A2C, DQN, PPO, SAC
# Usamos el mismo extractor que durante el entrenamiento
from core.learning.encoder import FeaturesExtractor
from core.learning.hats_extractor import HATSExtractor
from core.learning.maskable_sac import MaskableSAC
from stable_baselines3.common.vec_env import DummyVecEnv


from tools.definitions import OUT_DIR
from core.network.net import Flow, Link, Network, Net
from core.network.operation import Operation
from core.learning.environment import NetEnv

# Tipo para resultados
ScheduleRes = Dict[Link, List[Tuple[Flow, Operation]]]

class DrlScheduler:
    """Scheduler TSN usando Deep Reinforcement Learning con MaskablePPO"""
    
    SUPPORTING_ALG = {
        'A2C': A2C,
        'DQN': DQN,
        'PPO': PPO,
        'MaskablePPO': MaskablePPO,
        'SAC': SAC,
        'MaskableSAC': MaskableSAC
    }
    
    def __init__(self, network: Network, num_envs=1, timeout_s=300, use_curriculum=False, alg='MaskablePPO', **kwargs):
        """Inicializa el scheduler con una red y opcionalmente nÃºmero de entornos"""
        self.network = network
        self.num_flows = len(network.flows)
        self.timeout_s = timeout_s
        self.alg = alg        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        #  INFERENCIA â‡’ un solo entorno
        #     Con varios envs el mÃ©todo schedule() se detiene en cuanto
        #     cualquiera termina (sea Ã©xito o fallo), de modo que el
        #     primer fallo aborta el episodio y el recuento queda a 0.
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        self.num_envs = 1
        
        # Permitir seleccionar extractor desde kwargs
        fe_cls = kwargs.pop("features_extractor_class", None)
        if fe_cls is None:
            fe_cls = HATSExtractor if alg in ["SAC", "MaskableSAC"] else FeaturesExtractor
        
        # Determinar si usar observaciones de grafo basado en el extractor
        use_graph_obs = fe_cls == HATSExtractor or (fe_cls is not None and "HATS" in str(fe_cls))
        
        self.env = DummyVecEnv([
            lambda: NetEnv(
                network,
                curriculum_enabled=use_curriculum,
                initial_complexity=1.0,   # 100 % de los flujos en test
                use_graph_observation=use_graph_obs
            )
        ])
        
        policy_kwargs = dict(features_extractor_class=fe_cls)
        
        # Usar el algoritmo especificado
        alg_class = self.SUPPORTING_ALG.get(alg, MaskablePPO)
        self.model = alg_class(
            "MlpPolicy",
            self.env,
            verbose=0,
            policy_kwargs=policy_kwargs,
        )
        self.res = None
        logging.info(f"Scheduler inicializado con {self.num_flows} flujos (curriculum: {use_curriculum}), algoritmo: {alg}")
        
    def load_model(self, filepath: str, alg='MaskablePPO'):
        """Carga un modelo pre-entrenado"""
        if filepath.endswith('.zip'):
            filepath = filepath[:-4]
        if not os.path.isfile(f"{filepath}.zip"):
            logging.error(f"Modelo no encontrado: {filepath}.zip")
            return False
        try:
            # Usar el algoritmo correcto para cargar el modelo
            alg_class = self.SUPPORTING_ALG.get(alg, MaskablePPO)
            self.model = alg_class.load(filepath, self.env)
            self.alg = alg
            logging.info(f"Modelo cargado: {filepath} (algoritmo: {alg})")
            return True
        except Exception as e:
            logging.error(f"Error cargando modelo: {e}")
            return False
        
    def schedule(self, max_episodes: int = 50, patience: int = 5) -> int:
        """
        Intenta como mÃ¡ximo ``max_episodes`` y se detiene si durante
        ``patience`` episodios seguidos no mejora el mejor resultado.

        Devuelve cuÃ¡ntos flujos se lograron programar en el mejor
        episodio encontrado (0 â†’ ninguno).  La soluciÃ³n parcial queda en
        ``self.res`` y puede recuperarse con ``get_res()``.
        """

        self.res = None
        best_res: ScheduleRes | None = None
        best_count = 0
        no_improve = 0

        log = logging.getLogger(__name__)

        for ep in range(1, max_episodes + 1):
            obs = self.env.reset()
            done = False

            while not done:
                action_masks = np.vstack(self.env.env_method('action_masks'))
                action, _ = self.model.predict(
                    obs, deterministic=True,
                    action_masks=action_masks.astype(np.int8)
                )
                obs, _, dones, infos = self.env.step(action)
                done = any(dones)

                if not done:
                    continue

                # ---------- fin de episodio ----------
                scheduled_cnt = 0
                schedule_res = None
                for i, is_done in enumerate(dones):
                    if not is_done:
                        continue
                    schedule_res = infos[i].get('ScheduleRes')
                    if schedule_res:
                        scheduled_cnt = {
                            f.flow_id
                            for link_ops in schedule_res.values()
                            for f, _ in link_ops
                        }.__len__()
                    break

                if scheduled_cnt > best_count:
                    best_count = scheduled_cnt
                    best_res = schedule_res
                    no_improve = 0
                    log.info(f"[scheduler] episodio {ep}: "
                             f"nueva mejor marca {best_count}/{self.num_flows}")
                    if best_count == self.num_flows:          # Â¡perfecto!
                        self.res = best_res
                        return best_count
                else:
                    no_improve += 1

                if no_improve >= patience:
                    log.warning(f"[scheduler] sin mejora en {patience} episodios; "
                                f"mejor = {best_count}/{self.num_flows}.")
                    self.res = best_res
                    return best_count

        # agotÃ³ max_episodes
        logging.getLogger(__name__).warning(
            f"[scheduler] alcanzado lÃ­mite de {max_episodes} episodios; "
            f"mejor = {best_count}/{self.num_flows}")
        self.res = best_res
        return best_count

    def get_res(self) -> ScheduleRes:
        """Retorna el resultado del scheduling"""
        return self.res

class ResAnalyzer:
    """Analiza y guarda resultados del scheduling"""
    
    # Threshold for GCL entry generation - easily modifiable class variable
    DEFAULT_GCL_GAP_THRESHOLD = 30
    
    def __init__(self, network: Network, results: ScheduleRes):
        """
        Inicializa el analizador y guarda resultados
        
        Args:
            network: Red TSN
            results: Resultado del scheduling
        """
        self.network = network
        self.results = results
        self.analyzer_id = id(self) # Generar un ID Ãºnico para el analizador
        
        # Set instance variable from class default
        self.gap_threshold_us = self.DEFAULT_GCL_GAP_THRESHOLD
        
        # --- NUEVO: Calcular y almacenar tablas GCL estÃ¡ticas ---
        self._gcl_tables = self._calculate_gcl_tables(self.gap_threshold_us)
        # --- FIN NUEVO ---

        # Guardar resultados a archivo
        if results:
            # Asegurarse de que el directorio OUT_DIR existe
            os.makedirs(OUT_DIR, exist_ok=True)
            
            # Usar el formato "by_link" consistentemente
            filename = os.path.join(OUT_DIR, f'schedule_res_by_link_{self.analyzer_id}.log')
            try:
                with open(filename, 'w') as f:
                    for link, operations in results.items():
                        f.write(f"Enlace: {link}\n")
                        for flow, op in operations:
                            f.write(f"  Flujo: {flow.flow_id}, Op: {op}\n")
                        f.write("\n")
                logging.info(f"Resultados guardados en {filename}")
            except Exception as e:
                logging.error(f"Error al guardar resultados: {e}")

    # --- NUEVO: MÃ©todo para calcular las tablas GCL --------------------------
    def _calculate_gcl_tables(self, gap_thr_us: int = None) -> Dict[Link, List[Tuple[int, int]]]:
        """
        Genera la tabla GCL (lista de pares Â«tiempo, estadoÂ») para cada
        puerto-switch.

        â–¸ SÃ³lo se insertan 0/1 cuando el hueco entre la recepciÃ³n de un paquete
          y el comienzo del siguiente supera `gap_thr_us` Âµs (default: valor de self.gap_threshold_us).
        â–¸ Se enlaza el Ãºltimo paquete del hiperperÃ­odo con el primero para que
          el cierre final tambiÃ©n quede reflejado.
        â–¸ IMPORTANTE: Para cada par de eventos, se aÃ±ade un 0 (cierre) y un 1 (apertura)
        """
        from math import lcm
        
        # El umbral viene del atributo de instancia (por defecto 30 Âµs,
        # o el que se haya pasado vÃ­a --gcl-threshold).  **NO** lo
        # sobre-escribimos aquÃ­; asÃ­ generamos la "tabla corta".
        gcl_tables: Dict[Link, List[Tuple[int, int]]] = {}
        if not self.results:
            return gcl_tables

        for link, ops in self.results.items():
            # SÃ³lo puertos cuyo ORIGEN es un switch ("Sâ€¦", excluyendo "SRVâ€¦")
            src = link.link_id[0] if isinstance(link.link_id, tuple) else link.link_id.split("-")[0]
            if not (src.startswith("S") and not src.startswith("SRV")):
                continue

            # 1ï¸âƒ£  Ordenar operaciones por inicio real (gating_time o start_time)
            ops_sorted = sorted(ops, key=lambda p: (p[1].gating_time or p[1].start_time))
            n = len(ops_sorted)
            if n == 0:
                continue

            # 2ï¸âƒ£  Calcular hiperperÃ­odo de ese puerto
            gcl_cycle = 1
            for f, _ in ops_sorted:
                gcl_cycle = lcm(gcl_cycle, f.period)

            # 3ï¸âƒ£  Analizar cada operaciÃ³n â€“ reset de listas por-link
            all_transmission_times: list[tuple[int, str]] = []
            all_reception_times:    list[tuple[int, str]] = []
            
            hyperperiod_link = gcl_cycle  # Hiperperiodo para este enlace

            #     Crear un par de eventos 0/1 por paquete
            for i in range(n):
                f_curr, op_curr = ops_sorted[i]
                
                # Ãndice del siguiente paquete (con wraparound)
                next_idx = (i + 1) % n
                f_next, op_next = ops_sorted[next_idx]

                # Tiempo cuando termina de llegar este paquete (necesitamos cerrar el gate)
                close_time = op_curr.reception_time
                
                # Tiempo cuando inicia la transmisiÃ³n del siguiente paquete (reabrimos el gate)
                open_time = op_next.gating_time or op_next.start_time
                
                # Si es el Ãºltimo paquete, aÃ±adir un perÃ­odo para el wraparound
                if i == n - 1:
                    open_time += f_next.period

                # Calcular el gap entre recepciÃ³n y siguiente transmisiÃ³n
                gap = open_time - close_time
                
                # Para cada paquete, repetirlo durante todo el hiperperÃ­odo
                repetitions = hyperperiod_link // f_curr.period
                for rep in range(repetitions):
                    offset = rep * f_curr.period
                    # Guardar tiempo de inicio y recepciÃ³n (normalizado al hiperperiodo)
                    tx_t = (op_curr.start_time + offset) % hyperperiod_link
                    rx_t = (op_curr.reception_time + offset) % hyperperiod_link
                    all_transmission_times.append((tx_t, f_curr.flow_id))
                    all_reception_times.append((rx_t, f_curr.flow_id))

            # Ordenar los tiempos
            all_transmission_times.sort(key=lambda x: x[0])
            all_reception_times.sort(key=lambda x: x[0])
            
            # PASO 2: Generar eventos GCL con la tabla COMPLETA
            gcl_close_events: list[tuple[int,str,int,str]] = []
            
            # Buscar gaps significativos entre recepciÃ³n y siguiente transmisiÃ³n
            for rx_time, rx_flow in all_reception_times:
                
                # âŠ Iniciar variables cada iteraciÃ³n
                next_tx_time: int | None = None
                next_tx_flow: str | None = None

                # Buscar la siguiente transmisiÃ³n > rx_time
                for tx_time, tx_flow in all_transmission_times:
                    if tx_time > rx_time:
                        next_tx_time = tx_time
                        next_tx_flow = tx_flow
                        break

                # Si no hay ninguna (wraparound) usa la primera del ciclo + hiperperÃ­odo
                if next_tx_time is None and all_transmission_times:
                    first_tx_time, first_tx_flow = all_transmission_times[0]
                    next_tx_time = first_tx_time + hyperperiod_link
                    next_tx_flow = first_tx_flow
                
                # ProtecciÃ³n extra â€“ si, aun asÃ­, no existe TX, saltar este RX
                if next_tx_time is None:
                    continue
                 
                # Calcular el gap siempre (tabla completa â€“ sin filtro)
                gap = (next_tx_time - rx_time) % hyperperiod_link

                #  SÃ³lo aÃ±adimos el par 0/1 cuando el hueco supera
                #  el threshold definido por el usuario.
                if gap > gap_thr_us:
                    # AÃ±adir eventos de cierre/apertura
                    gcl_close_events.append(
                        (rx_time, rx_flow, next_tx_time, next_tx_flow)
                    )

            # PASO 3: Generar los pares de eventos 0/1 para cada gap significativo
            events: List[Tuple[int, int]] = []
            for rx_time, rx_flow, next_tx_time, next_tx_flow in gcl_close_events:
                # AÃ±adir evento de cierre (0) en el tiempo de recepciÃ³n
                events.append((rx_time, 0))
                    
                # AÃ±adir evento de apertura (1) cuando empieza el siguiente paquete
                events.append((next_tx_time % hyperperiod_link, 1))

            # 4ï¸âƒ£  Ordenar todos los eventos por tiempo
            events.sort(key=lambda x: (x[0], x[1]))
            
            # 5ï¸âƒ£  Eliminar estados duplicados o redundantes consecutivos
            final_table: List[Tuple[int, int]] = []
            last_state: int | None = None
            for t, s in events:
                if s != last_state:  # Solo aÃ±adir si cambia el estado
                    final_table.append((t, s))
                    last_state = s

            # 6ï¸âƒ£  Garantizar que la tabla empiece "abierta" en t = 0 Âµs
            if not final_table or final_table[0][0] != 0:
                final_table.insert(0, (0, 1))
            elif final_table[0][0] == 0 and final_table[0][1] == 0:
                # Si el primer evento es cerrar en t=0, aÃ±adir apertura en t=0 antes
                final_table.insert(0, (0, 1))

            gcl_tables[link] = final_table

        return gcl_tables

    # --- NUEVO: MÃ©todo para recalcular GCL con threshold diferente ---
    def recalculate_gcl_tables(self, new_threshold_us: int) -> Dict[Link, List[Tuple[int, int]]]:
        """
        Recalcula las tablas GCL con un nuevo threshold.
        
        Args:
            new_threshold_us: Nuevo valor de threshold en Âµs
            
        Returns:
            Diccionario con las nuevas tablas GCL
        """
        # Update instance threshold
        self.gap_threshold_us = new_threshold_us
        
        # Recalculate tables
        self._gcl_tables = self._calculate_gcl_tables(new_threshold_us)
        
        # Log the change
        print(f"GCL tables recalculated with threshold: {new_threshold_us}Âµs")
        
        return self._gcl_tables

    # --- NUEVO: MÃ©todo para imprimir informaciÃ³n de los flujos ---
    def print_flow_info(self):
        """Imprime una tabla con informaciÃ³n detallada de cada flujo, incluyendo tamaÃ±os de paquete."""
        if not self.network or not self.network.flows:
            print("\nNo hay informaciÃ³n de flujos disponible.")
            return
            
        print("\n" + "="*80)
        print("INFORMACIÃ“N DE FLUJOS")
        print("="*80)
        
        # Definir formato de tabla
        format_str = "{:<8} | {:<8} | {:<8} | {:<10} | {:<12} | {:<6}"
        
        # Imprimir cabecera
        print(format_str.format("Flujo", "Origen", "Destino", "PerÃ­odo (Âµs)", "Payload (B)", "Hops"))
        print("-"*8 + " | " + "-"*8 + " | " + "-"*8 + " | " + "-"*10 + " | " + "-"*12 + " | " + "-"*6)
        
        # Imprimir cada flujo
        scheduled_flows = set()
        if self.results:
            for link_ops in self.results.values():
                for flow, _ in link_ops:
                    scheduled_flows.add(flow.flow_id)
        
        for flow in self.network.flows:
            # Verificar si el flujo fue programado exitosamente
            status = "âœ“" if flow.flow_id in scheduled_flows else ""
            
            # Calcular nÃºmero de hops
            num_hops = len(flow.path)
            
            # Imprimir informaciÃ³n
            print(format_str.format(
                flow.flow_id, 
                flow.src_id, 
                flow.dst_id, 
                flow.period, 
                flow.payload,
                f"{num_hops} {status}"
            ))

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  RESUMEN GLOBAL  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        total_sched = len(scheduled_flows)
        total_flows = len(self.network.flows)
        print("-"*80)
        print(f"Programados con Ã©xito: {total_sched}/{total_flows} flujos")
        print("="*80 + "\n")

    # --- NUEVO: MÃ©todo para imprimir las tablas GCL ---
    def print_gcl_tables(self):
        """Print the generated GCL tables for visualization and debugging."""
        if not self._gcl_tables:
            print("\nNo se generaron tablas GCL (posiblemente no hubo gating o scheduling fallÃ³).")
            return

        print("\n" + "="*80)
        print("TABLA GCL GENERADA (t, estado)")
        print("="*80)

        for link, table in self._gcl_tables.items():
            # Re-calcular gcl_cycle aquÃ­ para mostrarlo
            gated_ops = [(f, op) for f, op in self.results.get(link, []) if op.gating_time is not None]
            if not gated_ops: continue
            gcl_cycle = 1
            for f, _ in gated_ops:
                gcl_cycle = math.lcm(gcl_cycle, f.period)

            print(f"\n--- Enlace: {link.link_id} (Ciclo GCL: {gcl_cycle} Âµs) ---")
            print(f"{'Tiempo (Âµs)':<12} | {'Estado':<6}")
            print(f"{'-'*12} | {'-'*6}")
            for time, state in table:
                print(f"{time:<12} | {state:<6}")

        print("="*80 + "\n")

    def calculate_latency_metrics(self):
        """
        Calcula mÃ©tricas de latencia extremo-a-extremo para todos los flujos programados.
        
        Returns:
            dict: Diccionario con mÃ©tricas de latencia (promedio, jitter, mÃ¡xima, muestras)
        """
        import statistics as _stat
        
        print("ğŸ” INICIANDO CÃLCULO DE MÃ‰TRICAS DE LATENCIA...")
        print(f"ğŸ“Š Tenemos {len(self.network.flows)} flujos totales")
        print(f"ğŸ“Š Tenemos {len(self.results)} enlaces con resultados")
        
        latencies = []
        flows_processed = []
        
        # Para cada flujo programado, calcular su latencia E2E
        for flow in self.network.flows:
            flow_id = flow.flow_id
            
            # Buscar todas las operaciones de este flujo
            flow_operations = []
            
            for link, operations in self.results.items():
                for f, op in operations:
                    if f.flow_id == flow_id:
                        flow_operations.append((link, op))
            
            print(f"ğŸ” Flujo {flow_id}: encontradas {len(flow_operations)} operaciones")
            
            # Si el flujo tiene operaciones programadas
            if flow_operations:
                flows_processed.append(flow_id)
                
                if len(flow_operations) == 1:
                    # Flujo de un solo hop
                    _, op = flow_operations[0]
                    latency = op.reception_time - op.start_time
                    latencies.append(latency)
                    print(f"  âœ… {flow_id} (1 hop): {op.start_time} â†’ {op.reception_time} = {latency} Âµs")
                
                else:
                    # Flujo multi-hop: ordenar por start_time para asegurar orden correcto
                    sorted_ops = sorted(flow_operations, key=lambda x: x[1].start_time)
                    first_op = sorted_ops[0][1]   # Primera operaciÃ³n
                    last_op = sorted_ops[-1][1]   # Ãšltima operaciÃ³n
                    
                    latency = last_op.reception_time - first_op.start_time
                    latencies.append(latency)
                    print(f"  âœ… {flow_id} ({len(flow_operations)} hops): {first_op.start_time} â†’ {last_op.reception_time} = {latency} Âµs")
            else:
                print(f"  âŒ {flow_id}: sin operaciones programadas")
        
        print(f"\nğŸ“ˆ RESUMEN: {len(flows_processed)} flujos procesados de {len(self.network.flows)} totales")
        print(f"ğŸ“Š Flujos con latencias calculadas: {flows_processed}")
        
        if not latencies:
            print("âš ï¸  NO SE ENCONTRARON LATENCIAS PARA CALCULAR")
            print("ğŸ”¥ FORZANDO RETORNO DE MÃ‰TRICAS VACÃAS")
            return {
                "average": 0,
                "jitter": 0,
                "maximum": 0,
                "minimum": 0,
                "samples": []
            }
        
        # Calcular estadÃ­sticas
        avg_lat = sum(latencies) / len(latencies)
        max_lat = max(latencies)
        min_lat = min(latencies)
        jitter = _stat.pstdev(latencies) if len(latencies) > 1 else 0
        
        # FORZAR SALIDA MÃšLTIPLE
        print("\n" + "="*80)
        print("ğŸ¯ MÃ‰TRICAS DE LATENCIA EXTREMO-A-EXTREMO")
        print("="*80)
        print(f"ğŸ“Š Promedio: {avg_lat:.1f} Âµs")
        print(f"ğŸ“Š Jitter:   {jitter:.1f} Âµs") 
        print(f"ğŸ“Š MÃ¡xima:   {max_lat} Âµs")
        print(f"ğŸ“Š MÃ­nima:   {min_lat} Âµs")
        print(f"ğŸ“Š Muestras: {len(latencies)} flujos")
        print(f"ğŸ“Š Valores:  {latencies}")
        print("="*80)
        
        # TambiÃ©n usar logging
        logging.info("ğŸ¯ MÃ‰TRICAS DE LATENCIA EXTREMO-A-EXTREMO")
        logging.info(f"ğŸ“Š Promedio: {avg_lat:.1f} Âµs | Jitter: {jitter:.1f} Âµs | MÃ¡xima: {max_lat} Âµs | MÃ­nima: {min_lat} Âµs | Muestras: {len(latencies)}")
        
        return {
            "average": avg_lat,
            "jitter": jitter,
            "maximum": max_lat,
            "minimum": min_lat,
            "samples": latencies.copy()
        }
    
    def calculate_link_utilization(self):
        """
        Calcula la utilizaciÃ³n de cada enlace como porcentaje del tiempo ocupado
        durante el hiperperÃ­odo.
        
        
        Returns:
            dict: Diccionario con utilizaciÃ³n por enlace y estadÃ­sticas globales
        """
        import math
        
        print("ğŸ”— INICIANDO CÃLCULO DE UTILIZACIÃ“N DE ENLACES...")
        
        if not self.results:
            print("âš ï¸  No hay resultados de scheduling para analizar")
            return {"link_utilizations": {}, "global_stats": {}}
        
        link_utilizations = {}
        all_utilizations = []
        
        # Calcular hiperperÃ­odo global
        all_periods = set()
        for link, operations in self.results.items():
            for flow, _ in operations:
                all_periods.add(flow.period)
        
        hyperperiod = 1
        for period in all_periods:
            hyperperiod = math.lcm(hyperperiod, period)
        
        print(f"ğŸ“Š HiperperÃ­odo global: {hyperperiod} Âµs")
        
        # Calcular utilizaciÃ³n para cada enlace
        for link, operations in self.results.items():
            if not operations:
                continue
                
            link_id = link.link_id if hasattr(link, 'link_id') else str(link)
            print(f"\nğŸ” Analizando enlace: {link_id}")
            
            # Tiempo total ocupado en el hiperperÃ­odo
            total_busy_time = 0
            transmission_events = []
            
            # Para cada operaciÃ³n, calcular todas sus repeticiones en el hiperperÃ­odo
            for flow, operation in operations:
                # Tiempo de transmisiÃ³n por paquete
                transmission_time = operation.end_time - (operation.gating_time or operation.start_time)
                
                # NÃºmero de repeticiones en el hiperperÃ­odo
                repetitions = hyperperiod // flow.period
                
                # Tiempo total de todas las repeticiones
                flow_total_time = transmission_time * repetitions
                total_busy_time += flow_total_time
                
                print(f"  â¤ Flujo {flow.flow_id}: {transmission_time}Âµs Ã— {repetitions} repeticiones = {flow_total_time}Âµs")
                
                # Guardar eventos para verificaciÃ³n (opcional)
                for rep in range(repetitions):
                    offset = rep * flow.period
                    start_tx = (operation.gating_time or operation.start_time) + offset
                    end_tx = operation.end_time + offset
                    transmission_events.append((start_tx, end_tx, flow.flow_id))
            
            # Calcular utilizaciÃ³n como porcentaje
            utilization_percent = (total_busy_time / hyperperiod) * 100
            
            link_utilizations[str(link_id)] = {
                "utilization_percent": utilization_percent,
                "busy_time_us": total_busy_time,
                "hyperperiod_us": hyperperiod,
                "num_flows": len(operations),
                "transmission_events": len(transmission_events)
            }
            
            all_utilizations.append(utilization_percent)
            
            print(f"  ğŸ“ˆ UtilizaciÃ³n: {utilization_percent:.2f}% ({total_busy_time}/{hyperperiod} Âµs)")
        
        # EstadÃ­sticas globales
        if all_utilizations:
            global_stats = {
                "average_utilization": sum(all_utilizations) / len(all_utilizations),
                "max_utilization": max(all_utilizations),
                "min_utilization": min(all_utilizations),
                "total_links": len(all_utilizations),
                "hyperperiod_us": hyperperiod
            }
        else:
            global_stats = {
                "average_utilization": 0,
                "max_utilization": 0,
                "min_utilization": 0,
                "total_links": 0,
                "hyperperiod_us": hyperperiod
            }
        
        # Mostrar resumen
        print("\n" + "="*80)
        print("ğŸ”— UTILIZACIÃ“N DE ENLACES")
        print("="*80)
        
        # Tabla detallada por enlace
        print(f"{'Enlace':<25} | {'UtilizaciÃ³n':<12} | {'Tiempo Ocupado':<15} | {'Flujos':<6}")
        print("-"*25 + " | " + "-"*12 + " | " + "-"*15 + " | " + "-"*6)
        
        for link_str, stats in link_utilizations.items():
            print(f"{link_str:<25} | {stats['utilization_percent']:>10.2f}% | "
                  f"{stats['busy_time_us']:>13} Âµs | {stats['num_flows']:>4}")
        
        print("-"*80)
        print(f"ğŸ“Š ESTADÃSTICAS GLOBALES:")
        print(f"   â€¢ UtilizaciÃ³n promedio: {global_stats['average_utilization']:.2f}%")
        print(f"   â€¢ UtilizaciÃ³n mÃ¡xima:   {global_stats['max_utilization']:.2f}%")
        print(f"   â€¢ UtilizaciÃ³n mÃ­nima:   {global_stats['min_utilization']:.2f}%")
        print(f"   â€¢ Enlaces analizados:   {global_stats['total_links']}")
        print(f"   â€¢ HiperperÃ­odo:         {global_stats['hyperperiod_us']} Âµs")
        print("="*80 + "\n")
        
        # Log tambiÃ©n las mÃ©tricas
        logging.info(
            f"ğŸ”— UtilizaciÃ³n de Enlaces â†’ "
            f"Promedio: {global_stats['average_utilization']:.2f}% | "
            f"MÃ¡xima: {global_stats['max_utilization']:.2f}% | "
            f"MÃ­nima: {global_stats['min_utilization']:.2f}% | "
            f"Enlaces: {global_stats['total_links']}"
        )
        
        return {
            "link_utilizations": link_utilizations,
            "global_stats": global_stats
        }



## ARCHIVO: path_setup.py
## ==================================================

import os
import sys
import logging

# Obtener la ruta del directorio actual (donde estÃ¡ este script)
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))

# AÃ±adir al sys.path para permitir importaciones relativas
if PROJECT_ROOT not in sys.path:
    # Insertar al inicio para asegurar que tiene prioridad sobre otras instalaciones
    sys.path.insert(0, PROJECT_ROOT)
    logging.getLogger(__name__).debug(f"Added {PROJECT_ROOT} to PYTHONPATH")

# DiagnÃ³stico: mostrar el path de Python completo






## ARCHIVO: project_analyzer.py
## ==================================================

#!/usr/bin/env python3

import os
import sys

# Archivos y directorios a excluir
EXCLUDED_FILES = {
  
}

EXCLUDED_DIRS = {
    '__pycache__', 
    '.git', 
    '.idea', 
    'venv', 
    'env', 
    'build', 
    'dist'
}


def collect_python_files(root_path):
    """Recopila todas las rutas de archivos Python en el proyecto"""
    py_files = []
    for root, dirs, files in os.walk(root_path):
        # Evitar directorios excluidos
        dirs[:] = [d for d in dirs if d not in EXCLUDED_DIRS]
        
        # Procesar archivos Python
        for file in files:
            if file.endswith('.py') and file not in EXCLUDED_FILES:
                full_path = os.path.join(root, file)
                rel_path = os.path.relpath(full_path, root_path)
                py_files.append((rel_path, full_path))
    
    # Ordenar los archivos por ruta relativa para mejor lectura
    return sorted(py_files)

def save_code_to_file(py_files, output_file='project_code.txt'):
    """Guarda el contenido de todos los archivos en un Ãºnico archivo de texto"""
    with open(output_file, 'w', encoding='utf-8') as f:
        
        f.write("# ===================================\n\n")
        
        for rel_path, full_path in py_files:
            f.write(f"## ARCHIVO: {rel_path}\n")
            f.write("## " + "=" * 50 + "\n\n")
            
            try:
                with open(full_path, 'r', encoding='utf-8') as src_file:
                    content = src_file.read()
                    f.write(content)
                    
                # AÃ±adir lÃ­neas en blanco para mejor separaciÃ³n
                f.write("\n\n\n")
            except Exception as e:
                f.write(f"# ERROR: No se pudo leer el archivo: {e}\n\n")
    
    print(f"CÃ³digo recopilado y guardado en: {output_file}")
    return output_file

def main():
    # Obtener la ruta del proyecto (directorio actual)
    project_path = os.path.dirname(os.path.abspath(__file__))
    output_file = os.path.join(project_path, "project_code.txt")
    
    print(f"Recopilando cÃ³digo fuente del proyecto en: {project_path}")
    py_files = collect_python_files(project_path)
    print(f"Se encontraron {len(py_files)} archivos Python")
    
    saved_file = save_code_to_file(py_files, output_file)
    print(f"Proceso completado. El cÃ³digo se ha guardado en: {saved_file}")

if __name__ == "__main__":
    main()



## ARCHIVO: setup.py
## ==================================================

from setuptools import setup, find_packages
from pathlib import Path


setup(
    name="gatingdrl",
    version="0.1.0",
    description="Time Sensitive Networks Scheduling with Deep Reinforcement Learning",
    long_description=(Path(__file__).with_name("README.md").read_text(encoding="utf-8")),
    long_description_content_type="text/markdown",
    url="https://github.com/juanpussa/gatingdrl",
    author="Juan Paz",
    license="MIT",
    packages=find_packages(),
    install_requires=[
        "torch>=2.0.0",
        "torch-geometric>=2.3.0",
        "torch-scatter>=2.1.0",
        "torch-sparse>=0.6.0", 
        "torch-cluster>=1.6.0",
        "torch-spline-conv>=1.2.0",
        "networkx>=3.0",
        "matplotlib",
        "pandas",
        "numpy",
        "gymnasium>=0.28.1",
        "sb3-contrib>=2.0.0",
        "stable-baselines3>=2.0.0",
        "plotly>=5.14.0",
        "seaborn>=0.12.0",            #  usada en tools/analyze_training.py
    ],
    classifiers=[
        "Development Status :: 3 - Alpha",
        "Intended Audience :: Science/Research",
        "License :: OSI Approved :: MIT License",
        "Programming Language :: Python :: 3 :: Only",
        "Programming Language :: Python :: 3.10",
        "Topic :: Scientific/Engineering :: Artificial Intelligence",
    ],
    python_requires=">=3.10",
)





## ARCHIVO: tools/__init__.py
## ==================================================




## ARCHIVO: tools/analyze_training.py
## ==================================================

#!/usr/bin/env python3

import os

import sys
import json
import argparse
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns


# AÃ±adir el directorio raÃ­z al path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from tools.definitions import OUT_DIR

def load_metrics(json_path=None):
    """Carga mÃ©tricas de un archivo .json de entrenamiento"""
    if json_path is None:
        # Buscar el Ãºltimo archivo metrics.json
        metrics_files = [f for f in os.listdir(OUT_DIR) if f.startswith('training_metrics_') and f.endswith('.json')]
        if not metrics_files:
            print("No se encontraron archivos de mÃ©tricas de entrenamiento")
            return None
        
        # Ordenar por fecha de modificaciÃ³n y usar el mÃ¡s reciente
        metrics_files.sort(key=lambda x: os.path.getmtime(os.path.join(OUT_DIR, x)), reverse=True)
        json_path = os.path.join(OUT_DIR, metrics_files[0])
    
    try:
        with open(json_path, 'r') as f:
            data = json.load(f)
        return data
    except Exception as e:
        print(f"Error cargando mÃ©tricas: {e}")
        return None

def analyze_rewards(metrics, save_dir=None):
    """Analiza y visualiza la evoluciÃ³n de recompensas"""
    if not metrics or 'rewards' not in metrics:
        print("No hay datos de recompensas disponibles")
        return
    
    rewards = metrics['rewards']
    if not rewards:
        return
    
    plt.figure(figsize=(12, 6))
    
    # GrÃ¡fico de recompensa por episodio
    plt.plot(rewards, 'b-', alpha=0.6)
    
    # AÃ±adir suavizado (media mÃ³vil)
    window = min(10, len(rewards) // 5 + 1)
    if window > 1:
        smooth_rewards = np.convolve(rewards, np.ones(window)/window, mode='valid')
        valid_idx = np.arange(len(smooth_rewards)) + window - 1
        plt.plot(valid_idx, smooth_rewards, 'r-', linewidth=2, label=f'Media mÃ³vil ({window} eps)')
    
    # EstadÃ­sticas de recompensas
    plt.axhline(y=np.mean(rewards), color='g', linestyle='--', label=f'Media: {np.mean(rewards):.2f}')
    
    # Set Y-axis to start from 0
    max_reward = max(rewards)
    plt.ylim(0, max_reward * 1.05)  # Start from 0, add 5% padding at top
    
    # Tramos de anÃ¡lisis
    n = len(rewards)
    if n >= 30:  # Solo si hay suficientes episodios
        first_third = np.mean(rewards[:n//3])
        mid_third = np.mean(rewards[n//3:2*n//3])
        last_third = np.mean(rewards[2*n//3:])
        
        plt.axhline(y=first_third, color='c', linestyle=':', alpha=0.7, 
                   label=f'Primer tercio: {first_third:.2f}')
        plt.axhline(y=last_third, color='m', linestyle=':', alpha=0.7,
                   label=f'Ãšltimo tercio: {last_third:.2f}')
        
        # Indicar tendencia
        if last_third > first_third * 1.1:
            trend = "â†—ï¸ Mejora"
        elif last_third < first_third * 0.9:
            trend = "â†˜ï¸ Deterioro"
        else:
            trend = "â†’ Estable"
            
        plt.text(0.02, 0.02, f"Tendencia: {trend}", transform=plt.gca().transAxes, 
                bbox=dict(facecolor='white', alpha=0.8))
    
    plt.title('EvoluciÃ³n de Recompensas por Episodio', fontsize=14)
    plt.xlabel('Episodio')
    plt.ylabel('Recompensa')
    plt.grid(True, alpha=0.3)
    plt.legend()
    
    if save_dir:
        os.makedirs(save_dir, exist_ok=True)
        plt.savefig(os.path.join(save_dir, 'rewards_analysis.png'))
    
    plt.show()

def analyze_action_distribution(metrics, save_dir=None):
    """Analiza la distribuciÃ³n de decisiones del agente"""
    if not metrics or 'action_distributions' not in metrics:
        print("No hay datos de distribuciones de acciones disponibles")
        return
    
    action_dist = metrics.get('action_distributions', {})
    if not action_dist:
        return
    
    # Crear un DataFrame para anÃ¡lisis
    action_data = {}
    for action_name, dist in action_dist.items():
        if isinstance(dist, dict):  # Asegurar que es un diccionario
            action_data[action_name] = dist
    
    if not action_data:
        return
        
    # Analizar cada dimensiÃ³n de acciÃ³n
    plt.figure(figsize=(15, 10))
    
    n_actions = len(action_data)
    n_cols = 2
    n_rows = (n_actions + n_cols - 1) // n_cols
    
    action_names = {
        "offset": "Offset temporal (Âµs)",
        "gcl_strategy": "Estrategia GCL",
        "guard_factor": "Guard factor",
        "priority": "Prioridad",
        "switch_gap": "Gap mÃ­nimo",
        "jitter": "Control jitter"
    }
    
    for i, (action, dist) in enumerate(action_data.items()):
        plt.subplot(n_rows, n_cols, i+1)
        
        # Extraer valores y frecuencias
        values = sorted(map(int, dist.keys()))
        counts = [dist.get(str(v), 0) for v in values]
        
        # Calcular entropÃ­a normalizada para medir aleatoriedad
        total = sum(counts)
        if total > 0:
            probs = [count/total for count in counts]
            entropy = -sum(p * np.log2(p) for p in probs if p > 0)
            max_entropy = np.log2(len(values)) if len(values) > 0 else 0
            norm_entropy = entropy / max_entropy if max_entropy > 0 else 0
            entropy_str = f"EntropÃ­a: {norm_entropy:.2f}"
            
            # AnÃ¡lisis de distribuciÃ³n
            if norm_entropy > 0.95:
                analysis = "Muy uniforme - posible indecisiÃ³n"
            elif norm_entropy > 0.85:
                analysis = "Bastante uniforme - poca preferencia"
            elif norm_entropy < 0.3:
                analysis = "Muy concentrada - fuerte preferencia"
            else:
                analysis = "DistribuciÃ³n normal"
        else:
            entropy_str = ""
            analysis = "Datos insuficientes"
        
        # Crear grÃ¡fico
        plt.bar(values, counts)
        plt.title(f"{action_names.get(action, action)}\n{entropy_str}\n{analysis}")
        plt.xlabel("Valor")
        plt.ylabel("Frecuencia")
        plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if save_dir:
        os.makedirs(save_dir, exist_ok=True)
        plt.savefig(os.path.join(save_dir, 'action_distribution.png'))
    
    plt.show()

def analyze_correlation(metrics, save_dir=None):
    """Analiza correlaciones entre decisiones y resultados"""
    # AquÃ­ se implementarÃ­a el anÃ¡lisis de correlaciÃ³n entre dimensiones
    # y entre decisiones y rendimiento (recompensas)
    pass

def main():
    parser = argparse.ArgumentParser(description='Analiza mÃ©tricas de entrenamiento DRL')
    parser.add_argument('--file', type=str, help='Ruta al archivo JSON de mÃ©tricas (opcional)')
    parser.add_argument('--save-dir', type=str, help='Directorio donde guardar los grÃ¡ficos (opcional)')
    parser.add_argument('--no-plots', action='store_true', help='No mostrar grÃ¡ficos, solo guardarlos')
    
    args = parser.parse_args()
    
    # Cargar mÃ©tricas
    metrics = load_metrics(args.file)
    if not metrics:
        print("No se pudieron cargar las mÃ©tricas")
        return
    
    # Configurar backend de matplotlib
    if args.no_plots:
        plt.switch_backend('Agg')  # No mostrar grÃ¡ficos
    
    # Realizar anÃ¡lisis
    print("Analizando recompensas...")
    analyze_rewards(metrics, args.save_dir)
    
    print("Analizando distribuciones de decisiones...")
    analyze_action_distribution(metrics, args.save_dir)
    
    print("AnÃ¡lisis completado")

if __name__ == "__main__":
    main()



## ARCHIVO: tools/definitions.py
## ==================================================

import os

ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))  # newtas root directory

OUT_DIR = os.path.join(ROOT_DIR, 'out')
# Create the output directory if it doesn't exist
os.makedirs(OUT_DIR, exist_ok=True)

CONFIG_DIR = os.path.join(ROOT_DIR, 'config')

LOG_DIR = os.path.join(OUT_DIR, 'log')
os.makedirs(LOG_DIR, exist_ok=True)






## ARCHIVO: tools/execute.py
## ==================================================

import argparse
import inspect
from typing import Any, Callable, List, Type, get_origin, get_args


def custom_list_type(item_type: Type) -> Callable[[str], List[Any]]:
    """
    Generates a function to convert a comma-separated string into a list of a specific type.

    Parameters
    ----------
    item_type : Type
        The type of the items in the resulting list.

    Returns
    -------
    Callable[[str], List[Any]]
        A function that converts a string to a list of the specified item type.
    """
    def convert(s: str) -> List[Any]:
        return [item_type(item) for item in s.split(',')]
    return convert




def execute_from_command_line(func: Callable):
    """
    Executes a given function using arguments provided from the command line.

    This function uses the inspect module to determine the required and optional
    arguments of the `func` function and their annotations. It then creates an
    ArgumentParser object and adds the function's arguments to it.

    The command-line arguments are expected to be provided in the format
    `--arg_name arg_value`. The function arguments can be either required or
    optional. If an optional argument is not provided, its default value from
    the function definition is used.

    After parsing the command-line arguments, this function calls `func` with
    the parsed arguments.

    Parameters
    ----------
    func : Callable
        The function to be executed. This function can have any number of
        required or optional arguments.

    Raises
    ------
    argparse.ArgumentError
        If a required argument is not provided in the command line.
    """
    # Get the signature of the function
    sig = inspect.signature(func)

    # Create the argument parser
    parser = argparse.ArgumentParser(description=func.__doc__)

    # Add arguments to the parser
    for name, param in sig.parameters.items():
        # Determine the type of the argument
        if param.annotation is not param.empty:
            arg_type = param.annotation
            # Check if the annotation is a generic type
            origin_type = get_origin(arg_type)
            if origin_type is list:
                # Get the inner type of the list
                inner_type = get_args(arg_type)[0]
                arg_type = custom_list_type(inner_type)
        else:
            arg_type = str

        if param.default is param.empty:  # it's a required argument
            parser.add_argument('--' + name, required=True, type=arg_type)
        else:  # it's an optional argument, use default value from function definition
            parser.add_argument('--' + name, default=param.default, type=arg_type)

    args = parser.parse_args()

    # Convert args to a dictionary
    args_dict = vars(args)

    # Call the function with the arguments
    return func(**args_dict)



## ARCHIVO: tools/log_config.py
## ==================================================

import logging


# 1. âœ… Configura el logger de matplotlib.font_manager ANTES de importar matplotlib
logging.getLogger('matplotlib.font_manager').setLevel(logging.WARNING)
logging.getLogger('matplotlib.pyplot').setLevel(logging.WARNING)
logging.getLogger('matplotlib').setLevel(logging.WARNING)

# 2. âœ… Configura la fuente por defecto
import matplotlib.pyplot as plt
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['font.sans-serif'] = ['DejaVu Sans']  # AsegÃºrate de que esta fuente estÃ© instalada
# Eliminar el parÃ¡metro invÃ¡lido font.cache_size
# Verificar si el parÃ¡metro get_no_warn es vÃ¡lido
try:
    plt.rcParams['font.get_no_warn'] = True
except KeyError:
    # Ignorar si este parÃ¡metro tampoco es vÃ¡lido
    pass


# 3. FunciÃ³n para configurar el logging general
def log_config(filename, level=logging.DEBUG):
    # ConfiguraciÃ³n bÃ¡sica del logger raÃ­z
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(filename, mode='w'),
            logging.StreamHandler()
        ]
    )
    # 4. ğŸš¨ Evita que matplotlib.font_manager herede el nivel DEBUG
    logging.getLogger('matplotlib').setLevel(logging.WARNING)
    logging.getLogger('matplotlib.font_manager').setLevel(logging.WARNING)
    logging.getLogger('matplotlib.pyplot').setLevel(logging.WARNING)
    logging.getLogger('PIL').setLevel(logging.WARNING)
    


## ARCHIVO: tools/schedule_report.py
## ==================================================






## ARCHIVO: ui/__init__.py
## ==================================================

"""
Interfaces de usuario y visualizaciÃ³n
"""




## ARCHIVO: ui/plot_training.py
## ==================================================

import matplotlib.pyplot as plt
import os.path
import sys

# AÃ±adir el directorio raÃ­z al path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from stable_baselines3.common.results_plotter import plot_results
from tools.execute import execute_from_command_line
from tools.definitions import OUT_DIR



def plot_training_rewards(dirname: str):
    plot_results([dirname], None, 'timesteps', next((s for s in dirname.split(r'/') if '100' in s), None))
    
    # Set Y-axis to start from 0
    ax = plt.gca()
    ylim = ax.get_ylim()
    if ylim[1] > 0:  # Only adjust if we have positive values
        ax.set_ylim(0, ylim[1] * 1.05)  # Start from 0, add 5% padding at top
    
    filename = os.path.join(os.path.dirname(dirname), f"training_reward.png")
    print(f"saving the figure to {filename}")
    plt.savefig(filename)
    plt.show(block=False)
    plt.pause(3)  # Espera 3 segundos para mostrar la grÃ¡fica
    plt.close()


if __name__ == '__main__': 
    execute_from_command_line(plot_training_rewards)




## ARCHIVO: ui/show_schedule.py
## ==================================================

import argparse
import os
import glob
import sys

# AÃ±adir el directorio raÃ­z al path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from tools.definitions import OUT_DIR





def display_schedule_log(log_file=None, by_link=True):
    """
    Display the contents of a scheduling log file.
    
    Args:
        log_file: Path to the log file. If None, displays the most recent log file.
        by_link: If True, show logs organized by link, otherwise by flow.
    """
    if log_file is None:
        # Find the most recent schedule_res log file
        pattern = 'schedule_res_by_link_*.log' if by_link else 'schedule_res_*.log'
        log_files = glob.glob(os.path.join(OUT_DIR, pattern))
        if not log_files:
            # Try the other format if no logs found
            alt_pattern = 'schedule_res_*.log' if by_link else 'schedule_res_by_link_*.log'
            log_files = glob.glob(os.path.join(OUT_DIR, alt_pattern))
            if not log_files:
                print("No scheduling log files found in", OUT_DIR)
                return
        log_file = max(log_files, key=os.path.getmtime)
    
    try:
        with open(log_file, 'r') as f:
            content = f.read()
            
        print("\n" + "="*80)
        print(f"SCHEDULING DETAILS FROM: {os.path.basename(log_file)}")
        print("="*80)
        print(content)
        print("="*80 + "\n")
    except FileNotFoundError:
        print(f"Error: Log file not found: {log_file}")
    except Exception as e:
        print(f"Error reading log file: {e}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Display scheduling log file contents")
    parser.add_argument('--file', type=str, help='Path to specific log file (optional)')
    parser.add_argument('--by-flow', action='store_true', help='Show logs organized by flow instead of by link')
    
    args = parser.parse_args()
    display_schedule_log(args.file, not args.by_flow)



## ARCHIVO: ui/test.py
## ==================================================

import logging
import argparse
import os
import sys
import random                       # payloads
import multiprocessing              # nÃºcleos
import matplotlib.pyplot as plt     # ğŸ”¹ para la grÃ¡fica
import numpy as np                  # ğŸ”¹
import math                         # ğŸ”¹
from core.network.net import Net    # ğŸ”¹ muestreador oficial

# Configure Qt to use offscreen rendering by default
os.environ["QT_QPA_PLATFORM"] = "offscreen"



# AÃ±adir el directorio raÃ­z al path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from tools.execute import execute_from_command_line
from core.network.net import generate_flows, generate_graph, Network
from core.scheduler.scheduler import DrlScheduler, ResAnalyzer
from ui.tsn_visualizer_plotly import visualize_tsn_schedule_plotly  # Nuevo visualizador con Plotly
from core.omnet_export import export_omnet_files                    # â† NUEVO

DEFAULT_MIN_PAYLOAD = 64   # Valor por defecto mÃ­nimo razonable
DEFAULT_MAX_PAYLOAD = 1518 # Valor por defecto mÃ¡ximo MTU
DEFAULT_MAX_JITTER  = 0    # â† NUEVO: por defecto sin jitter

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
#  HELPERS â€“ gaps reales y grÃ¡fica de verificaciÃ³n                   #
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _is_es(node_name: str) -> bool:
    """HeurÃ­stica rÃ¡pida: un nombre que empiece por E, C o SRV es End-Station."""
    return node_name.startswith(("E", "C", "SRV"))


def _extract_packet_gaps(schedule_res) -> list[int]:
    """
    Devuelve los *gaps* entre instantes de creaciÃ³n (**start_time**) de los
    paquetes cuyo primer hop sale de una End-Station.
    """
    starts = []
    for link, ops in schedule_res.items():
        src = link.link_id[0] if isinstance(link.link_id, tuple) else link.link_id.split("-")[0]
        if not _is_es(src):
            continue
        for _flow, op in ops:
            starts.append(op.start_time)          # primer hop â‡’ start_time

    starts.sort()
    return [s2 - s1 for s1, s2 in zip(starts, starts[1:])]

def _pdf_theoretical(dist: str, params: list[float], xs: np.ndarray) -> np.ndarray:
    if dist == "uniform":
        lo, hi = params
        return np.where((xs >= lo) & (xs <= hi), 1 / (hi - lo), 0)
    if dist == "exponential":
        Î¼, = params
        return (1 / Î¼) * np.exp(-xs / Î¼)
    if dist == "gaussian":
        Î¼, Ïƒ = params
        return np.where(
            xs >= 0,
            (1 / (Ïƒ * math.sqrt(2 * math.pi))) * np.exp(-(xs - Î¼) ** 2 / (2 * Ïƒ ** 2)),
            0,
        )
    if dist == "pareto":
        Î±, xm = params
        return np.where(xs >= xm, Î± * xm ** Î± / xs ** (Î± + 1), 0)
    if dist == "fixed":
        pdf = np.zeros_like(xs)
        pdf[np.abs(xs - params[0]) < 0.5] = 1.0
        return pdf
    return np.zeros_like(xs)


def _plot_gap_distribution(
    dist: str,
    params: list[float],
    out_dir: str = ".",
) -> None:
    """Histograma de las muestras crudas segÃºn Net.sample_packet_gap()."""

    import matplotlib.pyplot as plt
    import numpy as np

    fig, ax = plt.subplots(figsize=(6, 4))
    raw = [Net.sample_packet_gap() for _ in range(max(500, len(params) * 100))]
    ax.hist(raw, bins=50, density=True, alpha=0.7, label="Muestras")

    ax.set_xlabel("Gap (Âµs)")
    ax.set_ylabel("Densidad")
    # Formatear los parÃ¡metros con nombre segÃºn la distribuciÃ³n
    if dist == "fixed":
        param_str = f"gap={params[0]}"
    elif dist == "uniform":
        param_str = f"min={params[0]}, max={params[1]}"
    elif dist == "exponential":
        param_str = f"mean={params[0]}"
    elif dist == "gaussian":
        param_str = f"mu={params[0]}, sigma={params[1]}"
    elif dist == "pareto":
        param_str = f"alpha={params[0]}, xm={params[1]}"
    else:
        param_str = ", ".join(map(str, params))
    ax.set_title(f"FDP fuente('{dist}', {param_str})")
    ax.legend()

    os.makedirs(out_dir, exist_ok=True)
    out_path = os.path.join(out_dir, f"gap_dist_{dist}.png")
    fig.tight_layout()
    fig.savefig(out_path)
    plt.close(fig)
    logging.info(f"[test] GrÃ¡fico de distribuciÃ³n guardado en â†’ {out_path}")


# --------------------------------------------------------------------------- #
#  Helper â€“ grÃ¡fico de verificaciÃ³n de la distribuciÃ³n seleccionada           #
# --------------------------------------------------------------------------- #
# (la versiÃ³n previa basada en Net.sample_packet_gap se ha retirado)

def get_best_model_file(topo, alg='MaskablePPO'):
    """Retorna la ruta completa al archivo del mejor modelo para la topologÃ­a y algoritmo dados"""
    from tools.definitions import OUT_DIR
    return os.path.join(OUT_DIR, f"best_model_{topo}_{alg}", "best_model.zip")

def test(topo: str, num_flows: int, num_envs: int = 0,
         best_model_path: str = None, alg: str = 'MaskablePPO', link_rate: int = 100,
         min_payload: int = DEFAULT_MIN_PAYLOAD, max_payload: int = DEFAULT_MAX_PAYLOAD,
         visualize: bool = True, show_log: bool = True,
         gcl_threshold: int = 30, plot_gap_dist: bool = True):
    
    # Para la prueba / inferencia forzamos **un solo entorno**
    num_envs = 1
    logging.info("Usando 1 entorno (modo inferencia)")
    
    # Configurar logging: INFO para consola, DEBUG para archivo
    from tools.log_config import log_config
    from tools.definitions import OUT_DIR
    log_config(os.path.join(OUT_DIR, f'test_{topo}_{num_flows}.log'), level=logging.INFO)

    # Siempre usar la ruta predeterminada para la topologÃ­a y algoritmo
    if best_model_path is None:
        best_model_path = get_best_model_file(topo, alg)
        logging.info(f"Usando modelo predeterminado: {best_model_path}")
    
    # Verificar si el archivo existe
    if not os.path.exists(best_model_path):
        logging.error(f"Error: Modelo no encontrado: {best_model_path}")
        return False

    graph = generate_graph(topo, link_rate)

    # Generar flujos usando el rango de payload especificado
    flows = generate_flows(                       # â† SIN jitter
        graph, num_flows,
        unidirectional=topo.startswith("UNIDIR"),
        min_payload=min_payload,
        max_payload=max_payload
    )
    
    # Debug info: mostrar nÃºmero de flujos generados
    logging.info(f"Generados {len(flows)} flujos para topologÃ­a {topo} (solicitados: {num_flows})")
    
    # Create network with ALL flows - no curriculum learning in test mode
    network = Network(graph, flows)
    
    # Always use DrlScheduler with explicitly disabled curriculum learning
    scheduler = DrlScheduler(network, num_envs=num_envs, use_curriculum=False)
    
    if best_model_path:
        scheduler.load_model(best_model_path, alg)
    
    scheduler.schedule()                      # ejecuta con early-stop
    schedule_res = scheduler.get_res()

    scheduled_cnt = 0
    if schedule_res:
        scheduled_cnt = {
            f.flow_id
            for link_ops in schedule_res.values()
            for f, _ in link_ops
        }.__len__()

    logging.info(f"Flujos programados: {scheduled_cnt} de {num_flows} solicitados")
    is_scheduled = (scheduled_cnt == num_flows)
    
    # a partir de aquÃ­ usa `schedule_res` (si existe) independientemente
    if schedule_res:
        # Analizar y guardar logs detallados del scheduling
        analyzer = ResAnalyzer(network, schedule_res)
        
        # Apply custom GCL threshold if provided
        if gcl_threshold != 30:  # If different from default
            analyzer.gap_threshold_us = gcl_threshold
            analyzer.recalculate_gcl_tables(gcl_threshold)
        
        log_file = f'schedule_res_by_link_{analyzer.analyzer_id}.log'  # Usar el ID almacenado en el analizador
        log_path = os.path.join(OUT_DIR, log_file)
        logging.info(f"Schedule details saved to {log_path}")

        # Imprimir informaciÃ³n de flujos y tablas GCL estÃ¡ticas
        analyzer.print_flow_info()  # Mostrar tabla de flujos independientemente
        
        # Usar el mÃ©todo actualizado que solo muestra la tabla GCL generada
        analyzer.print_gcl_tables()

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        #  ğŸ“Š  FORZAR CÃLCULO Y MOSTRAR MÃ‰TRICAS DE LATENCIA E2E
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        print("\nğŸ”¥ FORZANDO CÃLCULO DE MÃ‰TRICAS DE LATENCIA...")
        try:
            latency_metrics = analyzer.calculate_latency_metrics()
            print(f"âœ… MÃ©tricas calculadas exitosamente: {latency_metrics}")
        except Exception as e:
            print(f"âŒ ERROR calculando mÃ©tricas de latencia: {e}")
            import traceback
            traceback.print_exc()

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        #  ğŸ”—  NUEVO: CALCULAR Y MOSTRAR UTILIZACIÃ“N DE ENLACES
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        print("\nğŸ”¥ CALCULANDO UTILIZACIÃ“N DE ENLACES...")
        try:
            utilization_metrics = analyzer.calculate_link_utilization()
            print(f"âœ… UtilizaciÃ³n calculada exitosamente")
        except Exception as e:
            print(f"âŒ ERROR calculando utilizaciÃ³n de enlaces: {e}")
            import traceback
            traceback.print_exc()

        # ------------------------------------------------------------- #
        #  VerificaciÃ³n grÃ¡fica de la distribuciÃ³n antes del scheduling
        # ------------------------------------------------------------- #
        if plot_gap_dist:
            # ---- parÃ¡metros efectivos segÃºn el modo ya aplicado a Net ----
            dist_mode = Net.PACKET_GAP_MODE
            
            if dist_mode == "fixed":
                eff_params = [Net.PACKET_GAP_EXTRA]
            elif dist_mode == "uniform":
                eff_params = list(Net.PACKET_GAP_UNIFORM)
            elif dist_mode == "exponential":
                eff_params = [Net.PACKET_GAP_EXTRA]
            elif dist_mode == "gaussian":
                eff_params = list(Net.PACKET_GAP_GAUSS)
            elif dist_mode == "pareto":
                eff_params = list(Net.PACKET_GAP_PARETO)
            else:
                eff_params = []

            from tools.definitions import OUT_DIR
            _plot_gap_distribution(dist_mode, eff_params, OUT_DIR)

        # Mostrar el contenido del log de scheduling en la consola si se solicita
        if show_log:
            try:
                if os.path.exists(log_path):
                    with open(log_path, 'r') as f:
                        log_content = f.read()
                    print("\n" + "="*80)
                    print("SCHEDULING DETAILS BY LINK:")
                    print("="*80)
                    print(log_content)
                    print("="*80 + "\n")
                else:
                    logging.error(f"Archivo de log no encontrado: {log_path}")
            except Exception as e:
                logging.error(f"Error reading schedule log: {e}")
        
        if visualize:
            # Try to visualize with error handling
            try:
                # Visualizar la programaciÃ³n usando Plotly (mÃ¡s estable)
                save_path = os.path.join(OUT_DIR, f'tsn_schedule_{topo}_{num_flows}.html')
                visualize_tsn_schedule_plotly(schedule_res, save_path)
                
                logging.info(f"VisualizaciÃ³n interactiva guardada en {save_path}")
            except Exception as e:
                logging.error(f"Error durante la visualizaciÃ³n: {e}")
                logging.info("Continuando sin visualizaciÃ³n interactiva.")

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        #  NUEVO: exportar .ned y .ini cada vez que haya scheduling OK
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        try:
            ned_path, ini_path = export_omnet_files(
                network,
                schedule_res,
                analyzer._gcl_tables,
                topo,
                OUT_DIR
            )
            logging.info(f"OMNeT++ files escritos:\n  â€¢ {ned_path}\n  â€¢ {ini_path}")

            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            #  RESUMEN GLOBAL DE FLOWS â€” Ãºltima lÃ­nea del log
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            logging.info(f"Programados con Ã©xito: {scheduled_cnt}/{num_flows} flujos")

        except Exception as e:
            logging.error(f"Error exportando ficheros OMNeT++: {e}")

    else:
        logging.error("No se pudo programar ningÃºn flujo.")

    return is_scheduled


if __name__ == '__main__':
    # Â«resolveÂ» evita choques de nombres si algÃºn mÃ³dulo aÃ±ade flags
    parser = argparse.ArgumentParser(conflict_handler="resolve")
    parser.add_argument('--topo', type=str, required=True)
    parser.add_argument('--num_flows', type=int, required=True)
    parser.add_argument('--alg', type=str, default='MaskablePPO')
    parser.add_argument('--link_rate', type=int, default=100)
    # AÃ±adir argumentos para min/max payload
    parser.add_argument('--min-payload', type=int, default=DEFAULT_MIN_PAYLOAD, help=f"TamaÃ±o mÃ­nimo de payload en bytes (default: {DEFAULT_MIN_PAYLOAD})")
    parser.add_argument('--max-payload', type=int, default=DEFAULT_MAX_PAYLOAD, help=f"TamaÃ±o mÃ¡ximo de payload en bytes (default: {DEFAULT_MAX_PAYLOAD})")
    # â”€â”€ parÃ¡metro eliminado: max-jitter ya no existe â”€â”€
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€ NUEVA interfaz unificada â”€â”€â”€â”€â”€â”€â”€â”€â”€
    parser.add_argument('--dist', type=str, default='fixed',
                        choices=['fixed', 'uniform', 'exponential', 'gaussian', 'pareto'],
                        help='DistribuciÃ³n de separaciÃ³n de paquetes')
    parser.add_argument('--dist-params', type=float, nargs='+', default=[],
                        help='ParÃ¡metros de la distribuciÃ³n (ver README)')
    parser.add_argument('--visualize', action='store_true', default=True,
                        help='Generar visualizaciÃ³n TSN')
    parser.add_argument('--show-log', action='store_true', default=True, help='Mostrar detalles del scheduling en consola')
    parser.add_argument('--gcl-threshold', type=int, default=30, 
                        help='Threshold in Âµs for GCL entry generation')
    parser.add_argument('--no-gap-plot',
                        action='store_false',
                        dest='plot_gap_dist',
                        help='Desactiva la grÃ¡fica de verificaciÃ³n de gaps')
    # Se eliminÃ³ completamente el parÃ¡metro --best_model_path
    
    args = parser.parse_args()
    
    # Validar rango de payload
    if args.min_payload > args.max_payload:
        logging.error(f"Error: min-payload ({args.min_payload}) no puede ser mayor que max-payload ({args.max_payload})")
        sys.exit(1)
    if args.min_payload < 1 or args.max_payload < 1:
        logging.error("Error: min-payload y max-payload deben ser >= 1")
        sys.exit(1)

    # ğŸ‘‰ Configurar la distribuciÃ³n global de separaciÃ³n entre paquetes
    from core.network.net import Net
    try:
        Net.set_gap_distribution(args.dist, args.dist_params)
    except (AssertionError, ValueError) as e:
        logging.error(e)
        sys.exit(1)

    # ----------------------------------------------------------------- #
    #  La verificaciÃ³n de gaps se harÃ¡ **despuÃ©s** del scheduling
    # ----------------------------------------------------------------- #

    # La funciÃ³n test ahora determinarÃ¡ automÃ¡ticamente la ruta del modelo y usarÃ¡ el rango de payload
    test(args.topo, args.num_flows, 0,
         None, args.alg, args.link_rate, 
         min_payload=args.min_payload, max_payload=args.max_payload,
         visualize=args.visualize, show_log=args.show_log,
         gcl_threshold=args.gcl_threshold,
         plot_gap_dist=args.plot_gap_dist)



## ARCHIVO: ui/train.py
## ==================================================

import argparse
import logging
import matplotlib.pyplot as plt
import multiprocessing
import numpy as np
import random
import os
import sys
import shutil  # Para eliminar directorios recursivamente
import time
from stable_baselines3.common.callbacks import BaseCallback


# Add Qt platform environment variable before any imports that might use Qt
# This helps Qt find the correct platform plugin
os.environ["QT_QPA_PLATFORM"] = "offscreen"  # Use offscreen rendering by default

# Configurar el path antes de cualquier otra importaciÃ³n
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
#print(f"Set Python path to include: {os.path.dirname(os.path.dirname(os.path.abspath(__file__)))}")

# Importar mÃ³dulos desde rutas relativas
from ui.test import test
from tools.definitions import OUT_DIR, LOG_DIR  # Add LOG_DIR to import
from core.learning.encoder import FeaturesExtractor
# Importar MaskablePPO directamente
from sb3_contrib import MaskablePPO
from core.scheduler.scheduler import DrlScheduler
from core.learning.environment import NetEnv # Eliminar TrainingNetEnv
from tools.log_config import log_config
from core.network.net import FlowGenerator, UniDirectionalFlowGenerator, generate_graph, Network
from tools.definitions import OUT_DIR, LOG_DIR

from stable_baselines3.common.callbacks import EvalCallback
from stable_baselines3.common.results_plotter import load_results, ts2xy
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import SubprocVecEnv

TOPO = 'SIMPLE'  # Cambiado de 'CEV' a 'SIMPLE' para coincidir con el valor por defecto

# Usar siempre el nÃºmero mÃ¡ximo de cores disponibles
NUM_ENVS = max(1, multiprocessing.cpu_count())
NUM_FLOWS = 50

# Cambiar la definiciÃ³n del algoritmo a una constante fija
DRL_ALG = 'MaskablePPO'

MONITOR_ROOT_DIR = os.path.join(OUT_DIR, "monitor")

DEFAULT_MIN_PAYLOAD = 64   # Valor por defecto mÃ­nimo razonable
DEFAULT_MAX_PAYLOAD = 1518 # Valor por defecto mÃ¡ximo MTU


def get_best_model_path(topo=TOPO, alg=DRL_ALG):
    """Retorna la ruta al modelo entrenado segÃºn la topologÃ­a y algoritmo"""
    return os.path.join(OUT_DIR, f"best_model_{topo}_{alg}")

def get_best_model_file(topo=TOPO, alg=DRL_ALG):
    """Retorna la ruta completa al archivo del modelo (best_model.zip)"""
    return os.path.join(get_best_model_path(topo, alg), "best_model.zip")


def make_env(num_flows, rank: int, topo: str, monitor_dir, training: bool = True, link_rate: int = 100, 
             min_payload: int = DEFAULT_MIN_PAYLOAD, max_payload: int = DEFAULT_MAX_PAYLOAD,
             use_curriculum: bool = True, use_graph_observation: bool = None):
    def _init():
        graph = generate_graph(topo, link_rate)

        # Simplificar - eliminar jitters
        # Cualquier variante "UNIDIR*" se trata como unidireccional
        is_unidir = topo.startswith("UNIDIR")
        # Pasar el rango de payload al generador
        if is_unidir:
            flow_generator = UniDirectionalFlowGenerator(graph, min_payload=min_payload, max_payload=max_payload)
        else:
            flow_generator = FlowGenerator(graph, min_payload=min_payload, max_payload=max_payload)

        # Generar todos los flujos - asegurarse de crear exactamente el nÃºmero solicitado
        flows = flow_generator(num_flows)
        logging.info(f"Generados {len(flows)} flujos para {topo} (solicitados: {num_flows})")
        
        network = Network(graph, flows)
        
        # Determinar si usar observaciones de grafo si no se especifica
        if use_graph_observation is None:
            use_graph_obs = DRL_ALG in ["SAC", "MaskableSAC"]
        else:
            use_graph_obs = use_graph_observation
        
        # Crear entorno con curriculum learning adaptativo
        env = NetEnv(
            network, 
            curriculum_enabled=use_curriculum,  
            initial_complexity=0.25 if use_curriculum else 1.0,  # Si no hay curriculum, usar 100% de complejidad
            curriculum_step=0.05,      # Incrementar 5% por cada Ã©xito
            use_graph_observation=use_graph_obs
        )

        # Wrap the environment with Monitor
        env = Monitor(env, os.path.join(monitor_dir, f'{"train" if training else "eval"}_{rank}'))
        return env

    return _init


def train(topo: str, num_time_steps, monitor_dir, num_flows=NUM_FLOWS, pre_trained_model=None, link_rate=100, min_payload: int = DEFAULT_MIN_PAYLOAD, max_payload: int = DEFAULT_MAX_PAYLOAD, use_curriculum: bool = True, show_log: bool = True):
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    #  NUEVO: Limpiar completamente el directorio de salida
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if os.path.exists(OUT_DIR):
        logging.info(f"Limpiando directorio de salida: {OUT_DIR}")
        shutil.rmtree(OUT_DIR)
    
    # Recrear el directorio vacÃ­o
    os.makedirs(OUT_DIR, exist_ok=True)
    os.makedirs(LOG_DIR, exist_ok=True)  # TambiÃ©n recreamos LOG_DIR    # Siempre usar todos los cores disponibles
    n_envs = NUM_ENVS
    logging.info(f"Usando {n_envs} entornos en paralelo (nÃºcleos CPU detectados: {multiprocessing.cpu_count()})")
    
    env = SubprocVecEnv([
        # Ya no hay distinciÃ³n entre entornos de entrenamiento y evaluaciÃ³n,
        # ambos usan la configuraciÃ³n completa de num_flows desde el principio
        make_env(num_flows, i, topo, monitor_dir, link_rate=link_rate, min_payload=min_payload, max_payload=max_payload, use_curriculum=use_curriculum, use_graph_observation=DRL_ALG in ["SAC", "MaskableSAC"])  # Pasar flag de curriculum
        for i in range(n_envs)
        ])# FORZAR ENTRENAMIENTO DESDE CERO - No cargar modelo pre-entrenado por defecto
    if pre_trained_model is not None and os.path.exists(pre_trained_model):
        logging.info(f"Cargando modelo pre-entrenado: {pre_trained_model}")
        model = MaskablePPO.load(pre_trained_model, env)
    else:
        if pre_trained_model is not None:
            logging.warning(f"Modelo pre-entrenado especificado no existe: {pre_trained_model}")
        
        logging.info("Iniciando entrenamiento DESDE CERO (sin modelo pre-entrenado)")
        
        # -------- seleccionar extractor automÃ¡ticamente --------
        from core.learning.hats_extractor import HATSExtractor
        extractor_cls = HATSExtractor if DRL_ALG in ["SAC", "MaskableSAC"] else FeaturesExtractor
        policy_kwargs = dict(features_extractor_class=extractor_cls)        # Crear modelo completamente nuevo sin cargar ningÃºn modelo previo
        model = MaskablePPO("MlpPolicy", env, policy_kwargs=policy_kwargs, verbose=1)

    eval_env = SubprocVecEnv([
        # El entorno de evaluaciÃ³n tambiÃ©n usa la misma configuraciÃ³n
        make_env(num_flows, i, topo, monitor_dir, training=False, link_rate=link_rate, min_payload=min_payload, max_payload=max_payload, use_curriculum=False, use_graph_observation=DRL_ALG in ["SAC", "MaskableSAC"])
        for i in range(n_envs)
        ])
    
    # Crear callback de evaluaciÃ³n y mÃ©tricas
    callbacks = [
        EvalCallback(eval_env, 
                   best_model_save_path=get_best_model_path(topo=topo, alg=DRL_ALG),
                   log_path=OUT_DIR, 
                   eval_freq=max(10000 // n_envs, 1))
    ]

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  ğŸ“Š NUEVO: Variables para medir tiempo de convergencia
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    convergence_data = {
        "rewards": [],           # Historial de rewards promedio
        "episode_numbers": [],   # NÃºmeros de episodio
        "timestamps": [],        # Timestamps reales
        "convergence_episode": None,    # Episodio donde converge
        "convergence_time": None,       # Tiempo real de convergencia
        "is_converged": False,          # Si ya convergiÃ³
        "stability_window": 100,        # Ventana para medir estabilidad
        "stability_threshold": 0.05,    # Umbral de variaciÃ³n para convergencia (5%)
        "min_episodes_for_convergence": 200  # MÃ­nimo de episodios antes de declarar convergencia
    }
    
    
    start_time = time.time()
    
    # Callback para capturar mÃ©tricas de convergencia
    class ConvergenceCallback(BaseCallback):
        def __init__(self, convergence_data, max_timesteps, verbose=0):
            super().__init__(verbose)
            self.convergence_data = convergence_data
            self.episode_count = 0
            self.max_timesteps = max_timesteps
            self.timesteps_count = 0
            self.logged_stop = False
            
        def _on_step(self) -> bool:
            # CONTROL ESTRICTO: Usar el contador interno del modelo
            current_timesteps = self.model.num_timesteps
            
            # FORZAR PARADA EXACTA cuando alcance el lÃ­mite
            if current_timesteps >= self.max_timesteps:
                if not self.logged_stop:
                    logging.info(f"ğŸ›‘ PARADA FORZADA: Alcanzado lÃ­mite exacto de {self.max_timesteps} timesteps")
                    logging.info(f"ğŸ”¢ Timesteps del modelo: {current_timesteps}")
                    self.logged_stop = True
                return False  # Detener inmediatamente
            
            # Capturar datos cada vez que termina un episodio
            if len(self.locals.get('dones', [])) > 0 and any(self.locals['dones']):
                self.episode_count += 1
                
                # Obtener reward promedio de los entornos activos
                if 'infos' in self.locals:
                    episode_rewards = []
                    for info in self.locals['infos']:
                        if isinstance(info, dict) and 'episode' in info:
                            episode_rewards.append(info['episode']['r'])
                    
                    if episode_rewards:
                        avg_reward = sum(episode_rewards) / len(episode_rewards)
                        current_time = time.time()
                        
                        # Guardar datos
                        self.convergence_data["rewards"].append(avg_reward)
                        self.convergence_data["episode_numbers"].append(self.episode_count)
                        self.convergence_data["timestamps"].append(current_time)
                        
                        # Verificar convergencia si tenemos suficientes datos
                        self._check_convergence()
                        
                        # Log de progreso cada 100 episodios
                        if self.episode_count % 100 == 0:
                            logging.info(f"ğŸ“Š Episodio {self.episode_count}, Timesteps: {current_timesteps}/{self.max_timesteps}, Reward promedio: {avg_reward:.3f}")
                        
            return True
            
        def _check_convergence(self):
            """Verifica si el algoritmo ha convergido basÃ¡ndose en la estabilidad de rewards"""
            data = self.convergence_data
            
            # No verificar hasta tener suficientes episodios
            if (len(data["rewards"]) < data["min_episodes_for_convergence"] or 
                data["is_converged"]):
                return
                
            window_size = data["stability_window"]
            threshold = data["stability_threshold"]
            
            # Verificar si tenemos suficientes datos para la ventana
            if len(data["rewards"]) < window_size:
                return
                
            # Obtener los Ãºltimos rewards en la ventana
            recent_rewards = data["rewards"][-window_size:]
            
            # Calcular estadÃ­sticas de estabilidad
            mean_reward = sum(recent_rewards) / len(recent_rewards)
            
            if mean_reward == 0:  # Evitar divisiÃ³n por cero
                return
                
            # Calcular coeficiente de variaciÃ³n (desviaciÃ³n estÃ¡ndar / media)
            variance = sum((r - mean_reward) ** 2 for r in recent_rewards) / len(recent_rewards)
            std_dev = variance ** 0.5
            coefficient_of_variation = std_dev / abs(mean_reward)
            
            # Declarar convergencia si la variaciÃ³n es menor al umbral
            if coefficient_of_variation <= threshold:
                data["is_converged"] = True
                data["convergence_episode"] = data["episode_numbers"][-1]
                data["convergence_time"] = data["timestamps"][-1] - data["timestamps"][0]
                
                logging.info(
                    f"ğŸ¯ CONVERGENCIA DETECTADA en episodio {data['convergence_episode']} "
                    f"(Coef. VariaciÃ³n: {coefficient_of_variation:.4f} â‰¤ {threshold})"
                )
    
    # Crear callback de convergencia con lÃ­mite de timesteps
    convergence_callback = ConvergenceCallback(convergence_data, num_time_steps)
    
    # Combinar callbacks existentes con el de convergencia
    callbacks = [
        EvalCallback(eval_env, 
                   best_model_save_path=get_best_model_path(topo=topo, alg=DRL_ALG),
                   log_path=OUT_DIR, 
                   eval_freq=max(10000 // n_envs, 1)),
        convergence_callback
    ]

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  ğŸ¯ ENTRENAMIENTO CON CONTROL EXACTO DE TIMESTEPS - VERSIÃ“N CORREGIDA
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    logging.info(f"Iniciando entrenamiento por EXACTAMENTE {num_time_steps} timesteps...")
    logging.info(f"Usando {n_envs} entornos en paralelo")
    
    # Crear un callback adicional mÃ¡s simple que solo controle timesteps
    class StrictTimestepCallback(BaseCallback):
        def __init__(self, max_timesteps, verbose=0):
            super().__init__(verbose)
            self.max_timesteps = max_timesteps
            self.logged = False
            
        def _on_step(self) -> bool:
            if self.model.num_timesteps >= self.max_timesteps:
                if not self.logged:
                    logging.info(f"ğŸš¨ PARADA ESTRICTA: {self.model.num_timesteps} timesteps alcanzados")
                    self.logged = True
                return False
            return True
    
    # Combinar callbacks con el de parada estricta como el Ãºltimo
    callbacks = [
        EvalCallback(eval_env, 
                   best_model_save_path=get_best_model_path(topo=topo, alg=DRL_ALG),
                   log_path=OUT_DIR, 
                   eval_freq=max(10000 // n_envs, 1)),
        convergence_callback,
        StrictTimestepCallback(num_time_steps)  # Este tiene la prioridad final
    ]

    # Entrenar con configuraciÃ³n mÃ¡s estricta - FORZAR RESET COMPLETO
    model.learn(
        total_timesteps=num_time_steps,
        callback=callbacks,
        progress_bar=True,
        reset_num_timesteps=True  # â† CRUCIAL: resetear completamente los timesteps
    )
    
    # Verificar timesteps finales con mayor detalle
    actual_timesteps = model.num_timesteps
    
    logging.info(f"âœ… Entrenamiento completado")
    logging.info(f"ğŸ¯ Timesteps solicitados: {num_time_steps}")
    logging.info(f"ğŸ“Š Timesteps ejecutados: {actual_timesteps}")
    
    if actual_timesteps == num_time_steps:
        logging.info(f"ğŸ¯ PERFECTO: Timesteps exactos ejecutados")
    elif actual_timesteps > num_time_steps:
        sobrepaso = actual_timesteps - num_time_steps
        sobrepaso_pct = (sobrepaso / num_time_steps) * 100
        logging.warning(f"âš ï¸ SOBREPASO: {sobrepaso} timesteps adicionales ({sobrepaso_pct:.2f}%)")
    else:
        deficit = num_time_steps - actual_timesteps
        logging.warning(f"âš ï¸ DÃ‰FICIT: {deficit} timesteps menos de lo esperado")
    
    end_time = time.time()
    total_training_time = end_time - start_time
    
    logging.info("Training complete.")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  ğŸ“Š ANÃLISIS Y REPORTE DE CONVERGENCIA
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    if show_log:
        print("\n" + "="*80)
        print("ğŸ¯ ANÃLISIS DE CONVERGENCIA DEL ENTRENAMIENTO")
        print("="*80)
        
        total_episodes = len(convergence_data["rewards"])
        
        if convergence_data["is_converged"]:
            convergence_episode = convergence_data["convergence_episode"]
            convergence_time_seconds = convergence_data["convergence_time"]
            convergence_percentage = (convergence_episode / total_episodes) * 100
            
            print(f"âœ… CONVERGENCIA ALCANZADA:")
            print(f"   ğŸ“ˆ Episodio de convergencia: {convergence_episode}/{total_episodes} ({convergence_percentage:.1f}%)")
            print(f"   â±ï¸  Tiempo hasta convergencia: {convergence_time_seconds:.1f} segundos")
            print(f"   ğŸ“Š Ventana de estabilidad: {convergence_data['stability_window']} episodios")
            print(f"   ğŸšï¸  Umbral de variaciÃ³n: {convergence_data['stability_threshold']*100:.1f}%")
            
            # Calcular eficiencia del entrenamiento
            efficiency = (total_training_time - convergence_time_seconds) / total_training_time * 100
            if efficiency > 0:
                print(f"   âš¡ Tiempo 'desperdiciado' post-convergencia: {efficiency:.1f}% del entrenamiento")
        
        else:
            print(f"âš ï¸  NO SE DETECTÃ“ CONVERGENCIA:")
            print(f"   ğŸ“ˆ Episodios totales: {total_episodes}")
            print(f"   â±ï¸  Tiempo total: {total_training_time:.1f} segundos")
            print(f"   ğŸ“Š Ventana requerida: {convergence_data['stability_window']} episodios estables")
            print(f"   ğŸšï¸  Umbral requerido: variaciÃ³n â‰¤ {convergence_data['stability_threshold']*100:.1f}%")
            
            # Analizar la tendencia final
            if len(convergence_data["rewards"]) >= 50:
                recent_50 = convergence_data["rewards"][-50:]
                mean_recent = sum(recent_50) / len(recent_50)
                variance_recent = sum((r - mean_recent) ** 2 for r in recent_50) / len(recent_50)
                std_recent = variance_recent ** 0.5
                cv_recent = std_recent / abs(mean_recent) if mean_recent != 0 else float('inf')
                
                print(f"   ğŸ“‰ VariaciÃ³n en Ãºltimos 50 episodios: {cv_recent*100:.2f}%")
                
                if cv_recent <= convergence_data['stability_threshold'] * 2:  # Doble del umbral
                    print(f"   ğŸ’¡ Sugerencia: El algoritmo estÃ¡ cerca de converger, considere mÃ¡s episodios")
        
        # EstadÃ­sticas generales del entrenamiento
        if convergence_data["rewards"]:
            max_reward = max(convergence_data["rewards"])
            min_reward = min(convergence_data["rewards"])
            final_reward = convergence_data["rewards"][-1]
            avg_reward = sum(convergence_data["rewards"]) / len(convergence_data["rewards"])
            
            print(f"\nğŸ“Š ESTADÃSTICAS DE RECOMPENSAS:")
            print(f"   ğŸ† MÃ¡xima: {max_reward:.3f}")
            print(f"   ğŸ“‰ MÃ­nima: {min_reward:.3f}")
            print(f"   ğŸ¯ Final: {final_reward:.3f}")
            print(f"   ğŸ“ˆ Promedio: {avg_reward:.3f}")
        
        print("="*80)
    else:
        # Siempre mostrar un resumen bÃ¡sico aunque show_log sea False
        total_episodes = len(convergence_data["rewards"])
        if convergence_data["is_converged"]:
            print(f"âœ… Convergencia alcanzada en episodio {convergence_data['convergence_episode']}/{total_episodes}")
        else:
            print(f"âš ï¸ Sin convergencia detectada en {total_episodes} episodios")
    
    # Log para archivo (siempre se escribe en el log)
    if convergence_data["is_converged"]:
        logging.info(
            f"ğŸ¯ Convergencia: episodio {convergence_data['convergence_episode']} "
            f"({convergence_data['convergence_time']:.1f}s) de {total_episodes} episodios totales"
        )
    else:
        logging.info(f"âš ï¸ Sin convergencia detectada en {total_episodes} episodios ({total_training_time:.1f}s)")

    # NUEVO: Ejecutar un episodio final y guardar los detalles del scheduling
    if show_log:
        logging.info("Ejecutando episodio final para generar informe detallado...")
        print("\nğŸ” Ejecutando episodio final para validar el entrenamiento...")
    
    obs = env.reset()
    done = False
    final_info = None
    
    # Ejecutar un episodio completo con el modelo entrenado
    while not done:
        action_masks = np.vstack(env.env_method('action_masks'))
        action, _ = model.predict(obs, deterministic=True, action_masks=action_masks.astype(np.int8))
        obs, _, dones, infos = env.step(action)
        done = any(dones)
        if done:
            # Buscar informaciÃ³n de Ã©xito en algÃºn entorno
            for i, is_done in enumerate(dones):
                if is_done and infos[i].get('success'):
                    final_info = infos[i]
                    break
    
    # Mostrar resultado del episodio final si show_log estÃ¡ activado
    if final_info and final_info.get('ScheduleRes'):
        if show_log:
            print(f"âœ… Episodio de validaciÃ³n exitoso: {len(final_info['ScheduleRes'])} enlaces programados")
            print(f"ğŸ“Š Flujos programados: {final_info.get('scheduled_flows', 0)}/{final_info.get('total_flows', num_flows)}")
        logging.info(f"Scheduling final exitoso con {len(final_info['ScheduleRes'])} enlaces programados")
    elif show_log:
        print("âš ï¸ Episodio de validaciÃ³n no completÃ³ el scheduling exitosamente")

    return None  # Return None instead of metrics


def moving_average(values, window):
    """
    Smooth values by doing a moving average
    :param values: (numpy array)
    :param window: (int)
    :return: (numpy array)
    """
    weights = np.repeat(1.0, window) / window
    return np.convolve(values, weights, "valid")


def plot_results(log_folder, title="Learning Curve"):
    """
    plot the results

    :param log_folder: (str) the save location of the results to plot
    :param title: (str) the title of the task to plot
    """
    try:
        x, y = ts2xy(load_results(log_folder), "timesteps")
        y = moving_average(y, window=50)
        # Truncate x
        x = x[len(x) - len(y):]

        fig = plt.figure(title)
        plt.plot(x, y)
        plt.xlabel("Number of Timesteps")
        plt.ylabel("Rewards")
        plt.title(title + " Smoothed")
        
        # Set Y-axis to start from 0 and go to max value with some padding
        if len(y) > 0:
            max_reward = max(y)
            plt.ylim(0, max_reward * 1.05)  # Add 5% padding at the top
        
        plt.savefig(os.path.join(log_folder, "reward.png"))
        
        # Use non-blocking display and catch any errors
        try:
            plt.show(block=False)
            plt.pause(3)  # Espera 3 segundos para mostrar la grÃ¡fica
            plt.close()
        except Exception as e:
            logging.warning(f"Could not display plot interactively: {e}")
            logging.info("Plot saved to file, continuing without interactive display.")
    except Exception as e:
        logging.error(f"Error plotting results: {e}")
        logging.info("Continuing without plotting.")


def main():
    # Mover todas las declaraciones global al inicio de la funciÃ³n
    global TOPO, NUM_ENVS, DRL_ALG
    
    # specify an existing model to train.
    parser = argparse.ArgumentParser(conflict_handler="resolve")
    parser.add_argument('--time_steps', type=int, required=True)
    parser.add_argument('--num_flows', type=int, nargs='?', default=NUM_FLOWS)
    # Eliminar la opciÃ³n de especificar num_envs, ahora es automÃ¡tico
    parser.add_argument('--topo', type=str, default="SIMPLE", help="Topology type (e.g., SIMPLE, UNIDIR)")
    parser.add_argument('--link_rate', type=int, default=100)
    # AÃ±adir argumentos para min/max payload
    parser.add_argument('--min-payload', type=int, default=DEFAULT_MIN_PAYLOAD, help=f"TamaÃ±o mÃ­nimo de payload en bytes (default: {DEFAULT_MIN_PAYLOAD})")
    parser.add_argument('--max-payload', type=int, default=DEFAULT_MAX_PAYLOAD, help=f"TamaÃ±o mÃ¡ximo de payload en bytes (default: {DEFAULT_MAX_PAYLOAD})")
    # Cambio: Hacer --model opcional con un valor por defecto de None
    parser.add_argument('--model', type=str, default=None, 
                       help="Ruta opcional a un modelo pre-entrenado. Por defecto no carga ninguno.")
    # ------------- distribuciÃ³n del gap ------------- 
    # Ahora **todo** se controla con --dist y --dist-params
    parser.add_argument('--dist', type=str, default='fixed',
                        choices=['fixed', 'uniform', 'exponential', 'gaussian', 'pareto'],
                        help="Tipo de distribuciÃ³n de separaciÃ³n de paquetes")
    parser.add_argument('--dist-params', type=float, nargs='+', default=[],
                        help="ParÃ¡metros numÃ©ricos de la distribuciÃ³n (ver doc)")

    # --- NUEVAS OPCIONES PARA COINCIDIR CON ui/test.py --------------------
    parser.add_argument('--visualize', action='store_true', default=True,
                        help='Generar visualizaciÃ³n TSN al terminar')
    parser.add_argument('--show-log', action='store_true', default=True,
                        help='Mostrar informaciÃ³n detallada del entrenamiento y scheduling')
    parser.add_argument('--gcl-threshold', type=int, default=30,
                        help='Umbral (Âµs) para generar entradas GCL')
                        
    # AÃ±adir opciÃ³n para controlar el curriculum learning
    parser.add_argument('--curriculum', action='store_true', default=True,
                      help='Usar curriculum learning adaptativo (por defecto activado)')
    parser.add_argument('--no-curriculum', action='store_false', dest='curriculum',
                      help='Desactivar curriculum learning adaptativo')
   
    args = parser.parse_args()


    if args.link_rate is not None:
        support_link_rates = [100, 1000]
        assert args.link_rate in support_link_rates, \
            f"Unknown link rate {args.link_rate}, which is not in supported link rates {support_link_rates}"

    # Validar rango de payload
    if args.min_payload > args.max_payload:
        logging.error(f"Error: min-payload ({args.min_payload}) no puede ser mayor que max-payload ({args.max_payload})")
        sys.exit(1)
    if args.min_payload < 1 or args.max_payload < 1:
        logging.error("Error: min-payload y max-payload deben ser >= 1")
        sys.exit(1)

    # Eliminar procesamiento de jitters
    TOPO = args.topo
    # NUM_ENVS = args.num_envs  # Esta lÃ­nea se elimina

    # ğŸ‘‰ Aplicar el valor elegido antes de crear cualquier entorno
    from core.network.net import Net
    
    # -------- configuraciÃ³n Ãºnica (vigente) -------- #
    try:
        Net.set_gap_distribution(args.dist, args.dist_params)
    except (AssertionError, ValueError) as e:
        logging.error(e)
        sys.exit(1)

    log_config(os.path.join(OUT_DIR, f"train.log"), logging.DEBUG)

    logging.info(args)

    done = False
    i = 0
    MONITOR_DIR = None
    while not done:
        try:
            MONITOR_DIR = os.path.join(MONITOR_ROOT_DIR, str(i))
            os.makedirs(MONITOR_DIR, exist_ok=False)
            done = True
        except OSError:
            i += 1
            continue
    assert MONITOR_DIR is not None

    logging.info("start training...")
    # Pasar show_log al mÃ©todo train
    train(args.topo, args.time_steps,
          MONITOR_DIR,  # Pasar MONITOR_DIR como parÃ¡metro
          num_flows=args.num_flows,
          pre_trained_model=args.model,  # Usar args.model (que podrÃ­a ser None)
          link_rate=args.link_rate,
          min_payload=args.min_payload, # Pasar min/max payload
          max_payload=args.max_payload,
          use_curriculum=args.curriculum,
          show_log=args.show_log)  # â† NUEVO: Pasar show_log

    # Add try-except block around plotting
    try:
        plot_results(MONITOR_DIR)
    except Exception as e:
        logging.error(f"Error during plotting: {e}")
        logging.info("Continuing without plotting.")

    # Remove metrics summary code
    logging.info(f"Training completed successfully.")

    # Al terminar training, invocamos test con los mismos args
    test(
        args.topo,
        args.num_flows,
        NUM_ENVS,
        alg=DRL_ALG,
        link_rate=args.link_rate,
        min_payload=args.min_payload,
        max_payload=args.max_payload,
        visualize=args.visualize,
        show_log=args.show_log,  # â† NUEVO: Pasar show_log tambiÃ©n a test
        gcl_threshold=args.gcl_threshold
    )

if __name__ == "__main__":
    main()



## ARCHIVO: ui/tsn_visualizer_plotly.py
## ==================================================

import plotly.graph_objects as go
from plotly.subplots import make_subplots
import numpy as np
from core.network.net import Net
import os
import math
import re
import colorsys
from collections import defaultdict
import webbrowser
import logging


from tools.definitions import OUT_DIR
from core.scheduler.scheduler import ScheduleRes

def visualize_tsn_schedule_plotly(schedule_res: ScheduleRes, save_path=None):
    """
    Genera una visualizaciÃ³n interactiva de la programaciÃ³n TSN usando Plotly.
    
    Args:
        schedule_res: Resultado de la programaciÃ³n
        save_path: Ruta para guardar el HTML, por defecto es 'out/tsn_schedule_plotly.html'
    """
    if not schedule_res:
        print("No hay resultados de programaciÃ³n para visualizar.")
        return
 
    
    # Organizar datos por enlace
    link_data = defaultdict(list)
    all_periods = set()
    all_flow_ids = set()
    max_end_time = 0
    
    print("\nProcesando datos para visualizaciÃ³n Plotly...")
    
    # ----- NUEVO: Extraer informaciÃ³n de ocupaciÃ³n de switches -----
    switch_busy_periods = defaultdict(list)
    
    # Track which legend entries have been shown globally
    legend_shown = set()
    
    # Extraer y procesar datos
    for link, operations in schedule_res.items():
        link_str = str(link)
        match = re.search(r"Link\('([^']+)', '([^']+)'\)", link_str)
        if not match:
            print(f"Error: No se pudo extraer origen/destino de {link_str}")
            continue
        
        src, dst = match.group(1), match.group(2)
        link_name = f"{src} â†’ {dst}"
        
        flow_ids = [flow.flow_id for flow, _ in operations]
        print(f"Enlace: {link_name}, Flujos: {flow_ids}")
        
        for flow, operation in operations:
            all_flow_ids.add(flow.flow_id)
            all_periods.add(flow.period)
            
            # Calculate earliest_time on the fly based on the current Operation structure
            earliest_time = operation.start_time if operation.gating_time is None else operation.gating_time
            
            # Guardamos tambiÃ©n el guard-band y el desglose de esperas
            link_data[link_name].append({
                'flow_id'        : flow.flow_id,
                'period'         : flow.period,
                'start_time'     : operation.start_time,
                'earliest_time'  : earliest_time,          # calculado
                'gating_time'    : operation.gating_time,
                'latest_time'    : operation.latest_time,
                'end_time'       : operation.end_time,
                'reception_time' : operation.reception_time,
                # âŠ NUEVOS campos â†’ visualizaciÃ³n de bloques temporales
                'guard_time'     : getattr(operation, 'guard_time', 0),
                'wait_breakdown' : operation.wait_breakdown,
                'min_gap_wait'   : getattr(operation, 'min_gap_wait', 0),
                # ---- acciÃ³n RL (se mantiene) ----
                'offset_idx'     : getattr(operation, 'offset_idx', None),
                'offset_us'      : getattr(operation, 'offset_us',  None),
            })
            
            max_end_time = max(max_end_time, operation.end_time)
            
            # ACTUALIZADO: Extraer ocupaciÃ³n del switch si este enlace sale de un switch
            if src.startswith('S') and not src.startswith('SRV'):
                # El puerto del switch estÃ¡ ocupado durante la transmisiÃ³n SOLAMENTE
                # El switch termina de estar ocupado cuando el paquete sale completamente
                switch_busy_start = earliest_time
                # Mostrar la ocupaciÃ³n real del puerto: transmisiÃ³n + guard-band
                guard_time = link.interference_time() if hasattr(link, "interference_time") else 1.22
                switch_busy_end = operation.end_time + guard_time
                
                # Almacenar perÃ­odo de ocupaciÃ³n para el switch
                switch_busy_periods[src].append({
                    'flow_id': flow.flow_id,
                    'start': switch_busy_start,
                    'end': switch_busy_end,  # El switch termina su trabajo cuando completa la transmisiÃ³n
                    'period': flow.period
                })


    # Calcular el hiperperÃ­odo
    hyperperiod = 1
    for period in all_periods:
        hyperperiod = math.lcm(hyperperiod, period)
    
    print(f"HiperperÃ­odo calculado: {hyperperiod}Âµs")

    # â–¸ valor mÃ¡ximo de Î”sw  â†’  nos permite ampliar el eje-X hacia la izquierda
    global_max_gap = max(
        (op['min_gap_wait'] for link_ops in link_data.values() for op in link_ops),
        default=0
    )
    
    # Ordenar enlaces para visualizaciÃ³n
    # Considera como "switch-link" todo enlace cuyo **origen** sea S<n>
    switch_links = [lnk for lnk in link_data.keys() if re.match(r'^S\d+\s+â†’', lnk)]
    client_links = [lnk for lnk in link_data.keys() if lnk not in switch_links]
    sorted_links = sorted(switch_links) + sorted(client_links)
    
    # --- TODOS los hops de switch van con gate â‡’ un solo esquema de colores ---
    # IMPORTANTE: Definir flow_colors ANTES de cualquier referencia
    flow_colors = {}
    for i, flow_id in enumerate(sorted(all_flow_ids)):
        hue = (i * 0.618033988749895) % 1
        r, g, b = colorsys.hsv_to_rgb(hue, 0.7, 0.9)
        flow_colors[flow_id] = f'rgb({int(r*255)},{int(g*255)},{int(b*255)})'
    
    # ----- NUEVO: AÃ±adir switches a la lista de elementos a visualizar -----
    # Ordenar switches para mostrarlos primero
    sorted_switches = sorted(switch_busy_periods.keys())
    
    # Crear figura de Plotly con subplots compartiendo el eje X
    fig = make_subplots(
        rows=3, 
        cols=1,
        row_heights=[0.15, 0.70, 0.15],  # Proporciones ajustadas para incluir switches
        vertical_spacing=0.02,     # Reducir espacio entre grÃ¡ficas para mejor integraciÃ³n visual
        shared_xaxes=True,         # Compartir eje X para que el zoom se sincronice
        subplot_titles=["OcupaciÃ³n de Switches", "ProgramaciÃ³n de Flujos TSN", "Gate Control List (GCL)"]
    )
    
    # -- NUEVO: SECCIÃ“N 1: OCUPACIÃ“N DE SWITCHES --
    for i, switch_name in enumerate(sorted_switches):
        periods = switch_busy_periods[switch_name]
        
        # Replicar perÃ­odos para todo el hiperperÃ­odo
        for period_info in periods:
            flow_id = period_info['flow_id']
            flow_period = period_info['period']
            repetitions = hyperperiod // flow_period
            
            for rep in range(repetitions):
                time_offset = rep * flow_period
                start_time = period_info['start'] + time_offset
                end_time = period_info['end'] + time_offset
                duration = end_time - start_time
                
                # AÃ±adir barra para el perÃ­odo ocupado
                fig.add_trace(
                    go.Bar(
                        x=[duration],
                        y=[switch_name],
                        orientation='h',
                        base=[start_time],
                        name=f"{switch_name} ocupado ({flow_id})",
                        marker=dict(
                            color='rgba(150,150,150,0.7)',
                            pattern=dict(
                                shape="x",
                                solidity=0.3,
                                fgcolor="black"
                            )
                        ),
                        showlegend=False,
                        hoverinfo='text',
                        hovertext=f"Switch: {switch_name}<br>Ocupado por flujo: {flow_id}<br>Inicio: {start_time}Âµs<br>Fin: {end_time}Âµs",
                    ),
                    row=1, col=1
                )
    
    # -- SECCIÃ“N 2: GRÃFICO PRINCIPAL DE BARRAS (ahora en la segunda fila) --
    # Recopilar eventos de GCL
    gcl_events = []
    for link, operations in schedule_res.items():
        link_str = str(link)
        match = re.search(r"Link\('([^']+)', '([^']+)'\)", link_str)
        if not match:
            continue
        src, dst = match.group(1), match.group(2)
        link_name = f"{src} â†’ {dst}"
        
        # Solo procesar enlaces que salen de un switch
        if not src.startswith('S'):
            continue
        
        # Tratar todos los flujos como crÃ­ticos para un GCL periÃ³dico
        link_operations = operations
        if link_operations:
            print(f"Generando GCL periÃ³dico para enlace switch: {link_name}")
            
            # Calcular el hiperperÃ­odo para todos los flujos
            periods = [flow.period for flow, _ in link_operations]
            hyperperiod_link = 1
            for period in periods:
                hyperperiod_link = math.lcm(hyperperiod_link, period)
                
            # Generar eventos GCL para todos los flujos con el mismo formato
            # ------------------------------------------------------------------
            # MODIFICACIÃ“N: Generar eventos "0" (cerrar) para TODOS los paquetes
            # en su tiempo de recepciÃ³n, sin filtrar por tamaÃ±o de gap.
            # ------------------------------------------------------------------

            # 1) Ordenar operaciones por inicio real de transmisiÃ³n
            ops_sorted = sorted(link_operations,
                                key=lambda p: (p[1].gating_time or p[1].start_time))
            n = len(ops_sorted)

            if n < 2:
                continue

            # 2) HiperperÃ­odo individual del enlace
            periods = [flow.period for flow, _ in ops_sorted]
            hyperperiod_link = 1
            for p in periods:
                hyperperiod_link = math.lcm(hyperperiod_link, p)

            # Variable para el umbral de gap (usado para filtrar quÃ© eventos mostrar)
            gap_thr_us = 50  # umbral de espacio mÃ­nimo para crear entradas GCL
            
            # PASO 1: Recopilar todos los tiempos de transmisiÃ³n y recepciÃ³n
            all_transmission_times = []
            all_reception_times = []
            
            # Recopilar todos los tiempos de transmisiÃ³n y recepciÃ³n
            for i in range(n):
                f_curr, op_curr = ops_sorted[i]
                tx_start = op_curr.gating_time if op_curr.gating_time is not None else op_curr.start_time
                # Para cada paquete, repetirlo durante todo el hiperperÃ­odo
                repetitions = hyperperiod_link // f_curr.period
                for rep in range(repetitions):
                    offset = rep * f_curr.period
                    # Guardar tiempo de inicio y recepciÃ³n (normalizado al hiperperÃ­odo)
                    tx_t = (tx_start + offset) % hyperperiod_link
                    rx_t = (op_curr.reception_time + offset) % hyperperiod_link
                    all_transmission_times.append((tx_t, f_curr.flow_id))
                    all_reception_times.append((rx_t, f_curr.flow_id))
            
            # Ordenar los tiempos
            all_transmission_times.sort(key=lambda x: x[0])
            all_reception_times.sort(key=lambda x: x[0])
            
            # PASO 2: Generar eventos GCL analizando los gaps significativos
            gcl_close_events = []  # Lista temporal para eventos de cierre (0)
            
            # Buscar gaps significativos entre recepciÃ³n y siguiente transmisiÃ³n
            for i in range(len(all_reception_times)):
                rx_time, rx_flow = all_reception_times[i]
                
                # Encontrar el siguiente tiempo de transmisiÃ³n despuÃ©s de esta recepciÃ³n
                next_tx_time = None
                next_tx_flow = None
                
                for tx_time, tx_flow in all_transmission_times:
                    # BÃºsqueda circular (considerando el wraparound del hiperperÃ­odo)
                    if tx_time > rx_time:
                        # Caso normal: siguiente TX estÃ¡ despuÃ©s de RX en este ciclo
                        next_tx_time = tx_time
                        next_tx_flow = tx_flow
                        break
                
                # Si no se encontrÃ³ ninguno, buscar el primero (wraparound)
                if next_tx_time is None and all_transmission_times:
                    next_tx_time = all_transmission_times[0][0] + hyperperiod_link
                    next_tx_flow = all_transmission_times[0][1]
                
                # Calcular el gap (si hay transmisiones)
                if next_tx_time is not None:
                    gap = next_tx_time - rx_time
                    if gap < 0:
                        gap += hyperperiod_link  # Ajustar para gaps negativos (wraparound)
                    
                    # Solo considerar gaps que superen el umbral
                    if gap > gap_thr_us:
                        # AÃ±adir eventos de cierre/apertura
                        gcl_close_events.append((rx_time, rx_flow, next_tx_time, next_tx_flow))
            
            # PASO 3: Generar los pares de eventos 0/1 para cada gap significativo
            for close_time, close_flow, next_tx_time, next_tx_flow in gcl_close_events:
                # Calcular repeticiones para todo el hiperperÃ­odo
                repetitions = hyperperiod_link // hyperperiod_link  # Simplificado a 1
                
                for rep in range(repetitions):
                    offset = rep * hyperperiod_link
                    
                    # AÃ±adir evento de cierre (0) en el tiempo de recepciÃ³n
                    close_t = (close_time + offset) % hyperperiod_link
                    gcl_events.append((close_t, 0, close_flow, link_name, True))
                    
                    # AÃ±adir evento de apertura (1) EXACTAMENTE cuando empieza el siguiente paquete
                    open_t = (next_tx_time + offset) % hyperperiod_link
                    gcl_events.append((open_t, 1, next_tx_flow, link_name, True))

    # Ordenar eventos por tiempo
    gcl_events.sort(key=lambda x: x[0])
    
    # Para barras muy estrechas o marcadores de ventana de transmisiÃ³n, mejorar visibilidad
    for i, link_name in enumerate(sorted_links):
        if (link_name not in link_data):
            continue
            
        operations = link_data[link_name]
        
        for op_data in operations:
            flow_id = op_data['flow_id']
            flow_period = op_data['period']
            repetitions = hyperperiod // flow_period
            
            for rep in range(repetitions):
                # Calcular tiempos con el desplazamiento del perÃ­odo
                time_offset = rep * flow_period
                # Usar los tiempos recalculados si hubo offset
                start_time = op_data['start_time'] + time_offset
                end_time = op_data['end_time'] + time_offset
                earliest_time = op_data['earliest_time'] + time_offset  # Use the computed value
                latest_time = op_data['latest_time'] + time_offset
                gating_time = op_data['gating_time'] + time_offset if op_data['gating_time'] is not None else None
                reception_time = op_data['reception_time'] + time_offset if op_data['reception_time'] is not None else None

                # Tiempo de transmisiÃ³n real (desde gating_time o start_time si no hay gating)
                actual_start_time = gating_time if gating_time is not None else start_time
                transmission_duration = end_time - actual_start_time

                # --- NUEVO: Barra de Tiempo de Espera con distinciÃ³n de tipos ---
                if gating_time is not None and gating_time > start_time:
                    wait_duration = gating_time - start_time
                    
                    # Solo dibujamos la barra base gris (Total)
                    fig.add_trace(
                        go.Bar(
                            x=[wait_duration],
                            y=[link_name],
                            orientation='h',
                            base=[start_time],
                            name="Espera Total",
                            marker=dict(
                                color='rgba(200,200,200,0.3)',
                                line=dict(width=1, color='black'),
                            ),
                            showlegend=False,
                            hoverinfo='none',
                        ),
                        row=2, col=1
                    )

                # â”€â”€â”€â”€â”€â”€â”€â”€â”€ BARRAS DE ESPERA DESGLOSADAS â”€â”€â”€â”€â”€â”€â”€â”€â”€
                if gating_time is not None and gating_time > start_time:
                    wb = op_data['wait_breakdown']          # dict: min_gap / other / total

                    # Dibujar la base gris con la espera total
                    fig.add_trace(
                        go.Bar(
                            x=[wb['total']],
                            y=[link_name],
                            orientation='h',
                            base=[start_time],
                            name="Espera Total",
                            marker=dict(color='rgba(200,200,200,0.25)'),
                            showlegend=False,
                            hoverinfo='none',
                        ),
                        row=2, col=1
                    )

                    waits = [
                        # Solo las esperas controladas por RL
                        ('min_gap', 'rgba(220, 20, 60,0.8)', "\\", "SeparaciÃ³n mÃ­nima"),
                        ('other',   'rgba(120,120,120,0.5)', "",   "Otros"),
                    ]

                    offset_acc = 0
                    for key, color, pattern, label in waits:
                        w = wb.get(key, 0)
                        if w == 0:
                            continue
                        
                        # Check if this legend entry has been shown before
                        legend_key = f"wait_{key}"
                        show_legend = legend_key not in legend_shown
                        if show_legend:
                            legend_shown.add(legend_key)
                            
                        fig.add_trace(
                            go.Bar(
                                x=[w],
                                y=[link_name],
                                orientation='h',
                                base=[start_time + offset_acc],
                                name=label,
                                marker=dict(color=color,
                                            pattern=dict(shape=pattern, solidity=0.35)),
                                showlegend=show_legend,
                                legendgroup=legend_key,
                                hoverinfo='text',
                                hovertext=f"{label}: {w} Âµs<br>Flujo: {flow_id}",
                            ),
                            row=2, col=1
                        )
                        offset_acc += w

                # ---------------------------------------------------------------
                #  BLOQUE 2 Â· ESPERA EN EL SWITCH  (Î”sw y resto de waits)
                # ---------------------------------------------------------------
                wb = op_data['wait_breakdown']
                # â–¶ï¸  dibujamos SIEMPRE que exista min_gap_wait, con o sin gating
                if op_data['min_gap_wait'] > 0 and not wb:
                    # reconstruir diccionario vacÃ­o
                    wb = {'min_gap': op_data['min_gap_wait'],
                          'other'  : 0,
                          'total'  : op_data['min_gap_wait']}

                if wb:
                    # Dibujar la base gris con la espera total
                    fig.add_trace(
                        go.Bar(
                            x=[wb['total']],
                            y=[link_name],
                            orientation='h',
                            base=[start_time],
                            name="Espera Total",
                            marker=dict(color='rgba(200,200,200,0.25)'),
                            showlegend=False,
                            hoverinfo='none',
                        ),
                        row=2, col=1
                    )

                    waits = [
                        ('min_gap', 'rgba(220, 20, 60,0.8)', "\\", "Î” sw   (gap mÃ­n.)"),
                        ('other',   'rgba(120,120,120,0.5)', "",   "Otros"),
                    ]

                    offset_acc = 0
                    for key, color, pattern, label in waits:
                        w = wb.get(key, 0)
                        if w == 0:
                            continue
                            
                        # Check if this legend entry has been shown before
                        legend_key = f"wait_{key}"
                        show_legend = legend_key not in legend_shown
                        if show_legend:
                            legend_shown.add(legend_key)
                            
                        fig.add_trace(
                            go.Bar(
                                x=[w],
                                y=[link_name],
                                orientation='h',
                                base=[start_time + offset_acc],
                                name=label,
                                marker=dict(color=color,
                                            pattern=dict(shape=pattern, solidity=0.35)),
                                showlegend=show_legend,
                                legendgroup=legend_key,
                                hoverinfo='text',
                                hovertext=f"{label}: {w} Âµs<br>Flujo: {flow_id}",
                            ),
                            row=2, col=1
                        )
                        offset_acc += w

                # ---------------------------------------------------------------
                #  BLOQUE 3 Â· GUARD-BAND  (Î³eÂ·dmax)
                # ---------------------------------------------------------------
                guard_time = op_data['guard_time']
                if guard_time > 0:
                    # Check if guard-band legend has been shown before
                    show_guard_legend = "guard_band" not in legend_shown
                    if show_guard_legend:
                        legend_shown.add("guard_band")
                        
                    fig.add_trace(
                        go.Bar(
                            x=[guard_time],
                            y=[link_name],
                            orientation='h',
                            base=[end_time],
                            name="Guard-band",
                            legendgroup="guard_band",
                            marker=dict(
                                color='rgba(30,144,255,0.5)',
                                pattern=dict(shape="/", solidity=0.35)
                            ),
                            showlegend=show_guard_legend,
                            hoverinfo='text',
                            hovertext=f"Guard-band: {guard_time} Âµs<br>Flujo: {flow_id}",
                        ),
                        row=2, col=1
                    )

                # --- Barra principal para transmisiÃ³n ---
                fig.add_trace(
                    go.Bar(
                        x=[transmission_duration],
                        y=[link_name],
                        orientation='h',
                        base=[actual_start_time],
                        name=flow_id,                    # sigue como texto interno
                        marker=dict(
                            color=flow_colors[flow_id],
                            opacity=0.8,
                            line=dict(width=1, color='black')
                        ),
                        text=flow_id,
                        textposition='inside',
                        insidetextanchor='middle',
                        hoverinfo='text',
                        hovertext=(
                            f"Flujo: {flow_id}"
                            f"<br>PerÃ­odo: {flow_period}Âµs"
                            f"<br>Inicio Tx: {actual_start_time}Âµs"
                            f"<br>Fin Tx: {end_time}Âµs"
                            f"<br>Recibido: {reception_time}Âµs"
                        ),
                        showlegend=False,                # â† leyenda desactivada
                    ),
                    row=2, col=1
                )

                # Obtener detalles de las decisiones del agente si estÃ¡n disponibles
                guard_factor = getattr(operation, 'guard_factor', 1.0)
                min_gap = getattr(operation, 'min_gap_value', 1.0)
                
                # Tooltip completo con parÃ¡metros de RL
                hover_text = (
                    f"Flujo: {flow.flow_id}"
                    f"<br>PerÃ­odo: {flow.period}Âµs"
                    f"<br>Inicio Tx: {actual_start_time}Âµs"
                    f"<br>Fin Tx: {end_time}Âµs"
                    f"<br>Recibido: {reception_time}Âµs"
                    f"<br>Guard Factor: {guard_factor:.2f}"
                    f"<br>SeparaciÃ³n MÃ­n: {min_gap:.1f}Âµs"
                )

                # Marcadores ---
                # Marcador para start_time (cuÃ¡ndo podrÃ­a haber empezado)
                fig.add_trace(
                    go.Scatter(
                        x=[start_time],
                        y=[link_name],
                        mode='markers',
                        marker=dict(symbol='line-ns', size=15, color='green', line=dict(width=2)),
                        name='Start Time (Available)',
                        showlegend=False,
                        hoverinfo='text',
                        hovertext=f"Disponible (Start Time): {start_time}Âµs<br>Flujo: {flow_id}",
                    ),
                    row=2, col=1
                )

                # Marcador para gating_time (cuÃ¡ndo empezÃ³ realmente si hubo gating)
                if gating_time is not None:
                    fig.add_trace(
                        go.Scatter(
                            x=[gating_time],
                            y=[link_name],
                            mode='markers',
                            marker=dict(symbol='line-ns', size=18, color='red', line=dict(width=3)),
                            name='Gate Time (Actual Start)',
                            showlegend=False,
                            hoverinfo='text',
                            hovertext=f"Inicio Real (Gate Time): {gating_time}Âµs<br>Flujo: {flow_id}",
                        ),
                        row=2, col=1
                    )

                # Marcador para latest_time (lÃ­mite ventana gating)
                # Solo relevante si hay gating
                if gating_time is not None:
                    fig.add_trace(
                        go.Scatter(
                            x=[latest_time],
                            y=[link_name],
                            mode='markers',
                            marker=dict(symbol='line-ns', size=15, color='orange', line=dict(width=2)),
                            name='Latest Time',
                            showlegend=False,
                            hoverinfo='text',
                            hovertext=f"Latest Time: {latest_time}Âµs<br>Flujo: {flow_id}",
                        ),
                        row=2, col=1
                    )
    
    # -- SECCIÃ“N 3 (GCL): una fila por switch --------------------------------
    #  â‘  Agrupamos los eventos por switch   â†’  gcl_by_switch[src] = [...]
    #  â‘¡ Para cada switch aÃ±adimos *dos* trazas (lÃ­nea punteada + markers)

    # gcl_events se genera mÃ¡s arriba; aquÃ­ lo re-estructuramos:
    gcl_by_switch = defaultdict(list)
    for t, state, flow_id, link_name, is_gating in gcl_events:
        # el nombre de la fila serÃ¡, p.ej.,  "GCL S1"
        src_sw = link_name.split(' ')[0]          # 'S1' de "S1 â†’ SRV1"
        gcl_by_switch[src_sw].append((t, state, flow_id, is_gating))

    if gcl_by_switch:
        print(f"Dibujando GCL para {len(gcl_by_switch)} switches")
        # Aseguramos orden estable
        for sw_name in sorted(gcl_by_switch.keys()):
            events = sorted(gcl_by_switch[sw_name], key=lambda x: x[0])
            if not events:
                continue

            # Descomponer para construir arrays
            times   = [e[0] for e in events]
            states  = [e[1] for e in events]
            colors  = ['rgba(0,0,255,0.9)' if s == 0 else 'rgba(0,180,0,0.9)'
                       for s in states]
            htexts  = [("Cierre" if s == 0 else "Apertura") +
                       f"<br>{sw_name}<br>t={t}Âµs<br>Flujo={f}"
                       for (t, s, f, _) in events]

            # LÃ­nea guÃ­a (gris) para esa fila
            fig.add_trace(
                go.Scatter(
                    x=times,
                    y=[f"GCL {sw_name}"]*len(times),
                    mode='lines',
                    line=dict(color='lightgray', width=1.5, dash='dot'),
                    showlegend=False,
                    hoverinfo='none',
                ),
                row=3, col=1
            )
            # Marcadores con 0/1
            fig.add_trace(
                go.Scatter(
                    x=times,
                    y=[f"GCL {sw_name}"]*len(times),
                    mode='markers+text',
                    marker=dict(symbol='circle', size=15, color=colors,
                                line=dict(width=2, color='black')),
                    text=[str(s) for s in states],
                    textposition='middle center',
                    textfont=dict(color='white', size=10,
                                  family='Arial Black'),
                    name=f"GCL {sw_name}",
                    showlegend=False,
                    hoverinfo='text',
                    hovertext=htexts,
                ),
                row=3, col=1
            )
    else:
        fig.add_annotation(
            x=hyperperiod/2, y=0,
            text="No hay eventos GCL para mostrar",
            showarrow=False, font=dict(size=12, color="gray"),
            row=3, col=1
        )
        
    # AÃ±adir lÃ­nea vertical para el hiperperÃ­odo
    fig.add_vline(
        x=hyperperiod,
        line_width=2,
        line_dash="solid",
        line_color="blue",
        annotation_text=f"HiperperÃ­odo: {hyperperiod}Âµs",
        annotation_position="top",
        annotation_font_size=12,
        annotation_font_color="blue"
    )
    
    # ConfiguraciÃ³n de diseÃ±o mejorada para ocupar toda la pÃ¡gina
    fig.update_layout(
        title=f"ProgramaciÃ³n TSN de Flujos (HiperperÃ­odo: {hyperperiod}Âµs)",
        barmode='overlay',
        height=max(800, len(sorted_links) * 45 + 250),  # Altura ajustada para mejor visualizaciÃ³n
        width=1400,                                     # Ancho aumentado para mejor visualizaciÃ³n
        margin=dict(l=50, r=50, t=80, b=80),           # MÃ¡rgenes reducidos
        legend=dict(
            orientation="h",
            yanchor="bottom",
            y=-0.18,                                    # Ajustar posiciÃ³n de leyenda
            xanchor="center",
            x=0.5,
            title="Flujos"
        ),
        plot_bgcolor='white',
        hovermode='closest',
    )
    
    # Agregar leyenda interactiva para filtrar por tipo de evento
    fig.update_layout(
        legend=dict(
            orientation="v",     # vertical
            yanchor="top",
            y=1,
            xanchor="left",
            x=1.02,             # fuera del Ã¡rea de trazado
            font=dict(size=11),
            bordercolor="rgba(0,0,0,0.2)",
            borderwidth=1,
        )
    )
    
    # Vincular los ejes X para que el zoom se sincronice entre grÃ¡ficas
    tick_interval = max(100, hyperperiod // 20)  # mÃ¡x. 20 ticks
    left_pad = max(global_max_gap * 1.1, 10)      # algo de margen
    fig.update_xaxes(
        title="Tiempo (Âµs)",
        range=[-left_pad, hyperperiod*1.02],
        gridcolor='lightgray',
        griddash='dot',
        tickvals=list(range(0, hyperperiod + tick_interval, tick_interval)),
        row=1, col=1
    )
    
    fig.update_xaxes(
        title="Tiempo (Âµs)",
        range=[-left_pad, hyperperiod*1.02],
        showgrid=True,
        gridcolor='lightgray',
        griddash='dot',
        tickvals=list(range(0, hyperperiod + tick_interval, tick_interval)),
        row=2, col=1,
        matches='x'  # Esto sincroniza este eje con el eje X de la primera grÃ¡fica
    )
    
    fig.update_xaxes(
        title="Tiempo (Âµs)",
        range=[-left_pad, hyperperiod*1.02],
        showgrid=True,
        gridcolor='lightgray',
        griddash='dot',
        tickvals=list(range(0, hyperperiod + tick_interval, tick_interval)),
        row=3, col=1,
        matches='x'  # Esto sincroniza este eje con el eje X de la primera grÃ¡fica
    )
    
    # Mejorar el estilo de las etiquetas del eje Y
    fig.update_yaxes(
        title="Switches",
        row=1, col=1,
        linecolor='black',
        gridcolor='rgba(200,200,200,0.3)'
    )
    
    fig.update_yaxes(
        title="Enlaces",
        row=2, col=1,
        linecolor='black',
        gridcolor='rgba(200,200,200,0.3)'
    )
    
    # Manejo especial para el eje Y del GCL
    gcl_rows = [f"GCL {sw}" for sw in sorted(gcl_by_switch.keys())] or ["GCL"]
    fig.update_yaxes(
        showticklabels=True,
        tickvals=gcl_rows,
        ticktext=gcl_rows,
        row=3, col=1,
        linecolor='black',
        gridcolor='rgba(200,200,200,0.3)'
    )
    
    # Agregar leyenda adicional para sÃ­mbolos
    symbols_legend = [
        dict(name="Disponible (Start)", marker=dict(color="green", symbol="line-ns", size=10)),
        dict(name="Inicio Real (Gate)", marker=dict(color="red", symbol="line-ns", size=10)),
        dict(name="Ãšltimo Inicio (Latest)", marker=dict(color="orange", symbol="line-ns", size=10)),
        dict(name="Espera por FCFS/Switch Ocupado", marker=dict(color="rgba(100,100,255,0.7)", symbol="square", size=10)),
        dict(name="Espera por SeparaciÃ³n MÃ­nima", marker=dict(color="rgba(220,20,60,0.7)", symbol="square", size=10)),
    ]
    
    for item in symbols_legend:
        # Asegurarse de que el marcador no contenga 'pattern'
        marker_config = item.get('marker', {})
        if 'pattern' in marker_config:
             del marker_config['pattern'] # Eliminar si existe por error
                
        fig.add_trace(
            go.Scatter(
                x=[None],
                y=[None],
                mode='markers',
                marker=marker_config, # Usar la configuraciÃ³n corregida
                name=item.get('name', ''),
                showlegend=True
            )
        )
    
    # Agregar leyenda para la ocupaciÃ³n de switches
    fig.add_trace(
        go.Bar(
            x=[None],
            y=[None],
            name="Switch Ocupado",
            marker=dict(
                color='rgba(150,150,150,0.7)',
                pattern=dict(shape="x", solidity=0.3, fgcolor="black")
            ),
            showlegend=True
        )
    )
    
    # â”€â”€ Leyendas para bloques temporales â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    fig.add_trace(
        go.Bar(
            x=[None], y=[None], name="Guard-band",
            marker=dict(color='rgba(30,144,255,0.5)',
                        pattern=dict(shape="/", solidity=0.35)),
            showlegend=True)
    )
    # ya no es necesario aÃ±adir trazas Â«fantasmaÂ»;
    # la barra Î” sw real se encarga de la entrada en la leyenda.
    
    # AÃ±adimos marcador de Â«TransmisiÃ³nÂ» genÃ©rico (color neutro)
    fig.add_trace(
        go.Bar(
            x=[None], y=[None], name="TransmisiÃ³n",
            marker=dict(color='rgba(160,160,160,0.7)'),
            showlegend=True)
    )
    
    # Guardar como HTML interactivo con opciones para mejor visualizaciÃ³n
    if save_path is None:
        save_path = os.path.join(OUT_DIR, 'tsn_schedule_plotly.html')
    
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    fig.write_html(
        save_path,
        include_plotlyjs='cdn',
        full_html=True,
        include_mathjax='cdn',
        config={
            'scrollZoom': True,            # Permitir zoom con rueda del ratÃ³n
            'displayModeBar': True,        # Mostrar barra de herramientas
            'displaylogo': False,          # No mostrar logo de Plotly
            'toImageButtonOptions': {      # ConfiguraciÃ³n para guardar imagen
                'format': 'png',
                'filename': 'tsn_schedule',
                'height': 1200,
                'width': 1800,
                'scale': 2                 # Alta resoluciÃ³n
            }
        }
    )
    print(f"VisualizaciÃ³n interactiva TSN guardada en: {save_path}")
    
    # Try to open in browser with error handling
    try:
        webbrowser.open('file://' + os.path.abspath(save_path))
    except Exception as e:
        logging.warning(f"Could not open browser automatically: {e}")
        print(f"Please open the visualization manually at: file://{os.path.abspath(save_path)}")
    
    return fig

def visualize_lcm_cycle_plotly(schedule_res: ScheduleRes, save_path=None):
    """
    FunciÃ³n de compatibilidad que redirige a visualize_tsn_schedule_plotly
    """
    print("La funcionalidad de visualizaciÃ³n del hiperperÃ­odo estÃ¡ integrada en visualize_tsn_schedule_plotly.")
    print("Llamando a visualize_tsn_schedule_plotly...")
    return visualize_tsn_schedule_plotly(schedule_res, save_path)

if __name__ == "__main__":
    print("Este mÃ³dulo debe ser importado y utilizado desde test.py")
    print("Ejemplo: visualize_tsn_schedule_plotly(scheduler.get_res())")



